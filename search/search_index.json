{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RustyStats Documentation","text":"<p>High-performance Generalized Linear Models with a Rust backend and Python API</p> <p>RustyStats is a statistical modeling library designed for actuarial and data science applications. It combines the performance of Rust with the ease of use of Python, providing a <code>statsmodels</code>-compatible API with significant performance improvements.</p>"},{"location":"#why-rustystats","title":"Why RustyStats?","text":"Feature RustyStats Statsmodels Parallel IRLS Solver \u2705 Multi-threaded via Rayon \u274c Single-threaded Native Polars Support \u2705 Formula API with Polars \u274c Pandas only Built-in Lasso/Elastic Net \u2705 All GLM families \u26a0\ufe0f Limited Performance (678K rows) ~1 second ~5-10 seconds"},{"location":"#quick-example","title":"Quick Example","text":"Formula APIArray API <pre><code>import rustystats as rs\nimport polars as pl\n\ndata = pl.read_parquet(\"insurance.parquet\")\n\nresult = rs.glm(\n    formula=\"ClaimCount ~ VehPower + VehAge + C(Area) + C(Region)\",\n    data=data,\n    family=\"poisson\",\n    offset=\"Exposure\"\n).fit()\n\nprint(result.summary())\nprint(result.relativities())  # exp(coef) for pricing\n</code></pre> <pre><code>import rustystats as rs\nimport numpy as np\n\nresult = rs.fit_glm(\n    y, X,\n    family=\"poisson\",\n    offset=np.log(exposure)\n)\n\nprint(f\"Coefficients: {result.params}\")\nprint(f\"Deviance: {result.deviance}\")\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This documentation is organized for maintainers who may be new to Rust and/or GLMs:</p>"},{"location":"#for-understanding-the-math","title":"For Understanding the Math","text":"<ul> <li>GLM Theory - Complete mathematical foundation</li> <li>Distribution Families - Variance functions and when to use each</li> <li>Link Functions - Connecting linear predictors to means</li> <li>IRLS Algorithm - How GLMs are actually fitted</li> </ul>"},{"location":"#for-understanding-the-code","title":"For Understanding the Code","text":"<ul> <li>Architecture Overview - How components connect</li> <li>Rust Core Library - The computational engine</li> <li>Python Bindings - PyO3 bridge layer</li> </ul>"},{"location":"#for-maintaining-the-code","title":"For Maintaining the Code","text":"<ul> <li>Rust Best Practices - Code style and patterns</li> <li>Adding New Components - Extending the library</li> <li>Testing Strategy - Test organization and practices</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#distribution-families","title":"Distribution Families","text":"<ul> <li>Gaussian - Continuous data (linear regression)</li> <li>Poisson - Count data (claim frequency)</li> <li>Binomial - Binary outcomes (logistic regression)</li> <li>Gamma - Positive continuous (claim severity)</li> <li>Tweedie - Mixed zeros and positives (pure premium)</li> <li>QuasiPoisson/QuasiBinomial - Overdispersed data</li> <li>Negative Binomial - Alternative for overdispersed counts</li> </ul>"},{"location":"#advanced-features","title":"Advanced Features","text":"<ul> <li>Regularization - Ridge, Lasso, Elastic Net with cross-validation</li> <li>Splines - B-splines and natural splines for non-linear effects</li> <li>Target Encoding - CatBoost-style encoding for high-cardinality categoricals</li> <li>Robust Standard Errors - HC0, HC1, HC2, HC3 sandwich estimators</li> <li>Model Diagnostics - Calibration, discrimination, residual analysis</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Development installation\ncd rustystats\nuv run maturin develop\n\n# Run tests\nuv run pytest tests/python/ -v\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>rustystats/\n\u251c\u2500\u2500 crates/\n\u2502   \u251c\u2500\u2500 rustystats-core/        # Pure Rust GLM library (no Python deps)\n\u2502   \u2502   \u251c\u2500\u2500 Cargo.toml\n\u2502   \u2502   \u2514\u2500\u2500 src/\n\u2502   \u2502       \u251c\u2500\u2500 lib.rs          # Crate entry, re-exports\n\u2502   \u2502       \u251c\u2500\u2500 error.rs        # Error types\n\u2502   \u2502       \u251c\u2500\u2500 families/       # Distribution families (Gaussian, Poisson, etc.)\n\u2502   \u2502       \u251c\u2500\u2500 links/          # Link functions (Identity, Log, Logit)\n\u2502   \u2502       \u251c\u2500\u2500 solvers/        # IRLS, coordinate descent\n\u2502   \u2502       \u251c\u2500\u2500 inference/      # Standard errors, p-values, robust SEs\n\u2502   \u2502       \u251c\u2500\u2500 diagnostics/    # Residuals, calibration, discrimination\n\u2502   \u2502       \u251c\u2500\u2500 splines/        # B-splines, natural splines\n\u2502   \u2502       \u251c\u2500\u2500 formula/        # Formula parsing\n\u2502   \u2502       \u251c\u2500\u2500 design_matrix/  # Design matrix construction\n\u2502   \u2502       \u251c\u2500\u2500 regularization/ # Lasso, Ridge, Elastic Net\n\u2502   \u2502       \u251c\u2500\u2500 target_encoding/# CatBoost-style encoding\n\u2502   \u2502       \u2514\u2500\u2500 interactions/   # Interaction term handling\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 rustystats/             # Python bindings (PyO3)\n\u2502       \u251c\u2500\u2500 Cargo.toml\n\u2502       \u2514\u2500\u2500 src/lib.rs          # PyO3 wrappers, NumPy conversion\n\u2502\n\u251c\u2500\u2500 python/rustystats/          # Python package\n\u2502   \u251c\u2500\u2500 __init__.py             # Public API exports\n\u2502   \u251c\u2500\u2500 glm.py                  # GLMResults, fit_glm()\n\u2502   \u251c\u2500\u2500 formula.py              # Formula API, glm()\n\u2502   \u251c\u2500\u2500 families.py             # Python family wrappers\n\u2502   \u251c\u2500\u2500 links.py                # Python link wrappers\n\u2502   \u251c\u2500\u2500 splines.py              # bs(), ns() functions\n\u2502   \u251c\u2500\u2500 selection.py            # lasso_path(), cv_glm()\n\u2502   \u251c\u2500\u2500 diagnostics.py          # ModelDiagnostics, explore_data()\n\u2502   \u251c\u2500\u2500 interactions.py         # Interaction utilities\n\u2502   \u2514\u2500\u2500 target_encoding.py      # TargetEncoder class\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 python/                 # Python integration tests\n\u2502       \u251c\u2500\u2500 test_glm.py\n\u2502       \u251c\u2500\u2500 test_families.py\n\u2502       \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 docs/                       # MkDocs documentation (you are here)\n\u251c\u2500\u2500 examples/                   # Jupyter notebook examples\n\u251c\u2500\u2500 Cargo.toml                  # Workspace configuration\n\u251c\u2500\u2500 pyproject.toml              # Python build config (maturin)\n\u2514\u2500\u2500 mkdocs.yml                  # Documentation config\n</code></pre>"},{"location":"api/array-api/","title":"Array API Reference","text":"<p>The array API provides low-level access to GLM fitting using NumPy arrays directly.</p>"},{"location":"api/array-api/#fit_glm","title":"fit_glm","text":"<p>Fit a Generalized Linear Model.</p> <pre><code>rustystats.fit_glm(\n    y,\n    X,\n    family=\"gaussian\",\n    link=None,\n    offset=None,\n    weights=None,\n    alpha=0.0,\n    l1_ratio=1.0,\n    max_iterations=25,\n    tolerance=1e-8,\n    theta=None,\n    var_power=1.5,\n)\n</code></pre>"},{"location":"api/array-api/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y</code> array-like required Response variable (n,) <code>X</code> array-like required Design matrix (n, p), include intercept column <code>family</code> str <code>\"gaussian\"</code> Distribution family <code>link</code> str <code>None</code> Link function (uses family default if None) <code>offset</code> array-like <code>None</code> Offset term (n,) <code>weights</code> array-like <code>None</code> Prior weights (n,) <code>alpha</code> float <code>0.0</code> Regularization strength <code>l1_ratio</code> float <code>1.0</code> Elastic Net mixing (1=Lasso, 0=Ridge) <code>max_iterations</code> int <code>25</code> Maximum IRLS iterations <code>tolerance</code> float <code>1e-8</code> Convergence tolerance <code>theta</code> float <code>None</code> Negative Binomial dispersion <code>var_power</code> float <code>1.5</code> Tweedie variance power"},{"location":"api/array-api/#family-options","title":"Family Options","text":"Value Description <code>\"gaussian\"</code> Normal/Gaussian - continuous data <code>\"poisson\"</code> Poisson - count data <code>\"binomial\"</code> Binomial - binary/proportion data <code>\"gamma\"</code> Gamma - positive continuous <code>\"tweedie\"</code> Tweedie - mixed zeros and positives <code>\"quasipoisson\"</code> QuasiPoisson - overdispersed counts <code>\"quasibinomial\"</code> QuasiBinomial - overdispersed binary <code>\"negbinomial\"</code> Negative Binomial - overdispersed counts"},{"location":"api/array-api/#returns","title":"Returns","text":"<p><code>GLMResults</code> object with fitted model.</p>"},{"location":"api/array-api/#examples","title":"Examples","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\n# Basic Gaussian model\ny = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\nX = np.column_stack([np.ones(5), [1, 2, 3, 4, 5]])\nresult = rs.fit_glm(y, X, family=\"gaussian\")\n\n# Poisson with offset\nclaims = np.array([0, 1, 2, 0, 1])\nexposure = np.array([1.0, 1.0, 2.0, 0.5, 1.5])\nX = np.column_stack([np.ones(5), np.random.randn(5)])\nresult = rs.fit_glm(claims, X, family=\"poisson\", offset=np.log(exposure))\n\n# Regularized model\nresult = rs.fit_glm(y, X, family=\"gaussian\", alpha=0.1, l1_ratio=0.5)\n\n# Negative Binomial with theta\nresult = rs.fit_glm(counts, X, family=\"negbinomial\", theta=2.0)\n</code></pre>"},{"location":"api/array-api/#fit_negbinomial","title":"fit_negbinomial","text":"<p>Fit Negative Binomial GLM with automatic \u03b8 estimation.</p> <pre><code>rustystats.fit_negbinomial(\n    y,\n    X,\n    offset=None,\n    weights=None,\n    max_iterations=25,\n    tolerance=1e-8,\n)\n</code></pre>"},{"location":"api/array-api/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>y</code> array-like required Count response (n,) <code>X</code> array-like required Design matrix (n, p) <code>offset</code> array-like <code>None</code> Offset term <code>weights</code> array-like <code>None</code> Prior weights"},{"location":"api/array-api/#returns_1","title":"Returns","text":"<p><code>GLMResults</code> with estimated \u03b8 in <code>result.family</code>.</p>"},{"location":"api/array-api/#example","title":"Example","text":"<pre><code>result = rs.fit_negbinomial(counts, X)\nprint(result.family)  # \"NegativeBinomial(theta=2.34)\"\n</code></pre>"},{"location":"api/array-api/#glmresults","title":"GLMResults","text":"<p>Results object returned by <code>fit_glm</code>.</p>"},{"location":"api/array-api/#attributes","title":"Attributes","text":"Attribute Type Description <code>params</code> ndarray Fitted coefficients (p,) <code>fittedvalues</code> ndarray Predicted means (n,) <code>linear_predictor</code> ndarray Linear predictor \u03b7 = X\u03b2 (n,) <code>deviance</code> float Model deviance <code>iterations</code> int Number of IRLS iterations <code>converged</code> bool Whether fitting converged <code>nobs</code> int Number of observations <code>df_resid</code> int Residual degrees of freedom (n - p) <code>df_model</code> int Model degrees of freedom (p - 1) <code>family</code> str Family name"},{"location":"api/array-api/#inference-methods","title":"Inference Methods","text":"<pre><code># Standard errors\nresult.bse()           # Standard errors of coefficients\n\n# Test statistics\nresult.tvalues()       # z-statistics (coef / SE)\nresult.pvalues()       # Two-sided p-values\n\n# Confidence intervals\nresult.conf_int(alpha=0.05)  # Returns (lower, upper) arrays\n</code></pre>"},{"location":"api/array-api/#robust-inference-methods","title":"Robust Inference Methods","text":"<pre><code># Robust standard errors (sandwich estimators)\nresult.bse_robust(hc_type=\"HC1\")     # HC0, HC1, HC2, HC3\nresult.tvalues_robust(hc_type=\"HC1\")\nresult.pvalues_robust(hc_type=\"HC1\")\nresult.conf_int_robust(alpha=0.05, hc_type=\"HC1\")\n\n# Full covariance matrix\nresult.cov_params()        # Model-based covariance\nresult.cov_robust(\"HC1\")   # Robust covariance\n</code></pre>"},{"location":"api/array-api/#residual-methods","title":"Residual Methods","text":"<pre><code>result.resid_response()   # y - \u03bc\nresult.resid_pearson()    # (y - \u03bc) / \u221aV(\u03bc)\nresult.resid_deviance()   # sign(y-\u03bc) \u00d7 \u221ad\nresult.resid_working()    # (y - \u03bc) \u00d7 g'(\u03bc)\n</code></pre>"},{"location":"api/array-api/#fit-statistics","title":"Fit Statistics","text":"<pre><code>result.llf()              # Log-likelihood\nresult.aic()              # Akaike Information Criterion\nresult.bic()              # Bayesian Information Criterion\nresult.null_deviance()    # Deviance of intercept-only model\nresult.pearson_chi2()     # Pearson chi-squared statistic\nresult.scale()            # Dispersion (deviance-based)\nresult.scale_pearson()    # Dispersion (Pearson-based)\n</code></pre>"},{"location":"api/array-api/#regularization-methods","title":"Regularization Methods","text":"<pre><code>result.n_nonzero()         # Number of non-zero coefficients\nresult.selected_features() # Indices of non-zero coefficients\n</code></pre>"},{"location":"api/array-api/#example_1","title":"Example","text":"<pre><code>result = rs.fit_glm(y, X, family=\"poisson\")\n\nprint(f\"Coefficients: {result.params}\")\nprint(f\"Standard Errors: {result.bse()}\")\nprint(f\"P-values: {result.pvalues()}\")\nprint(f\"AIC: {result.aic():.2f}\")\nprint(f\"Converged: {result.converged} in {result.iterations} iterations\")\n\n# Check significance\nfor i, (coef, pval) in enumerate(zip(result.params, result.pvalues())):\n    sig = \"***\" if pval &lt; 0.001 else \"**\" if pval &lt; 0.01 else \"*\" if pval &lt; 0.05 else \"\"\n    print(f\"\u03b2{i}: {coef:.4f} (p={pval:.4f}) {sig}\")\n</code></pre>"},{"location":"api/array-api/#spline-functions","title":"Spline Functions","text":""},{"location":"api/array-api/#bs","title":"bs","text":"<p>B-spline basis matrix.</p> <pre><code>rustystats.bs(\n    x,\n    df=None,\n    knots=None,\n    degree=3,\n    boundary_knots=None,\n    include_intercept=False,\n)\n</code></pre>"},{"location":"api/array-api/#ns","title":"ns","text":"<p>Natural spline basis matrix.</p> <pre><code>rustystats.ns(\n    x,\n    df=None,\n    knots=None,\n    boundary_knots=None,\n    include_intercept=False,\n)\n</code></pre>"},{"location":"api/array-api/#example_2","title":"Example","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\n\n# B-spline basis\nbasis_bs = rs.bs(x, df=5)\nprint(basis_bs.shape)  # (100, 4) - df-1 columns without intercept\n\n# Natural spline basis\nbasis_ns = rs.ns(x, df=5)\nprint(basis_ns.shape)  # (100, 4)\n\n# Custom boundary knots\nbasis = rs.bs(x, df=6, boundary_knots=(0, 10))\n</code></pre>"},{"location":"api/array-api/#target-encoding","title":"Target Encoding","text":""},{"location":"api/array-api/#target_encode","title":"target_encode","text":"<p>Apply CatBoost-style target encoding.</p> <pre><code>rustystats.target_encode(\n    categories,\n    target,\n    column_name=\"category\",\n    prior_weight=1.0,\n    n_permutations=4,\n    seed=None,\n)\n</code></pre>"},{"location":"api/array-api/#returns_2","title":"Returns","text":"<p>Tuple: <code>(encoded_values, column_name, prior, level_stats)</code></p>"},{"location":"api/array-api/#apply_target_encoding","title":"apply_target_encoding","text":"<p>Apply learned encoding to new data.</p> <pre><code>rustystats.apply_target_encoding(\n    categories,\n    level_stats,\n    prior,\n)\n</code></pre>"},{"location":"api/array-api/#targetencoder","title":"TargetEncoder","text":"<p>Sklearn-style interface.</p> <pre><code>encoder = rs.TargetEncoder(prior_weight=1.0, n_permutations=4)\ntrain_encoded = encoder.fit_transform(train_categories, train_target)\ntest_encoded = encoder.transform(test_categories)\n</code></pre>"},{"location":"api/array-api/#example_3","title":"Example","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\ncategories = [\"A\", \"B\", \"A\", \"C\", \"B\"]\ntarget = np.array([1.0, 0.0, 0.5, 1.0, 0.2])\n\n# Encode\nencoded, name, prior, stats = rs.target_encode(\n    categories, target, \"category\",\n    prior_weight=1.0, n_permutations=4, seed=42\n)\n\n# Apply to new data\nnew_cats = [\"A\", \"D\"]  # D is unseen\nnew_encoded = rs.apply_target_encoding(new_cats, stats, prior)\n</code></pre>"},{"location":"api/diagnostics/","title":"Diagnostics API Reference","text":"<p>This page documents the model diagnostics functionality.</p>"},{"location":"api/diagnostics/#resultdiagnostics","title":"result.diagnostics()","text":"<p>Compute comprehensive model diagnostics.</p> <pre><code>diagnostics = result.diagnostics(\n    data,\n    categorical_factors=None,\n    continuous_factors=None,\n    exposure=None,\n    n_bins=10,\n    detect_interactions=True,\n)\n</code></pre>"},{"location":"api/diagnostics/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>data</code> DataFrame required Original data used for fitting <code>categorical_factors</code> list <code>None</code> Categorical columns to analyze <code>continuous_factors</code> list <code>None</code> Continuous columns to analyze <code>exposure</code> str <code>None</code> Exposure column name <code>n_bins</code> int <code>10</code> Number of bins for calibration <code>detect_interactions</code> bool <code>True</code> Whether to detect interactions"},{"location":"api/diagnostics/#returns","title":"Returns","text":"<p><code>ModelDiagnostics</code> object.</p>"},{"location":"api/diagnostics/#modeldiagnostics","title":"ModelDiagnostics","text":""},{"location":"api/diagnostics/#attributes","title":"Attributes","text":""},{"location":"api/diagnostics/#model_summary","title":"model_summary","text":"<p>Basic model information.</p> <pre><code>diagnostics.model_summary\n# {\n#     'family': 'poisson',\n#     'link': 'log',\n#     'n_observations': 10000,\n#     'n_parameters': 15,\n#     'converged': True,\n#     'iterations': 5\n# }\n</code></pre>"},{"location":"api/diagnostics/#fit_statistics","title":"fit_statistics","text":"<p>Goodness-of-fit metrics.</p> <pre><code>diagnostics.fit_statistics\n# {\n#     'deviance': 12345.67,\n#     'null_deviance': 15000.00,\n#     'aic': 12375.67,\n#     'bic': 12450.00,\n#     'log_likelihood': -6172.84,\n#     'dispersion_pearson': 1.05,\n#     'dispersion_deviance': 1.03,\n#     'pseudo_r2': 0.177\n# }\n</code></pre>"},{"location":"api/diagnostics/#calibration","title":"calibration","text":"<p>Calibration metrics.</p> <pre><code>diagnostics.calibration\n# {\n#     'overall_ae': 0.998,\n#     'by_decile': [\n#         {'decile': 1, 'actual': 100, 'expected': 95, 'ae_ratio': 1.053, ...},\n#         {'decile': 2, 'actual': 150, 'expected': 148, 'ae_ratio': 1.014, ...},\n#         ...\n#     ],\n#     'hosmer_lemeshow': {'statistic': 8.5, 'df': 8, 'pvalue': 0.38}\n# }\n</code></pre>"},{"location":"api/diagnostics/#discrimination","title":"discrimination","text":"<p>Discrimination metrics.</p> <pre><code>diagnostics.discrimination\n# {\n#     'gini_coefficient': 0.42,\n#     'auc': 0.71,\n#     'ks_statistic': 0.35,\n#     'lorenz_curve': [(0.0, 0.0), (0.1, 0.05), ..., (1.0, 1.0)],\n#     'lift_top_decile': 2.5\n# }\n</code></pre>"},{"location":"api/diagnostics/#factors","title":"factors","text":"<p>Per-factor diagnostics (list of <code>FactorDiagnostic</code>).</p> <pre><code>for factor in diagnostics.factors:\n    print(f\"{factor.name}:\")\n    print(f\"  Type: {factor.factor_type}\")\n    print(f\"  In model: {factor.in_model}\")\n    print(f\"  A/E range: {factor.ae_range}\")\n    print(f\"  Residual correlation: {factor.residual_correlation}\")\n</code></pre>"},{"location":"api/diagnostics/#interaction_candidates","title":"interaction_candidates","text":"<p>Detected potential interactions.</p> <pre><code>for ic in diagnostics.interaction_candidates:\n    print(f\"{ic['factor1']} \u00d7 {ic['factor2']}: strength={ic['strength']:.3f}\")\n</code></pre>"},{"location":"api/diagnostics/#warnings","title":"warnings","text":"<p>Auto-generated warnings.</p> <pre><code>for warning in diagnostics.warnings:\n    print(f\"[{warning['type']}] {warning['message']}\")\n</code></pre>"},{"location":"api/diagnostics/#methods","title":"Methods","text":""},{"location":"api/diagnostics/#to_json","title":"to_json()","text":"<p>Export diagnostics as JSON string.</p> <pre><code>json_str = diagnostics.to_json()\n</code></pre>"},{"location":"api/diagnostics/#to_dict","title":"to_dict()","text":"<p>Export as Python dictionary.</p> <pre><code>data = diagnostics.to_dict()\n</code></pre>"},{"location":"api/diagnostics/#resultdiagnostics_json","title":"result.diagnostics_json()","text":"<p>Convenience method to get JSON directly.</p> <pre><code>json_str = result.diagnostics_json(\n    data=data,\n    categorical_factors=[\"region\"],\n    continuous_factors=[\"age\"],\n)\n</code></pre>"},{"location":"api/diagnostics/#explore_data","title":"explore_data()","text":"<p>Pre-fit data exploration (no model required).</p> <pre><code>exploration = rs.explore_data(\n    data,\n    response,\n    categorical_factors=None,\n    continuous_factors=None,\n    exposure=None,\n    family=\"poisson\",\n    detect_interactions=True,\n)\n</code></pre>"},{"location":"api/diagnostics/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>data</code> DataFrame Data to explore <code>response</code> str Response column name <code>categorical_factors</code> list Categorical columns <code>continuous_factors</code> list Continuous columns <code>exposure</code> str Exposure column <code>family</code> str Expected family (for rate calculation) <code>detect_interactions</code> bool Whether to detect interactions"},{"location":"api/diagnostics/#returns_1","title":"Returns","text":"<p><code>DataExploration</code> object.</p>"},{"location":"api/diagnostics/#dataexploration","title":"DataExploration","text":"<pre><code>exploration.response_stats\n# {\n#     'n_observations': 10000,\n#     'mean': 0.05,\n#     'std': 0.22,\n#     'min': 0,\n#     'max': 5,\n#     'zeros_pct': 95.2,\n#     'total_exposure': 9500.0\n# }\n\nexploration.factor_stats\n# [\n#     {'name': 'region', 'type': 'categorical', 'n_levels': 5, ...},\n#     {'name': 'age', 'type': 'continuous', 'mean': 42.3, 'std': 15.2, ...},\n# ]\n\nexploration.interaction_candidates\n# [{'factor1': 'age', 'factor2': 'region', 'strength': 0.08}, ...]\n\nexploration.to_json()  # Export as JSON\n</code></pre>"},{"location":"api/diagnostics/#example","title":"Example","text":"<pre><code>import rustystats as rs\nimport polars as pl\n\ndata = pl.read_parquet(\"insurance.parquet\")\n\n# Explore before fitting\nexploration = rs.explore_data(\n    data=data,\n    response=\"ClaimNb\",\n    categorical_factors=[\"Region\", \"VehBrand\", \"Area\"],\n    continuous_factors=[\"DrivAge\", \"VehPower\", \"Density\"],\n    exposure=\"Exposure\",\n    family=\"poisson\",\n    detect_interactions=True,\n)\n\nprint(\"Response distribution:\")\nprint(exploration.response_stats)\n\nprint(\"\\nFactor summary:\")\nfor f in exploration.factor_stats:\n    print(f\"  {f['name']}: {f['type']}\")\n\nprint(\"\\nSuggested interactions:\")\nfor ic in exploration.interaction_candidates[:5]:\n    print(f\"  {ic['factor1']} \u00d7 {ic['factor2']} (strength: {ic['strength']:.3f})\")\n</code></pre>"},{"location":"api/diagnostics/#factordiagnostic","title":"FactorDiagnostic","text":"<p>Per-factor diagnostics object.</p>"},{"location":"api/diagnostics/#attributes_1","title":"Attributes","text":"Attribute Type Description <code>name</code> str Factor name <code>factor_type</code> str <code>\"categorical\"</code> or <code>\"continuous\"</code> <code>in_model</code> bool Whether factor is in the model <code>actual_vs_expected</code> list A/E by level/bin <code>residual_pattern</code> dict Residual analysis <code>ae_range</code> tuple (min A/E, max A/E) <code>residual_correlation</code> float Correlation with residuals"},{"location":"api/diagnostics/#actual_vs_expected","title":"actual_vs_expected","text":"<p>For categorical factors: <pre><code>[\n    {'level': 'A', 'exposure': 1000, 'actual': 50, 'expected': 48, 'ae_ratio': 1.04},\n    {'level': 'B', 'exposure': 1500, 'actual': 70, 'expected': 75, 'ae_ratio': 0.93},\n    ...\n]\n</code></pre></p> <p>For continuous factors: <pre><code>[\n    {'bin': 1, 'range': (18, 25), 'exposure': 500, 'actual': 30, 'expected': 25, 'ae_ratio': 1.20},\n    {'bin': 2, 'range': (25, 35), 'exposure': 800, 'actual': 35, 'expected': 38, 'ae_ratio': 0.92},\n    ...\n]\n</code></pre></p>"},{"location":"api/diagnostics/#json-structure","title":"JSON Structure","text":"<p>The JSON export is optimized for LLM consumption:</p> <pre><code>{\n  \"model_summary\": {\n    \"family\": \"poisson\",\n    \"link\": \"log\",\n    \"n_observations\": 10000,\n    \"n_parameters\": 15\n  },\n  \"fit_statistics\": {\n    \"deviance\": 12345.67,\n    \"aic\": 12375.67,\n    \"dispersion_pearson\": 1.05\n  },\n  \"calibration\": {\n    \"overall_ae\": 0.998,\n    \"by_decile\": [...]\n  },\n  \"discrimination\": {\n    \"gini_coefficient\": 0.42,\n    \"auc\": 0.71\n  },\n  \"factors\": [\n    {\n      \"name\": \"Region\",\n      \"factor_type\": \"categorical\",\n      \"in_model\": true,\n      \"actual_vs_expected\": [...],\n      \"residual_pattern\": {\"correlation\": 0.01}\n    }\n  ],\n  \"interaction_candidates\": [\n    {\"factor1\": \"Age\", \"factor2\": \"Region\", \"strength\": 0.03}\n  ],\n  \"warnings\": [\n    {\"type\": \"overdispersion\", \"message\": \"Dispersion ratio is 1.5...\"}\n  ]\n}\n</code></pre>"},{"location":"api/diagnostics/#complete-example","title":"Complete Example","text":"<pre><code>import rustystats as rs\nimport polars as pl\n\n# Load and fit\ndata = pl.read_parquet(\"insurance.parquet\")\nresult = rs.glm(\n    \"ClaimNb ~ C(Area) + C(VehBrand) + bs(DrivAge, df=4)\",\n    data=data,\n    family=\"poisson\",\n    offset=\"Exposure\"\n).fit()\n\n# Compute diagnostics (including non-fitted factors)\ndiagnostics = result.diagnostics(\n    data=data,\n    categorical_factors=[\"Area\", \"VehBrand\", \"Region\"],  # Region not in model\n    continuous_factors=[\"DrivAge\", \"VehPower\", \"Density\"],  # VehPower, Density not in model\n)\n\n# Check overall calibration\nae = diagnostics.calibration['overall_ae']\nprint(f\"Overall A/E: {ae:.3f}\")\nif abs(ae - 1.0) &gt; 0.02:\n    print(\"  \u26a0\ufe0f Model may be miscalibrated\")\n\n# Check discrimination\ngini = diagnostics.discrimination['gini_coefficient']\nprint(f\"Gini: {gini:.3f}\")\n\n# Check for missing factors\nfor factor in diagnostics.factors:\n    if not factor.in_model and abs(factor.residual_correlation) &gt; 0.03:\n        print(f\"  \u26a0\ufe0f Consider adding {factor.name} (residual corr: {factor.residual_correlation:.3f})\")\n\n# Check for interactions\nfor ic in diagnostics.interaction_candidates[:3]:\n    print(f\"  Consider: {ic['factor1']} \u00d7 {ic['factor2']}\")\n\n# View warnings\nfor w in diagnostics.warnings:\n    print(f\"  \u26a0\ufe0f [{w['type']}] {w['message']}\")\n\n# Export for LLM analysis\njson_output = diagnostics.to_json()\n</code></pre>"},{"location":"api/formula-api/","title":"Formula API Reference","text":"<p>The formula API provides a high-level interface using R-style formulas and DataFrames.</p>"},{"location":"api/formula-api/#glm","title":"glm","text":"<p>Create a GLM specification with formula.</p> <pre><code>rustystats.glm(\n    formula,\n    data,\n    family=\"gaussian\",\n    link=None,\n    offset=None,\n    weights=None,\n    alpha=0.0,\n    l1_ratio=1.0,\n    theta=None,\n    var_power=1.5,\n)\n</code></pre>"},{"location":"api/formula-api/#parameters","title":"Parameters","text":"Parameter Type Description <code>formula</code> str R-style formula (e.g., <code>\"y ~ x1 + x2\"</code>) <code>data</code> DataFrame Polars or Pandas DataFrame <code>family</code> str Distribution family <code>link</code> str Link function (optional) <code>offset</code> str Column name for offset <code>weights</code> str Column name for weights <code>alpha</code> float Regularization strength <code>l1_ratio</code> float Elastic Net mixing <code>theta</code> float Negative Binomial dispersion <code>var_power</code> float Tweedie variance power"},{"location":"api/formula-api/#returns","title":"Returns","text":"<p><code>FormulaGLM</code> object - call <code>.fit()</code> to fit the model.</p>"},{"location":"api/formula-api/#example","title":"Example","text":"<pre><code>import rustystats as rs\nimport polars as pl\n\ndata = pl.DataFrame({\n    \"y\": [1, 2, 3, 4, 5],\n    \"x1\": [1.0, 2.0, 3.0, 4.0, 5.0],\n    \"x2\": [5.0, 4.0, 3.0, 2.0, 1.0],\n    \"region\": [\"A\", \"B\", \"A\", \"B\", \"A\"],\n})\n\nresult = rs.glm(\n    formula=\"y ~ x1 + x2 + C(region)\",\n    data=data,\n    family=\"gaussian\"\n).fit()\n</code></pre>"},{"location":"api/formula-api/#formula-syntax","title":"Formula Syntax","text":""},{"location":"api/formula-api/#basic-terms","title":"Basic Terms","text":"<pre><code>\"y ~ x1 + x2\"           # Continuous variables\n\"y ~ x1 + C(region)\"    # Categorical with C()\n\"y ~ x1 - 1\"            # Remove intercept\n</code></pre>"},{"location":"api/formula-api/#categorical-variables","title":"Categorical Variables","text":"<p>Use <code>C()</code> to mark categorical variables:</p> <pre><code>\"y ~ C(region)\"                    # Single categorical\n\"y ~ C(region) + C(vehicle_type)\"  # Multiple categoricals\n\"y ~ x1 + C(region, ref='A')\"      # Custom reference level\n</code></pre>"},{"location":"api/formula-api/#interactions","title":"Interactions","text":"<pre><code>\"y ~ x1 * x2\"           # Main effects + interaction: x1 + x2 + x1:x2\n\"y ~ x1 : x2\"           # Pure interaction only\n\"y ~ C(region) * age\"   # Categorical \u00d7 continuous\n\"y ~ C(a) * C(b)\"       # Categorical \u00d7 categorical\n</code></pre>"},{"location":"api/formula-api/#splines","title":"Splines","text":"<pre><code>\"y ~ bs(age, df=5)\"                    # B-spline with 5 df\n\"y ~ ns(income, df=4)\"                 # Natural spline\n\"y ~ bs(age, df=5, degree=3)\"          # Specify degree\n\"y ~ bs(age, df=4) * C(gender)\"        # Spline \u00d7 categorical\n</code></pre>"},{"location":"api/formula-api/#target-encoding","title":"Target Encoding","text":"<pre><code>\"y ~ TE(brand)\"                        # Target encoding\n\"y ~ TE(zipcode, prior_weight=2.0)\"    # With options\n\"y ~ TE(brand) + age + C(region)\"      # Mixed\n</code></pre>"},{"location":"api/formula-api/#complex-formulas","title":"Complex Formulas","text":"<pre><code># Insurance frequency model\n\"ClaimNb ~ C(Area) + C(VehBrand) + bs(DrivAge, df=4) + VehPower + log(Density)\"\n\n# With interactions\n\"ClaimNb ~ C(Area) * VehPower + C(VehBrand) + ns(DrivAge, df=5)\"\n\n# With target encoding for high-cardinality\n\"ClaimNb ~ TE(VehicleModel) + C(Area) + bs(Age, df=4)\"\n</code></pre>"},{"location":"api/formula-api/#formulaglm","title":"FormulaGLM","text":"<p>Object returned by <code>glm()</code>.</p>"},{"location":"api/formula-api/#methods","title":"Methods","text":"Method Description <code>.fit()</code> Fit the model, returns FormulaGLMResults"},{"location":"api/formula-api/#example_1","title":"Example","text":"<pre><code>model = rs.glm(\"y ~ x1 + C(region)\", data, family=\"poisson\")\nresult = model.fit()\n</code></pre>"},{"location":"api/formula-api/#formulaglmresults","title":"FormulaGLMResults","text":"<p>Results from fitting a formula-based GLM. Inherits all methods from <code>GLMResults</code> plus formula-specific methods.</p>"},{"location":"api/formula-api/#additional-attributes","title":"Additional Attributes","text":"Attribute Type Description <code>feature_names</code> list Names of all features including encoded categoricals <code>formula</code> str Original formula string"},{"location":"api/formula-api/#additional-methods","title":"Additional Methods","text":"<pre><code># Formatted summary table\nresult.summary()\n\n# Coefficient table as DataFrame\nresult.coef_table()\n\n# Relativities (exp(coef) for log-link models)\nresult.relativities()\nresult.relativities_table()  # As DataFrame\n</code></pre>"},{"location":"api/formula-api/#summary","title":"summary()","text":"<p>Print formatted regression table:</p> <pre><code>result = rs.glm(\"y ~ x1 + C(region)\", data, family=\"poisson\").fit()\nprint(result.summary())\n</code></pre> <p>Output: <pre><code>                 GLM Results                  \n==============================================\nFamily:        Poisson\nLink:          Log\nObservations:  1000\nDf Residual:   997\nDf Model:      2\nDeviance:      1234.56\nAIC:           1240.56\n\n                  coef    std err      z      P&gt;|z|\n--------------------------------------------------\nIntercept       0.5234     0.0512   10.22    0.000 ***\nx1              0.1234     0.0234    5.27    0.000 ***\nC(region)_B    -0.2341     0.0456   -5.13    0.000 ***\nC(region)_C     0.1567     0.0423    3.70    0.000 ***\n--------------------------------------------------\nSignif. codes: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05\n</code></pre></p>"},{"location":"api/formula-api/#coef_table","title":"coef_table()","text":"<p>Get coefficients as a DataFrame:</p> <pre><code>table = result.coef_table()\nprint(table)\n</code></pre> feature coef std_err z pvalue ci_lower ci_upper Intercept 0.523 0.051 10.22 0.000 0.423 0.624 x1 0.123 0.023 5.27 0.000 0.078 0.169 ... ... ... ... ... ... ..."},{"location":"api/formula-api/#relativities","title":"relativities()","text":"<p>Get multiplicative effects (exp(coef)) for log-link models:</p> <pre><code>rel = result.relativities()\n# Returns array: [exp(\u03b20), exp(\u03b21), ...]\n</code></pre>"},{"location":"api/formula-api/#relativities_table","title":"relativities_table()","text":"<p>Get relativities as formatted DataFrame:</p> <pre><code>table = result.relativities_table()\n</code></pre> feature coef relativity ci_lower ci_upper Intercept 0.523 1.687 1.526 1.868 C(region)_B -0.234 0.791 0.722 0.868 ... ... ... ... ..."},{"location":"api/formula-api/#prediction","title":"Prediction","text":""},{"location":"api/formula-api/#predict","title":"predict()","text":"<p>Make predictions on new data:</p> <pre><code>new_data = pl.DataFrame({\n    \"x1\": [1.0, 2.0],\n    \"region\": [\"A\", \"B\"],\n})\n\npredictions = result.predict(new_data)\n</code></pre>"},{"location":"api/formula-api/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>data</code> DataFrame New data with same columns as training <code>type</code> str <code>\"response\"</code> (default) or <code>\"link\"</code>"},{"location":"api/formula-api/#example_2","title":"Example","text":"<pre><code># Predict on response scale (default)\nmu = result.predict(new_data)\n\n# Predict on link scale (linear predictor)\neta = result.predict(new_data, type=\"link\")\n</code></pre>"},{"location":"api/formula-api/#diagnostics","title":"Diagnostics","text":""},{"location":"api/formula-api/#diagnostics_1","title":"diagnostics()","text":"<p>Compute comprehensive model diagnostics:</p> <pre><code>diagnostics = result.diagnostics(\n    data=data,\n    categorical_factors=[\"region\", \"brand\"],\n    continuous_factors=[\"age\", \"income\"],\n)\n</code></pre>"},{"location":"api/formula-api/#diagnostics_json","title":"diagnostics_json()","text":"<p>Get diagnostics as JSON string:</p> <pre><code>json_str = result.diagnostics_json(\n    data=data,\n    categorical_factors=[\"region\"],\n    continuous_factors=[\"age\"],\n)\n</code></pre> <p>See Diagnostics API for details.</p>"},{"location":"api/formula-api/#complete-example","title":"Complete Example","text":"<pre><code>import rustystats as rs\nimport polars as pl\n\n# Load data\ndata = pl.read_parquet(\"insurance_claims.parquet\")\n\n# Fit model\nresult = rs.glm(\n    formula=\"ClaimNb ~ bs(DrivAge, df=5) + C(Area) + C(VehBrand) + VehPower\",\n    data=data,\n    family=\"poisson\",\n    offset=\"Exposure\"  # log(Exposure) applied automatically\n).fit()\n\n# View results\nprint(result.summary())\n\n# Relativities for pricing\nprint(\"\\nRelativities:\")\nprint(result.relativities_table())\n\n# Model diagnostics\ndiagnostics = result.diagnostics(\n    data=data,\n    categorical_factors=[\"Area\", \"VehBrand\"],\n    continuous_factors=[\"DrivAge\", \"VehPower\"],\n)\nprint(f\"\\nGini: {diagnostics.discrimination['gini_coefficient']:.3f}\")\nprint(f\"A/E: {diagnostics.calibration['overall_ae']:.3f}\")\n\n# Predict on new data\nnew_policies = pl.DataFrame({\n    \"DrivAge\": [25, 45],\n    \"Area\": [\"A\", \"B\"],\n    \"VehBrand\": [\"Toyota\", \"BMW\"],\n    \"VehPower\": [5, 7],\n    \"Exposure\": [1.0, 1.0],\n})\npredictions = result.predict(new_policies)\nprint(f\"\\nPredicted frequencies: {predictions}\")\n</code></pre>"},{"location":"api/regularization/","title":"Regularization API Reference","text":"<p>This page documents the regularization and variable selection functionality.</p>"},{"location":"api/regularization/#lasso_path","title":"lasso_path","text":"<p>Compute coefficients along a regularization path.</p> <pre><code>rustystats.lasso_path(\n    y,\n    X,\n    family=\"gaussian\",\n    n_alphas=100,\n    alpha_min_ratio=0.001,\n    l1_ratio=1.0,\n    max_iterations=25,\n    tolerance=1e-8,\n)\n</code></pre>"},{"location":"api/regularization/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>y</code> array-like required Response variable <code>X</code> array-like required Design matrix <code>family</code> str <code>\"gaussian\"</code> Distribution family <code>n_alphas</code> int <code>100</code> Number of alpha values <code>alpha_min_ratio</code> float <code>0.001</code> Ratio of min to max alpha <code>l1_ratio</code> float <code>1.0</code> Elastic Net mixing <code>max_iterations</code> int <code>25</code> Max IRLS iterations per alpha <code>tolerance</code> float <code>1e-8</code> Convergence tolerance"},{"location":"api/regularization/#returns","title":"Returns","text":"<p><code>LassoPathResult</code> object.</p>"},{"location":"api/regularization/#lassopathresult","title":"LassoPathResult","text":"Attribute Type Description <code>alphas</code> ndarray Alpha values (descending) <code>coefs</code> ndarray Coefficients (p \u00d7 n_alphas) <code>deviances</code> ndarray Deviance at each alpha <code>n_nonzero</code> ndarray Non-zero count at each alpha"},{"location":"api/regularization/#methods","title":"Methods","text":"<pre><code>path.plot()           # Plot coefficient paths\npath.coef_at(alpha)   # Interpolate coefficients at specific alpha\n</code></pre>"},{"location":"api/regularization/#example","title":"Example","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\ny = np.random.randn(100)\nX = np.column_stack([np.ones(100), np.random.randn(100, 10)])\n\n# Compute path\npath = rs.lasso_path(y, X, family=\"gaussian\", n_alphas=50)\n\n# View results\nprint(f\"Alphas: {path.alphas[:5]}...\")\nprint(f\"Coefficients shape: {path.coefs.shape}\")\n\n# Find alpha where 5 features are selected\nfor i, n in enumerate(path.n_nonzero):\n    if n &lt;= 5:\n        print(f\"Alpha={path.alphas[i]:.4f} selects {n} features\")\n        break\n\n# Plot\npath.plot()\n</code></pre>"},{"location":"api/regularization/#cv_glm","title":"cv_glm","text":"<p>Cross-validation for optimal regularization parameter.</p> <pre><code>rustystats.cv_glm(\n    y,\n    X,\n    family=\"gaussian\",\n    cv=5,\n    n_alphas=100,\n    alpha_min_ratio=0.001,\n    l1_ratio=1.0,\n    scoring=\"deviance\",\n    max_iterations=25,\n    tolerance=1e-8,\n    seed=None,\n)\n</code></pre>"},{"location":"api/regularization/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>y</code> array-like required Response variable <code>X</code> array-like required Design matrix <code>family</code> str <code>\"gaussian\"</code> Distribution family <code>cv</code> int <code>5</code> Number of CV folds <code>n_alphas</code> int <code>100</code> Number of alpha values <code>alpha_min_ratio</code> float <code>0.001</code> Ratio of min to max alpha <code>l1_ratio</code> float <code>1.0</code> Elastic Net mixing <code>scoring</code> str <code>\"deviance\"</code> CV scoring metric <code>seed</code> int <code>None</code> Random seed for fold assignment"},{"location":"api/regularization/#returns_1","title":"Returns","text":"<p><code>CVGLMResult</code> object.</p>"},{"location":"api/regularization/#cvglmresult","title":"CVGLMResult","text":"Attribute Type Description <code>alphas</code> ndarray Alpha values tested <code>cv_scores</code> ndarray Mean CV score at each alpha <code>cv_scores_std</code> ndarray Std of CV score at each alpha <code>alpha_best</code> float Alpha with minimum CV score <code>alpha_1se</code> float Largest alpha within 1 SE of minimum <code>coef_best</code> ndarray Coefficients at alpha_best <code>coef_1se</code> ndarray Coefficients at alpha_1se <code>n_nonzero_best</code> int Non-zero count at alpha_best <code>n_nonzero_1se</code> int Non-zero count at alpha_1se"},{"location":"api/regularization/#methods_1","title":"Methods","text":"<pre><code>cv_result.plot()              # Plot CV curve with error bars\ncv_result.refit(y, X)         # Refit model at alpha_best\ncv_result.refit_1se(y, X)     # Refit model at alpha_1se\n</code></pre>"},{"location":"api/regularization/#example_1","title":"Example","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\nnp.random.seed(42)\nn, p = 500, 20\nX = np.column_stack([np.ones(n), np.random.randn(n, p)])\nbeta_true = np.zeros(p + 1)\nbeta_true[:6] = [1.0, 0.5, -0.5, 0.3, -0.3, 0.2]\ny = np.random.poisson(np.exp(X @ beta_true))\n\n# Cross-validation\ncv_result = rs.cv_glm(y, X, family=\"poisson\", cv=5, l1_ratio=1.0)\n\nprint(f\"Best alpha: {cv_result.alpha_best:.4f}\")\nprint(f\"Features at best: {cv_result.n_nonzero_best}\")\nprint(f\"1-SE alpha: {cv_result.alpha_1se:.4f}\")\nprint(f\"Features at 1-SE: {cv_result.n_nonzero_1se}\")\n\n# Plot CV curve\ncv_result.plot()\n\n# Refit final model\nfinal_result = cv_result.refit_1se(y, X)\nprint(f\"Final model has {final_result.n_nonzero()} features\")\n</code></pre>"},{"location":"api/regularization/#cv_lasso","title":"cv_lasso","text":"<p>Convenience function for Lasso cross-validation (l1_ratio=1.0).</p> <pre><code>rustystats.cv_lasso(y, X, family=\"gaussian\", cv=5, **kwargs)\n</code></pre> <p>Equivalent to <code>cv_glm(..., l1_ratio=1.0)</code>.</p>"},{"location":"api/regularization/#cv_ridge","title":"cv_ridge","text":"<p>Convenience function for Ridge cross-validation (l1_ratio=0.0).</p> <pre><code>rustystats.cv_ridge(y, X, family=\"gaussian\", cv=5, **kwargs)\n</code></pre> <p>Equivalent to <code>cv_glm(..., l1_ratio=0.0)</code>.</p>"},{"location":"api/regularization/#regularized-fit_glm","title":"Regularized fit_glm","text":"<p>The main <code>fit_glm</code> function supports regularization via <code>alpha</code> and <code>l1_ratio</code> parameters.</p>"},{"location":"api/regularization/#lasso-l1","title":"Lasso (L1)","text":"<pre><code>result = rs.fit_glm(y, X, family=\"poisson\", alpha=0.1, l1_ratio=1.0)\n</code></pre>"},{"location":"api/regularization/#ridge-l2","title":"Ridge (L2)","text":"<pre><code>result = rs.fit_glm(y, X, family=\"gaussian\", alpha=0.1, l1_ratio=0.0)\n</code></pre>"},{"location":"api/regularization/#elastic-net","title":"Elastic Net","text":"<pre><code>result = rs.fit_glm(y, X, family=\"gaussian\", alpha=0.1, l1_ratio=0.5)\n</code></pre>"},{"location":"api/regularization/#regularized-results","title":"Regularized Results","text":"<p>Regularized models have additional methods:</p> <pre><code>result.n_nonzero()         # Number of non-zero coefficients\nresult.selected_features() # Indices of selected features\n</code></pre>"},{"location":"api/regularization/#choosing-alpha","title":"Choosing Alpha","text":""},{"location":"api/regularization/#using-cv","title":"Using CV","text":"<pre><code>cv_result = rs.cv_glm(y, X, family=\"poisson\", l1_ratio=1.0, cv=5)\n\n# Option 1: Minimum CV error\nresult = rs.fit_glm(y, X, family=\"poisson\", \n                     alpha=cv_result.alpha_best, l1_ratio=1.0)\n\n# Option 2: 1-SE rule (more parsimonious)\nresult = rs.fit_glm(y, X, family=\"poisson\",\n                     alpha=cv_result.alpha_1se, l1_ratio=1.0)\n</code></pre>"},{"location":"api/regularization/#manual-selection","title":"Manual Selection","text":"<pre><code># Trace the path\npath = rs.lasso_path(y, X, family=\"poisson\")\n\n# Find alpha for desired sparsity\ntarget_features = 10\nfor i, n in enumerate(path.n_nonzero):\n    if n &lt;= target_features:\n        alpha = path.alphas[i]\n        break\n\nresult = rs.fit_glm(y, X, family=\"poisson\", alpha=alpha, l1_ratio=1.0)\n</code></pre>"},{"location":"api/regularization/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\n# Generate data\nnp.random.seed(42)\nn, p = 1000, 50\nX = np.column_stack([np.ones(n), np.random.randn(n, p)])\n\n# True sparse model: only 5 features matter\nbeta_true = np.zeros(p + 1)\nbeta_true[:6] = [2.0, 0.5, -0.3, 0.2, -0.1, 0.1]\ny = np.random.poisson(np.exp(X @ beta_true))\n\n# Step 1: Cross-validation\nprint(\"Running cross-validation...\")\ncv_result = rs.cv_glm(y, X, family=\"poisson\", l1_ratio=1.0, cv=5)\nprint(f\"Best \u03b1: {cv_result.alpha_best:.4f} ({cv_result.n_nonzero_best} features)\")\nprint(f\"1-SE \u03b1: {cv_result.alpha_1se:.4f} ({cv_result.n_nonzero_1se} features)\")\n\n# Step 2: Visualize path\nprint(\"\\nCoefficient path:\")\npath = rs.lasso_path(y, X, family=\"poisson\", n_alphas=50)\n# path.plot()\n\n# Step 3: Fit final model with 1-SE rule\nprint(\"\\nFitting final model...\")\nresult = rs.fit_glm(y, X, family=\"poisson\",\n                     alpha=cv_result.alpha_1se, l1_ratio=1.0)\n\n# Step 4: Evaluate\nprint(f\"\\nFinal model:\")\nprint(f\"  Selected features: {result.n_nonzero()} of {p + 1}\")\nprint(f\"  Deviance: {result.deviance:.2f}\")\nprint(f\"  Non-zero indices: {result.selected_features()}\")\n\n# Step 5: Compare to true\nselected = set(result.selected_features())\ntrue_nonzero = set(np.where(beta_true != 0)[0])\nprint(f\"\\nTrue positives: {len(selected &amp; true_nonzero)}\")\nprint(f\"False positives: {len(selected - true_nonzero)}\")\nprint(f\"False negatives: {len(true_nonzero - selected)}\")\n</code></pre>"},{"location":"api/results/","title":"Results Object Reference","text":"<p>This page provides complete documentation for the <code>GLMResults</code> and <code>FormulaGLMResults</code> objects.</p>"},{"location":"api/results/#glmresults","title":"GLMResults","text":"<p>Returned by <code>fit_glm()</code>. Contains all fitted model information.</p>"},{"location":"api/results/#construction","title":"Construction","text":"<pre><code>result = rs.fit_glm(y, X, family=\"poisson\")\n</code></pre>"},{"location":"api/results/#coefficient-access","title":"Coefficient Access","text":""},{"location":"api/results/#params","title":"params","text":"<p>Fitted coefficients as NumPy array.</p> <pre><code>coefficients = result.params\nprint(coefficients)  # array([0.5, 0.3, -0.2])\n</code></pre>"},{"location":"api/results/#coefficients","title":"coefficients","text":"<p>Alias for <code>params</code> (statsmodels compatibility).</p>"},{"location":"api/results/#fitted-values","title":"Fitted Values","text":""},{"location":"api/results/#fittedvalues","title":"fittedvalues","text":"<p>Predicted means \u03bc = g\u207b\u00b9(X\u03b2).</p> <pre><code>predicted = result.fittedvalues\nprint(f\"Mean prediction: {predicted.mean():.4f}\")\n</code></pre>"},{"location":"api/results/#linear_predictor","title":"linear_predictor","text":"<p>Linear predictor \u03b7 = X\u03b2 + offset.</p> <pre><code>eta = result.linear_predictor\n</code></pre>"},{"location":"api/results/#model-information","title":"Model Information","text":""},{"location":"api/results/#deviance","title":"deviance","text":"<p>Total model deviance.</p> <pre><code>print(f\"Deviance: {result.deviance:.2f}\")\n</code></pre>"},{"location":"api/results/#iterations","title":"iterations","text":"<p>Number of IRLS iterations until convergence.</p> <pre><code>print(f\"Converged in {result.iterations} iterations\")\n</code></pre>"},{"location":"api/results/#converged","title":"converged","text":"<p>Whether the algorithm converged.</p> <pre><code>if not result.converged:\n    print(\"Warning: Model did not converge!\")\n</code></pre>"},{"location":"api/results/#nobs","title":"nobs","text":"<p>Number of observations.</p> <pre><code>n = result.nobs\n</code></pre>"},{"location":"api/results/#df_resid","title":"df_resid","text":"<p>Residual degrees of freedom (n - p).</p> <pre><code>df = result.df_resid\n</code></pre>"},{"location":"api/results/#df_model","title":"df_model","text":"<p>Model degrees of freedom (p - 1, excluding intercept).</p> <pre><code>df = result.df_model\n</code></pre>"},{"location":"api/results/#family","title":"family","text":"<p>Family name as string.</p> <pre><code>print(result.family)  # \"Poisson\" or \"NegativeBinomial(theta=2.34)\"\n</code></pre>"},{"location":"api/results/#standard-errors-and-inference","title":"Standard Errors and Inference","text":""},{"location":"api/results/#bse","title":"bse()","text":"<p>Standard errors of coefficients.</p> <pre><code>se = result.bse()\n</code></pre> <p>Formula: SE(\u03b2\u0302) = \u221a(\u03c6 \u00d7 diag((X'WX)\u207b\u00b9))</p>"},{"location":"api/results/#tvalues","title":"tvalues()","text":"<p>z-statistics (or t-statistics).</p> <pre><code>z = result.tvalues()\n</code></pre> <p>Formula: z = \u03b2\u0302 / SE(\u03b2\u0302)</p>"},{"location":"api/results/#pvalues","title":"pvalues()","text":"<p>Two-sided p-values from z-distribution.</p> <pre><code>p = result.pvalues()\nfor i, pval in enumerate(p):\n    if pval &lt; 0.05:\n        print(f\"Coefficient {i} is significant (p={pval:.4f})\")\n</code></pre>"},{"location":"api/results/#conf_int","title":"conf_int()","text":"<p>Confidence intervals for coefficients.</p> <pre><code>lower, upper = result.conf_int(alpha=0.05)  # 95% CI\n</code></pre> <p>Parameters: - <code>alpha</code>: Significance level (default 0.05)</p>"},{"location":"api/results/#significance_codes","title":"significance_codes()","text":"<p>Get significance markers for each coefficient.</p> <pre><code>codes = result.significance_codes()\n# ['***', '**', '*', ''] for p &lt; 0.001, 0.01, 0.05, otherwise\n</code></pre>"},{"location":"api/results/#robust-standard-errors","title":"Robust Standard Errors","text":"<p>Sandwich estimators that are robust to heteroscedasticity.</p>"},{"location":"api/results/#bse_robust","title":"bse_robust()","text":"<p>Robust standard errors.</p> <pre><code>se_robust = result.bse_robust(hc_type=\"HC1\")\n</code></pre> <p>HC Types: - <code>\"HC0\"</code>: White's original estimator - <code>\"HC1\"</code>: With (n/(n-p)) adjustment (default for most software) - <code>\"HC2\"</code>: With leverage adjustment - <code>\"HC3\"</code>: Jackknife-like (most conservative)</p>"},{"location":"api/results/#tvalues_robust","title":"tvalues_robust()","text":"<p>z-statistics using robust SEs.</p> <pre><code>z_robust = result.tvalues_robust(hc_type=\"HC1\")\n</code></pre>"},{"location":"api/results/#pvalues_robust","title":"pvalues_robust()","text":"<p>p-values using robust SEs.</p> <pre><code>p_robust = result.pvalues_robust(hc_type=\"HC1\")\n</code></pre>"},{"location":"api/results/#conf_int_robust","title":"conf_int_robust()","text":"<p>Confidence intervals using robust SEs.</p> <pre><code>lower, upper = result.conf_int_robust(alpha=0.05, hc_type=\"HC1\")\n</code></pre>"},{"location":"api/results/#cov_robust","title":"cov_robust()","text":"<p>Full robust covariance matrix.</p> <pre><code>cov = result.cov_robust(hc_type=\"HC1\")\n</code></pre>"},{"location":"api/results/#covariance-matrices","title":"Covariance Matrices","text":""},{"location":"api/results/#cov_params_unscaled","title":"cov_params_unscaled","text":"<p>Unscaled covariance matrix (X'WX)\u207b\u00b9.</p> <pre><code>cov_unscaled = result.cov_params_unscaled\n</code></pre>"},{"location":"api/results/#cov_params","title":"cov_params()","text":"<p>Scaled covariance matrix \u03c6(X'WX)\u207b\u00b9.</p> <pre><code>cov = result.cov_params()\n</code></pre>"},{"location":"api/results/#residuals","title":"Residuals","text":""},{"location":"api/results/#resid_response","title":"resid_response()","text":"<p>Response residuals: y - \u03bc.</p> <pre><code>r = result.resid_response()\n</code></pre>"},{"location":"api/results/#resid_pearson","title":"resid_pearson()","text":"<p>Pearson residuals: (y - \u03bc) / \u221aV(\u03bc).</p> <pre><code>r = result.resid_pearson()\n# Should be approximately N(0,1) if model is correct\n</code></pre>"},{"location":"api/results/#resid_deviance","title":"resid_deviance()","text":"<p>Deviance residuals: sign(y - \u03bc) \u00d7 \u221ad.</p> <pre><code>r = result.resid_deviance()\n# Sum of squares equals deviance\nprint(f\"Check: {(r**2).sum():.2f} \u2248 {result.deviance:.2f}\")\n</code></pre>"},{"location":"api/results/#resid_working","title":"resid_working()","text":"<p>Working residuals: (y - \u03bc) \u00d7 g'(\u03bc).</p> <pre><code>r = result.resid_working()\n</code></pre>"},{"location":"api/results/#fit-statistics","title":"Fit Statistics","text":""},{"location":"api/results/#llf","title":"llf()","text":"<p>Log-likelihood of the fitted model.</p> <pre><code>ll = result.llf()\n</code></pre>"},{"location":"api/results/#aic","title":"aic()","text":"<p>Akaike Information Criterion.</p> <pre><code>aic = result.aic()\n</code></pre> <p>Formula: AIC = -2 \u00d7 loglik + 2p</p>"},{"location":"api/results/#bic","title":"bic()","text":"<p>Bayesian Information Criterion.</p> <pre><code>bic = result.bic()\n</code></pre> <p>Formula: BIC = -2 \u00d7 loglik + p \u00d7 log(n)</p>"},{"location":"api/results/#null_deviance","title":"null_deviance()","text":"<p>Deviance of intercept-only model.</p> <pre><code>null_dev = result.null_deviance()\npseudo_r2 = 1 - result.deviance / null_dev\n</code></pre>"},{"location":"api/results/#pearson_chi2","title":"pearson_chi2()","text":"<p>Pearson chi-squared statistic.</p> <pre><code>chi2 = result.pearson_chi2()\n</code></pre> <p>Formula: \u03a3 (y - \u03bc)\u00b2 / V(\u03bc)</p>"},{"location":"api/results/#scale","title":"scale()","text":"<p>Dispersion parameter (deviance-based).</p> <pre><code>phi = result.scale()\n</code></pre> <p>For Poisson/Binomial: Always 1 For QuasiPoisson/QuasiBinomial: Estimated from Pearson residuals For Gaussian/Gamma: Deviance / df_resid</p>"},{"location":"api/results/#scale_pearson","title":"scale_pearson()","text":"<p>Dispersion parameter (Pearson-based).</p> <pre><code>phi = result.scale_pearson()\n</code></pre> <p>Formula: Pearson \u03c7\u00b2 / df_resid</p>"},{"location":"api/results/#regularization-methods","title":"Regularization Methods","text":""},{"location":"api/results/#n_nonzero","title":"n_nonzero()","text":"<p>Number of non-zero coefficients (for regularized models).</p> <pre><code>result = rs.fit_glm(y, X, alpha=0.1, l1_ratio=1.0)\nprint(f\"Selected {result.n_nonzero()} of {len(result.params)} features\")\n</code></pre>"},{"location":"api/results/#selected_features","title":"selected_features()","text":"<p>Indices of non-zero coefficients.</p> <pre><code>indices = result.selected_features()\nprint(f\"Selected features: {indices}\")\n</code></pre>"},{"location":"api/results/#diagnostics-integration","title":"Diagnostics Integration","text":""},{"location":"api/results/#diagnostics","title":"diagnostics()","text":"<p>Compute comprehensive diagnostics.</p> <pre><code>diag = result.diagnostics(\n    data=data,\n    categorical_factors=[\"region\"],\n    continuous_factors=[\"age\"],\n)\n</code></pre>"},{"location":"api/results/#diagnostics_json","title":"diagnostics_json()","text":"<p>Get diagnostics as JSON string.</p> <pre><code>json_str = result.diagnostics_json(\n    data=data,\n    categorical_factors=[\"region\"],\n)\n</code></pre>"},{"location":"api/results/#formulaglmresults","title":"FormulaGLMResults","text":"<p>Extends <code>GLMResults</code> with formula-specific functionality.</p>"},{"location":"api/results/#feature_names","title":"feature_names","text":"<p>List of feature names.</p> <pre><code>names = result.feature_names\n# ['Intercept', 'x1', 'C(region)_B', 'C(region)_C']\n</code></pre>"},{"location":"api/results/#summary","title":"summary()","text":"<p>Print formatted summary table.</p> <pre><code>print(result.summary())\n</code></pre>"},{"location":"api/results/#coef_table","title":"coef_table()","text":"<p>Coefficients as Polars DataFrame.</p> <pre><code>df = result.coef_table()\n</code></pre>"},{"location":"api/results/#relativities","title":"relativities()","text":"<p>Multiplicative effects (exp(coef)).</p> <pre><code>rel = result.relativities()\n</code></pre>"},{"location":"api/results/#relativities_table","title":"relativities_table()","text":"<p>Relativities as Polars DataFrame.</p> <pre><code>df = result.relativities_table()\n</code></pre>"},{"location":"api/results/#predict","title":"predict()","text":"<p>Predict on new data.</p> <pre><code>predictions = result.predict(new_data)\npredictions_link = result.predict(new_data, type=\"link\")\n</code></pre>"},{"location":"architecture/data-flow/","title":"Data Flow","text":"<p>This chapter traces data through the system from user input to final results, helping you understand where transformations happen and how to debug issues.</p>"},{"location":"architecture/data-flow/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 User Input                                                       \u2502\n\u2502   \u2022 NumPy arrays (y, X)                                         \u2502\n\u2502   \u2022 Or: DataFrame + formula string                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Python API Layer                                                 \u2502\n\u2502   \u2022 Validate inputs                                              \u2502\n\u2502   \u2022 Parse formula (if used)                                      \u2502\n\u2502   \u2022 Build design matrix                                          \u2502\n\u2502   \u2022 Convert to NumPy arrays                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PyO3 Binding Layer                                               \u2502\n\u2502   \u2022 Convert NumPy \u2192 ndarray                                      \u2502\n\u2502   \u2022 Create Rust Family/Link objects                              \u2502\n\u2502   \u2022 Call core library                                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Rust Core (rustystats-core)                                      \u2502\n\u2502   \u2022 Run IRLS/Coordinate Descent                                  \u2502\n\u2502   \u2022 Return IRLSResult                                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PyO3 Binding Layer                                               \u2502\n\u2502   \u2022 Wrap IRLSResult as PyGLMResults                             \u2502\n\u2502   \u2022 Convert ndarray \u2192 NumPy (when accessed)                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 User Output                                                      \u2502\n\u2502   \u2022 GLMResults object with all methods                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/data-flow/#path-1-array-api","title":"Path 1: Array API","text":""},{"location":"architecture/data-flow/#user-code","title":"User Code","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\ny = np.array([1, 2, 3, 4, 5], dtype=np.float64)\nX = np.column_stack([np.ones(5), [1, 2, 3, 4, 5]])\n\nresult = rs.fit_glm(y, X, family=\"poisson\")\n</code></pre>"},{"location":"architecture/data-flow/#step-1-python-api-pythonrustystatsglmpy","title":"Step 1: Python API (<code>python/rustystats/glm.py</code>)","text":"<pre><code>def fit_glm(y, X, family=\"gaussian\", link=None, offset=None, \n            weights=None, alpha=0.0, l1_ratio=1.0, ...):\n\n    # 1. Convert to numpy arrays\n    y = np.asarray(y, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n\n    # 2. Validate dimensions\n    if y.ndim != 1:\n        raise ValueError(\"y must be 1-dimensional\")\n    if X.ndim != 2:\n        raise ValueError(\"X must be 2-dimensional\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"y and X must have same number of rows\")\n\n    # 3. Call Rust binding\n    from rustystats._rustystats import fit_glm as _fit_glm\n    rust_result = _fit_glm(y, X, family, alpha, l1_ratio, offset, weights)\n\n    # 4. Return wrapped result\n    return GLMResults(rust_result)\n</code></pre>"},{"location":"architecture/data-flow/#step-2-pyo3-binding-cratesrustystatssrclibrs","title":"Step 2: PyO3 Binding (<code>crates/rustystats/src/lib.rs</code>)","text":"<pre><code>#[pyfunction]\n#[pyo3(signature = (y, x, family=\"gaussian\", alpha=0.0, l1_ratio=1.0, offset=None, weights=None))]\nfn fit_glm&lt;'py&gt;(\n    py: Python&lt;'py&gt;,\n    y: PyReadonlyArray1&lt;f64&gt;,\n    x: PyReadonlyArray2&lt;f64&gt;,\n    family: &amp;str,\n    alpha: f64,\n    l1_ratio: f64,\n    offset: Option&lt;PyReadonlyArray1&lt;f64&gt;&gt;,\n    weights: Option&lt;PyReadonlyArray1&lt;f64&gt;&gt;,\n) -&gt; PyResult&lt;PyGLMResults&gt; {\n\n    // 1. Convert NumPy to ndarray (owned copies)\n    let y_arr = y.as_array().to_owned();\n    let x_arr = x.as_array().to_owned();\n    let offset_arr = offset.map(|o| o.as_array().to_owned());\n    let weights_arr = weights.map(|w| w.as_array().to_owned());\n\n    // 2. Create Family and Link objects\n    let (family_obj, link_obj): (Box&lt;dyn Family&gt;, Box&lt;dyn Link&gt;) = \n        match family {\n            \"gaussian\" =&gt; (Box::new(GaussianFamily), Box::new(IdentityLink)),\n            \"poisson\" =&gt; (Box::new(PoissonFamily), Box::new(LogLink)),\n            // ... other families\n        };\n\n    // 3. Release GIL and call core library\n    let result = py.allow_threads(|| {\n        if alpha &gt; 0.0 {\n            fit_glm_regularized(...)\n        } else {\n            fit_glm_full(...)\n        }\n    })?;\n\n    // 4. Wrap result\n    Ok(PyGLMResults::from_irls_result(result, family))\n}\n</code></pre>"},{"location":"architecture/data-flow/#step-3-rust-core-cratesrustystats-coresrcsolversirlsrs","title":"Step 3: Rust Core (<code>crates/rustystats-core/src/solvers/irls.rs</code>)","text":"<pre><code>pub fn fit_glm_full(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,\n    link: &amp;dyn Link,\n    config: &amp;IRLSConfig,\n    offset: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; Result&lt;IRLSResult&gt; {\n\n    // 1. Initialize\n    let n = y.len();\n    let p = x.ncols();\n    let mut mu = family.initialize_mu(y);\n\n    // 2. IRLS iterations\n    for iter in 0..config.max_iterations {\n        // Compute weights, working response\n        // Solve WLS\n        // Update predictions\n        // Check convergence\n    }\n\n    // 3. Return result\n    Ok(IRLSResult {\n        coefficients,\n        fitted_values: mu,\n        deviance,\n        converged,\n        // ...\n    })\n}\n</code></pre>"},{"location":"architecture/data-flow/#step-4-return-path","title":"Step 4: Return Path","text":"<p>The <code>IRLSResult</code> is wrapped as <code>PyGLMResults</code> and returned to Python. Arrays are converted to NumPy only when accessed:</p> <pre><code>// In PyGLMResults\n#[getter]\nfn params&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n    // Conversion happens here, when user accesses .params\n    self.coefficients.clone().into_pyarray_bound(py)\n}\n</code></pre>"},{"location":"architecture/data-flow/#path-2-formula-api","title":"Path 2: Formula API","text":""},{"location":"architecture/data-flow/#user-code_1","title":"User Code","text":"<pre><code>import rustystats as rs\nimport polars as pl\n\ndata = pl.read_parquet(\"insurance.parquet\")\n\nresult = rs.glm(\n    formula=\"ClaimNb ~ Age + C(Region) + bs(VehPower, df=4)\",\n    data=data,\n    family=\"poisson\",\n    offset=\"Exposure\"\n).fit()\n</code></pre>"},{"location":"architecture/data-flow/#step-1-formula-parsing-pythonrustystatsformulapy","title":"Step 1: Formula Parsing (<code>python/rustystats/formula.py</code>)","text":"<pre><code>class FormulaGLM:\n    def __init__(self, formula, data, family, offset=None, ...):\n        self.formula = formula\n        self.data = data\n\n        # Parse formula\n        self.parsed = parse_formula(formula)\n        # parsed = {\n        #     'response': 'ClaimNb',\n        #     'terms': ['Age', 'C(Region)', 'bs(VehPower, df=4)'],\n        #     'interactions': []\n        # }\n\n    def fit(self):\n        # 1. Extract response\n        y = self.data[self.parsed['response']].to_numpy()\n\n        # 2. Build design matrix\n        X = self._build_design_matrix()\n\n        # 3. Handle offset\n        offset = None\n        if self.offset_col:\n            offset = np.log(self.data[self.offset_col].to_numpy())\n\n        # 4. Call array API\n        result = fit_glm(y, X, family=self.family, offset=offset, ...)\n\n        # 5. Attach metadata\n        result._feature_names = self._feature_names\n        return result\n</code></pre>"},{"location":"architecture/data-flow/#step-2-design-matrix-construction","title":"Step 2: Design Matrix Construction","text":"<pre><code>def _build_design_matrix(self):\n    columns = [np.ones(len(self.data))]  # Intercept\n    names = ['Intercept']\n\n    for term in self.parsed['terms']:\n        if term.startswith('C('):\n            # Categorical encoding\n            col_name = extract_column_name(term)\n            encoded, term_names = encode_categorical(\n                self.data[col_name].to_numpy()\n            )\n            columns.append(encoded)\n            names.extend(term_names)\n\n        elif term.startswith('bs(') or term.startswith('ns('):\n            # Spline basis\n            col_name, df = parse_spline_term(term)\n            x = self.data[col_name].to_numpy()\n            basis = bs(x, df=df) if term.startswith('bs') else ns(x, df=df)\n            columns.append(basis)\n            names.extend([f\"{term}_{i}\" for i in range(basis.shape[1])])\n\n        else:\n            # Continuous variable\n            columns.append(self.data[term].to_numpy().reshape(-1, 1))\n            names.append(term)\n\n    self._feature_names = names\n    return np.column_stack(columns)\n</code></pre>"},{"location":"architecture/data-flow/#subsequent-steps","title":"Subsequent Steps","text":"<p>After design matrix construction, the flow follows the Array API path.</p>"},{"location":"architecture/data-flow/#data-type-conversions","title":"Data Type Conversions","text":""},{"location":"architecture/data-flow/#python-rust","title":"Python \u2192 Rust","text":"Python Type Rust Type Notes <code>np.ndarray</code> (1D, float64) <code>Array1&lt;f64&gt;</code> Via <code>PyReadonlyArray1</code> <code>np.ndarray</code> (2D, float64) <code>Array2&lt;f64&gt;</code> Via <code>PyReadonlyArray2</code> <code>str</code> <code>&amp;str</code> Borrowed reference <code>float</code> <code>f64</code> Direct conversion <code>int</code> <code>i64</code> or <code>usize</code> Direct conversion <code>None</code> <code>None</code> (Option) For optional params"},{"location":"architecture/data-flow/#rust-python","title":"Rust \u2192 Python","text":"Rust Type Python Type Notes <code>Array1&lt;f64&gt;</code> <code>np.ndarray</code> (1D) Via <code>into_pyarray_bound</code> <code>Array2&lt;f64&gt;</code> <code>np.ndarray</code> (2D) Via <code>into_pyarray_bound</code> <code>String</code> or <code>&amp;str</code> <code>str</code> Automatic <code>f64</code> <code>float</code> Automatic <code>usize</code> <code>int</code> Automatic <code>bool</code> <code>bool</code> Automatic"},{"location":"architecture/data-flow/#memory-considerations","title":"Memory Considerations","text":""},{"location":"architecture/data-flow/#copies-vs-views","title":"Copies vs Views","text":"<p>View (no copy): <pre><code>let y_view = y.as_array();  // Borrows NumPy memory\n</code></pre></p> <p>Copy (owns data): <pre><code>let y_owned = y.as_array().to_owned();  // Copies to Rust memory\n</code></pre></p>"},{"location":"architecture/data-flow/#when-copies-happen","title":"When Copies Happen","text":"<ol> <li>Python \u2192 Rust: Always copied before IRLS (Rust needs ownership for parallel processing)</li> <li>Inside Rust: Minimal copies; use views and in-place operations</li> <li>Rust \u2192 Python: Copied when returning arrays (NumPy takes ownership)</li> </ol>"},{"location":"architecture/data-flow/#large-dataset-considerations","title":"Large Dataset Considerations","text":"<p>For large datasets: - Design matrix is the largest object - IRLS stores: X, y, \u03bc, \u03b7, weights, covariance matrix - Regularization additionally stores the Gram matrix (p \u00d7 p)</p> <p>Memory estimate: ~5\u00d7 the size of the design matrix during fitting.</p>"},{"location":"architecture/data-flow/#tracing-issues","title":"Tracing Issues","text":""},{"location":"architecture/data-flow/#debug-points","title":"Debug Points","text":"<ol> <li>Python input validation: Add prints in <code>fit_glm()</code></li> <li>Design matrix: Check <code>X.shape</code>, <code>X.dtype</code>, look for NaN/Inf</li> <li>Rust entry: Add <code>println!</code> in the binding function</li> <li>IRLS iterations: Enable <code>verbose=True</code> in config</li> </ol>"},{"location":"architecture/data-flow/#common-issues","title":"Common Issues","text":"Symptom Likely Cause Debug Step Type error Wrong dtype Check <code>y.dtype</code>, <code>X.dtype</code> Dimension error Shape mismatch Print shapes at each step Convergence failure Data issues Check for outliers, collinearity NaN in results Numerical overflow Check input ranges, add clamping"},{"location":"architecture/data-flow/#example-debug-session","title":"Example Debug Session","text":"<pre><code># Enable verbose IRLS\nresult = rs.fit_glm(y, X, family=\"poisson\", verbose=True)\n\n# Check intermediate values\nprint(f\"y range: [{y.min()}, {y.max()}]\")\nprint(f\"X shape: {X.shape}\")\nprint(f\"Any NaN in X: {np.isnan(X).any()}\")\nprint(f\"X condition number: {np.linalg.cond(X)}\")\n</code></pre>"},{"location":"architecture/data-flow/#performance-profiling","title":"Performance Profiling","text":""},{"location":"architecture/data-flow/#python-level","title":"Python Level","text":"<pre><code>import time\n\nstart = time.time()\nresult = rs.fit_glm(y, X, family=\"poisson\")\nprint(f\"Total time: {time.time() - start:.3f}s\")\nprint(f\"IRLS iterations: {result.iterations}\")\n</code></pre>"},{"location":"architecture/data-flow/#rust-level","title":"Rust Level","text":"<pre><code># Build with profiling\ncargo build --release -p rustystats-core\n\n# Use perf or flamegraph\nperf record -g ./target/release/benchmark\nperf report\n</code></pre>"},{"location":"architecture/data-flow/#key-bottlenecks","title":"Key Bottlenecks","text":"<ol> <li>X'WX computation: O(np\u00b2) - parallelized</li> <li>Matrix solve: O(p\u00b3) - uses optimized BLAS via nalgebra</li> <li>Prediction updates: O(np) - parallelized</li> </ol> <p>For large n, the bottleneck is usually X'WX. For large p, matrix inversion dominates.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>RustyStats is a hybrid Rust/Python library. This chapter explains how the components fit together and the design principles behind the architecture.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Python User Code                          \u2502\n\u2502         import rustystats as rs; rs.glm(...).fit()              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Python API Layer                              \u2502\n\u2502              python/rustystats/*.py                              \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 glm.py  \u2502 \u2502formula.py\u2502 \u2502splines.py\u2502 \u2502diagnostics\u2502             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502           \u2502           \u2502            \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    PyO3 Bindings Layer                           \u2502\n\u2502              crates/rustystats/src/lib.rs                        \u2502\n\u2502   Converts Python objects \u2194 Rust types using NumPy/PyO3         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Rust Core Library                             \u2502\n\u2502              crates/rustystats-core/src/                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502families \u2502 \u2502 links   \u2502 \u2502 solvers \u2502 \u2502inference \u2502             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502   \u2502 splines \u2502 \u2502 formula \u2502 \u2502design_mx\u2502 \u2502diagnostics\u2502             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":""},{"location":"architecture/overview/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>The codebase is split into three layers:</p> Layer Location Responsibility Python API <code>python/rustystats/</code> User-facing interface, DataFrame handling PyO3 Bindings <code>crates/rustystats/</code> Type conversion, Python \u2194 Rust bridge Rust Core <code>crates/rustystats-core/</code> All mathematical computation"},{"location":"architecture/overview/#2-pure-rust-core","title":"2. Pure Rust Core","text":"<p><code>rustystats-core</code> has no Python dependencies. It's a pure Rust library that could be used independently. Benefits:</p> <ul> <li>Testable without Python</li> <li>Could support other language bindings (R, Julia)</li> <li>Clear API boundary</li> </ul>"},{"location":"architecture/overview/#3-minimal-python-dependencies","title":"3. Minimal Python Dependencies","text":"<p>The Python layer requires only <code>numpy</code>. Optional dependencies (<code>polars</code>) are imported lazily.</p>"},{"location":"architecture/overview/#4-trait-based-extensibility","title":"4. Trait-Based Extensibility","text":"<p>Core abstractions use Rust traits:</p> <pre><code>pub trait Family: Send + Sync { ... }\npub trait Link: Send + Sync { ... }\n</code></pre> <p>New families/links can be added by implementing these traits.</p>"},{"location":"architecture/overview/#5-parallel-by-default","title":"5. Parallel by Default","text":"<p>Computationally intensive operations use Rayon for automatic parallelization:</p> <pre><code>use rayon::prelude::*;\n\n// Parallel matrix multiplication\nlet result = (0..n).into_par_iter()\n    .fold(|| init, |acc, i| compute(acc, i))\n    .reduce(|| init, |a, b| combine(a, b));\n</code></pre>"},{"location":"architecture/overview/#crate-structure","title":"Crate Structure","text":""},{"location":"architecture/overview/#rustystats-core","title":"rustystats-core","text":"<p>The pure Rust computation library:</p> <pre><code>crates/rustystats-core/\n\u251c\u2500\u2500 Cargo.toml\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 lib.rs              # Re-exports, module declarations\n    \u251c\u2500\u2500 error.rs            # Error types\n    \u251c\u2500\u2500 families/           # Distribution families\n    \u2502   \u251c\u2500\u2500 mod.rs          # Family trait\n    \u2502   \u251c\u2500\u2500 gaussian.rs\n    \u2502   \u251c\u2500\u2500 poisson.rs\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 links/              # Link functions\n    \u2502   \u251c\u2500\u2500 mod.rs          # Link trait\n    \u2502   \u251c\u2500\u2500 identity.rs\n    \u2502   \u251c\u2500\u2500 log.rs\n    \u2502   \u2514\u2500\u2500 logit.rs\n    \u251c\u2500\u2500 solvers/            # Fitting algorithms\n    \u2502   \u251c\u2500\u2500 mod.rs\n    \u2502   \u251c\u2500\u2500 irls.rs         # Main IRLS solver\n    \u2502   \u2514\u2500\u2500 coordinate_descent.rs\n    \u251c\u2500\u2500 inference/          # Statistical inference\n    \u2502   \u2514\u2500\u2500 mod.rs          # SEs, p-values, robust SEs\n    \u251c\u2500\u2500 diagnostics/        # Model diagnostics\n    \u2502   \u251c\u2500\u2500 mod.rs\n    \u2502   \u251c\u2500\u2500 residuals.rs\n    \u2502   \u251c\u2500\u2500 calibration.rs\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 splines/            # Spline basis functions\n    \u2502   \u2514\u2500\u2500 mod.rs\n    \u251c\u2500\u2500 formula/            # Formula parsing\n    \u2502   \u2514\u2500\u2500 mod.rs\n    \u251c\u2500\u2500 design_matrix/      # Design matrix construction\n    \u2502   \u2514\u2500\u2500 mod.rs\n    \u251c\u2500\u2500 target_encoding/    # Target encoding\n    \u2502   \u2514\u2500\u2500 mod.rs\n    \u251c\u2500\u2500 regularization/     # Penalty configuration\n    \u2502   \u2514\u2500\u2500 mod.rs\n    \u2514\u2500\u2500 interactions/       # Interaction terms\n        \u2514\u2500\u2500 mod.rs\n</code></pre>"},{"location":"architecture/overview/#rustystats-python-bindings","title":"rustystats (Python bindings)","text":"<p>The PyO3 bridge:</p> <pre><code>crates/rustystats/\n\u251c\u2500\u2500 Cargo.toml\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 lib.rs              # All Python-facing code\n</code></pre> <p>This single file: - Wraps Rust types as Python classes (<code>#[pyclass]</code>) - Exposes functions to Python (<code>#[pyfunction]</code>) - Handles NumPy array conversion</p>"},{"location":"architecture/overview/#python-package","title":"Python Package","text":"<p>High-level Python API:</p> <pre><code>python/rustystats/\n\u251c\u2500\u2500 __init__.py             # Public exports\n\u251c\u2500\u2500 glm.py                  # GLM class, fit_glm()\n\u251c\u2500\u2500 formula.py              # Formula API, glm()\n\u251c\u2500\u2500 families.py             # Python family wrappers\n\u251c\u2500\u2500 links.py                # Python link wrappers\n\u251c\u2500\u2500 splines.py              # bs(), ns() functions\n\u251c\u2500\u2500 target_encoding.py      # target_encode(), TargetEncoder\n\u251c\u2500\u2500 selection.py            # lasso_path(), cv_glm()\n\u2514\u2500\u2500 diagnostics.py          # ModelDiagnostics, explore_data()\n</code></pre>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#fitting-a-model","title":"Fitting a Model","text":"<pre><code>User calls rs.fit_glm(y, X, family=\"poisson\")\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 python/rustystats/glm.py: fit_glm()      \u2502\n\u2502 - Parse family string \u2192 Family object    \u2502\n\u2502 - Validate inputs                         \u2502\n\u2502 - Call Rust via _rustystats.fit_glm()    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 crates/rustystats/src/lib.rs             \u2502\n\u2502 - Convert PyArray \u2192 ndarray::Array       \u2502\n\u2502 - Create Rust Family/Link objects        \u2502\n\u2502 - Call fit_glm_full()                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 crates/rustystats-core/src/solvers/      \u2502\n\u2502 - Run IRLS iterations                    \u2502\n\u2502 - Compute X'WX, X'Wz (parallel)          \u2502\n\u2502 - Solve linear system                    \u2502\n\u2502 - Return IRLSResult                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Back to Python                           \u2502\n\u2502 - Wrap IRLSResult as PyGLMResults        \u2502\n\u2502 - Convert arrays back to NumPy           \u2502\n\u2502 - Return GLMResults to user              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#formula-api-flow","title":"Formula API Flow","text":"<pre><code>User calls rs.glm(\"y ~ x1 + C(cat)\", data).fit()\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 python/rustystats/formula.py             \u2502\n\u2502 - Parse formula string                   \u2502\n\u2502 - Extract columns from DataFrame         \u2502\n\u2502 - Build design matrix                    \u2502\n\u2502 - Handle categoricals, splines, etc.     \u2502\n\u2502 - Call fit_glm() with arrays             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n        (Same as array API from here)\n</code></pre>"},{"location":"architecture/overview/#error-handling","title":"Error Handling","text":""},{"location":"architecture/overview/#rust-errors","title":"Rust Errors","text":"<p>Custom error type with context:</p> <pre><code>#[derive(Debug)]\npub enum RustyStatsError {\n    InvalidInput(String),\n    ConvergenceFailure { iterations: usize, tolerance: f64 },\n    NumericalError(String),\n    DimensionMismatch { expected: usize, got: usize },\n}\n</code></pre>"},{"location":"architecture/overview/#python-errors","title":"Python Errors","text":"<p>Rust errors are converted to Python exceptions:</p> <pre><code>impl From&lt;RustyStatsError&gt; for PyErr {\n    fn from(err: RustyStatsError) -&gt; PyErr {\n        PyValueError::new_err(format!(\"{}\", err))\n    }\n}\n</code></pre>"},{"location":"architecture/overview/#memory-management","title":"Memory Management","text":""},{"location":"architecture/overview/#zero-copy-when-possible","title":"Zero-Copy When Possible","text":"<p>NumPy arrays can be viewed without copying:</p> <pre><code>fn process(arr: PyReadonlyArray1&lt;f64&gt;) -&gt; ... {\n    let view = arr.as_array();  // No copy, just a view\n    // ... work with view\n}\n</code></pre>"},{"location":"architecture/overview/#copies-when-necessary","title":"Copies When Necessary","text":"<p>When Rust needs ownership or the array will be modified:</p> <pre><code>let owned = arr.as_array().to_owned();  // Copy to owned Array\n</code></pre>"},{"location":"architecture/overview/#returning-to-python","title":"Returning to Python","text":"<p>Arrays are converted back to NumPy:</p> <pre><code>result.into_pyarray_bound(py)  // Moves ownership to Python\n</code></pre>"},{"location":"architecture/overview/#thread-safety","title":"Thread Safety","text":""},{"location":"architecture/overview/#rust-side","title":"Rust Side","text":"<p>All traits require <code>Send + Sync</code>:</p> <pre><code>pub trait Family: Send + Sync { ... }\n</code></pre> <p>This allows parallel iteration with Rayon.</p>"},{"location":"architecture/overview/#python-gil","title":"Python GIL","text":"<p>PyO3 handles the GIL automatically. Rust code releases the GIL during computation:</p> <pre><code>py.allow_threads(|| {\n    // This code runs without the GIL\n    // Python can run other threads\n    expensive_computation()\n})\n</code></pre>"},{"location":"architecture/overview/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture/overview/#rust-unit-tests","title":"Rust Unit Tests","text":"<p>Each module has tests:</p> <pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_variance_function() {\n        let family = PoissonFamily;\n        let mu = array![1.0, 2.0, 3.0];\n        let var = family.variance(&amp;mu);\n        assert_eq!(var, mu);  // Poisson: V(\u03bc) = \u03bc\n    }\n}\n</code></pre> <p>Run with: <code>cargo test -p rustystats-core</code></p>"},{"location":"architecture/overview/#python-integration-tests","title":"Python Integration Tests","text":"<p>Located in <code>tests/python/</code>:</p> <pre><code>def test_poisson_fit():\n    y = np.random.poisson(5, 100)\n    X = np.column_stack([np.ones(100), np.random.randn(100)])\n    result = rs.fit_glm(y, X, family=\"poisson\")\n    assert result.converged\n</code></pre> <p>Run with: <code>uv run pytest tests/python/ -v</code></p>"},{"location":"architecture/overview/#comparison-tests","title":"Comparison Tests","text":"<p>Compare against statsmodels:</p> <pre><code>def test_vs_statsmodels():\n    # Fit with RustyStats\n    rs_result = rs.fit_glm(y, X, family=\"gaussian\")\n\n    # Fit with statsmodels\n    sm_result = sm.GLM(y, X, family=sm.families.Gaussian()).fit()\n\n    # Compare coefficients\n    np.testing.assert_allclose(rs_result.params, sm_result.params, rtol=1e-5)\n</code></pre>"},{"location":"architecture/overview/#build-system","title":"Build System","text":""},{"location":"architecture/overview/#maturin","title":"Maturin","text":"<p>The Python package is built using maturin:</p> <pre><code># pyproject.toml\n[build-system]\nrequires = [\"maturin&gt;=1.4\"]\nbuild-backend = \"maturin\"\n\n[tool.maturin]\nmanifest-path = \"crates/rustystats/Cargo.toml\"\npython-source = \"python\"\nmodule-name = \"rustystats._rustystats\"\n</code></pre>"},{"location":"architecture/overview/#development-workflow","title":"Development Workflow","text":"<pre><code># Compile Rust and install Python package\nmaturin develop\n\n# Release build (optimized)\nmaturin develop --release\n\n# Run tests\ncargo test                          # Rust tests\nuv run pytest tests/python/ -v      # Python tests\n</code></pre>"},{"location":"architecture/python-bindings/","title":"Python Bindings","text":"<p>The <code>rustystats</code> crate provides the bridge between Python and the Rust core library using PyO3. This chapter explains how the bindings work and how to extend them.</p>"},{"location":"architecture/python-bindings/#pyo3-overview","title":"PyO3 Overview","text":"<p>PyO3 is a Rust crate that enables: - Calling Rust code from Python - Calling Python code from Rust - Converting between Python and Rust types</p>"},{"location":"architecture/python-bindings/#key-concepts","title":"Key Concepts","text":"Concept Description <code>#[pymodule]</code> Defines a Python module <code>#[pyclass]</code> Makes a Rust struct a Python class <code>#[pymethods]</code> Exposes methods to Python <code>#[pyfunction]</code> Exposes a standalone function <code>#[getter]</code> Makes a method a property getter"},{"location":"architecture/python-bindings/#module-structure","title":"Module Structure","text":"<p>All bindings are in a single file:</p> <pre><code>crates/rustystats/src/lib.rs\n</code></pre> <p>The file is organized into sections: 1. Link function wrappers 2. Family wrappers 3. GLMResults class 4. Standalone functions 5. Module registration</p>"},{"location":"architecture/python-bindings/#the-module-entry-point","title":"The Module Entry Point","text":"<pre><code>#[pymodule]\nfn _rustystats(m: &amp;Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    // Register classes\n    m.add_class::&lt;PyIdentityLink&gt;()?;\n    m.add_class::&lt;PyLogLink&gt;()?;\n    m.add_class::&lt;PyLogitLink&gt;()?;\n\n    m.add_class::&lt;PyGaussianFamily&gt;()?;\n    m.add_class::&lt;PyPoissonFamily&gt;()?;\n    m.add_class::&lt;PyBinomialFamily&gt;()?;\n    m.add_class::&lt;PyGammaFamily&gt;()?;\n    m.add_class::&lt;PyTweedieFamily&gt;()?;\n    m.add_class::&lt;PyQuasiPoissonFamily&gt;()?;\n    m.add_class::&lt;PyQuasiBinomialFamily&gt;()?;\n    m.add_class::&lt;PyNegativeBinomialFamily&gt;()?;\n\n    m.add_class::&lt;PyGLMResults&gt;()?;\n\n    // Register functions\n    m.add_function(wrap_pyfunction!(fit_glm, m)?)?;\n    m.add_function(wrap_pyfunction!(fit_glm_regularized, m)?)?;\n    // ... more functions\n\n    Ok(())\n}\n</code></pre> <p>The module is named <code>_rustystats</code> (with underscore) and exposed as <code>rustystats._rustystats</code> in Python. The Python package imports from this and re-exports.</p>"},{"location":"architecture/python-bindings/#wrapping-rust-types","title":"Wrapping Rust Types","text":""},{"location":"architecture/python-bindings/#pattern-inner-type-wrapper","title":"Pattern: Inner Type Wrapper","text":"<p>Each Rust type is wrapped in a Python-facing struct:</p> <pre><code>// The Rust type (from rustystats-core)\nuse rustystats_core::families::PoissonFamily;\n\n// The Python wrapper\n#[pyclass(name = \"PoissonFamily\")]\n#[derive(Clone)]\npub struct PyPoissonFamily {\n    inner: PoissonFamily,  // Holds the actual Rust type\n}\n\n#[pymethods]\nimpl PyPoissonFamily {\n    #[new]\n    fn new() -&gt; Self {\n        Self { inner: PoissonFamily }\n    }\n\n    fn name(&amp;self) -&gt; &amp;str {\n        self.inner.name()\n    }\n\n    fn variance&lt;'py&gt;(\n        &amp;self,\n        py: Python&lt;'py&gt;,\n        mu: PyReadonlyArray1&lt;f64&gt;\n    ) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        let mu_array = mu.as_array().to_owned();\n        let result = self.inner.variance(&amp;mu_array);\n        result.into_pyarray_bound(py)\n    }\n    // ... more methods\n}\n</code></pre>"},{"location":"architecture/python-bindings/#why-this-pattern","title":"Why This Pattern?","text":"<ol> <li>Separation: Python concerns stay in the wrapper</li> <li>Type conversion: Handle NumPy \u2194 ndarray conversion</li> <li>Safety: PyO3 requirements (Clone, etc.) don't affect core types</li> </ol>"},{"location":"architecture/python-bindings/#numpy-integration","title":"NumPy Integration","text":""},{"location":"architecture/python-bindings/#reading-arrays-from-python","title":"Reading Arrays from Python","text":"<pre><code>use numpy::{PyReadonlyArray1, PyReadonlyArray2};\n\nfn process(\n    y: PyReadonlyArray1&lt;f64&gt;,  // Read-only 1D array\n    x: PyReadonlyArray2&lt;f64&gt;,  // Read-only 2D array\n) {\n    // Get a view (no copy)\n    let y_view = y.as_array();\n\n    // Get an owned copy (when needed)\n    let y_owned = y.as_array().to_owned();\n}\n</code></pre>"},{"location":"architecture/python-bindings/#returning-arrays-to-python","title":"Returning Arrays to Python","text":"<pre><code>use numpy::{IntoPyArray, PyArray1, PyArray2};\n\nfn compute&lt;'py&gt;(py: Python&lt;'py&gt;, n: usize) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n    let result: Array1&lt;f64&gt; = Array1::zeros(n);\n    result.into_pyarray_bound(py)  // Transfer ownership to Python\n}\n</code></pre>"},{"location":"architecture/python-bindings/#the-lifetime-parameter","title":"The Lifetime Parameter","text":"<p>The <code>'py</code> lifetime ties returned objects to the Python interpreter:</p> <pre><code>fn method&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n    // The returned PyArray1 lives as long as 'py (the Python session)\n}\n</code></pre>"},{"location":"architecture/python-bindings/#the-glmresults-class","title":"The GLMResults Class","text":"<p>This is the most complex wrapper, providing all inference methods:</p> <pre><code>#[pyclass(name = \"GLMResults\")]\n#[derive(Clone)]\npub struct PyGLMResults {\n    coefficients: Array1&lt;f64&gt;,\n    fitted_values: Array1&lt;f64&gt;,\n    linear_predictor: Array1&lt;f64&gt;,\n    deviance: f64,\n    iterations: usize,\n    converged: bool,\n    covariance_unscaled: Array2&lt;f64&gt;,\n    n_obs: usize,\n    n_params: usize,\n    y: Array1&lt;f64&gt;,\n    family_name: String,\n    prior_weights: Array1&lt;f64&gt;,\n    penalty: Penalty,\n    design_matrix: Array2&lt;f64&gt;,\n    irls_weights: Array1&lt;f64&gt;,\n}\n\n#[pymethods]\nimpl PyGLMResults {\n    // Properties (getters)\n    #[getter]\n    fn params&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        self.coefficients.clone().into_pyarray_bound(py)\n    }\n\n    #[getter]\n    fn fittedvalues&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        self.fitted_values.clone().into_pyarray_bound(py)\n    }\n\n    #[getter]\n    fn deviance(&amp;self) -&gt; f64 {\n        self.deviance\n    }\n\n    // Methods\n    fn bse&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        let scale = self.scale();\n        let se: Array1&lt;f64&gt; = (0..self.n_params)\n            .map(|i| (scale * self.covariance_unscaled[[i, i]]).sqrt())\n            .collect();\n        se.into_pyarray_bound(py)\n    }\n\n    fn pvalues&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        // Compute z-statistics and convert to p-values\n        // ...\n    }\n\n    // Residual methods call back to rustystats-core\n    fn resid_response&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        use rustystats_core::diagnostics::resid_response;\n        let resid = resid_response(&amp;self.y, &amp;self.fitted_values);\n        resid.into_pyarray_bound(py)\n    }\n}\n</code></pre>"},{"location":"architecture/python-bindings/#handling-optional-parameters","title":"Handling Optional Parameters","text":"<p>Use <code>#[pyo3(signature = ...)]</code> for optional arguments:</p> <pre><code>#[pyfunction]\n#[pyo3(signature = (y, x, family=\"gaussian\", alpha=0.0, l1_ratio=1.0, offset=None, weights=None))]\nfn fit_glm&lt;'py&gt;(\n    py: Python&lt;'py&gt;,\n    y: PyReadonlyArray1&lt;f64&gt;,\n    x: PyReadonlyArray2&lt;f64&gt;,\n    family: &amp;str,\n    alpha: f64,\n    l1_ratio: f64,\n    offset: Option&lt;PyReadonlyArray1&lt;f64&gt;&gt;,\n    weights: Option&lt;PyReadonlyArray1&lt;f64&gt;&gt;,\n) -&gt; PyResult&lt;PyGLMResults&gt; {\n    // ...\n}\n</code></pre> <p>In Python: <pre><code>result = rs._rustystats.fit_glm(y, X)  # Uses defaults\nresult = rs._rustystats.fit_glm(y, X, family=\"poisson\", alpha=0.1)\n</code></pre></p>"},{"location":"architecture/python-bindings/#error-handling","title":"Error Handling","text":""},{"location":"architecture/python-bindings/#rust-python-errors","title":"Rust \u2192 Python Errors","text":"<p>Convert Rust errors to Python exceptions:</p> <pre><code>use pyo3::exceptions::PyValueError;\n\nimpl From&lt;RustyStatsError&gt; for PyErr {\n    fn from(err: RustyStatsError) -&gt; PyErr {\n        match err {\n            RustyStatsError::InvalidInput(msg) =&gt; \n                PyValueError::new_err(msg),\n            RustyStatsError::DimensionMismatch { expected, got } =&gt;\n                PyValueError::new_err(\n                    format!(\"Dimension mismatch: expected {}, got {}\", expected, got)\n                ),\n            _ =&gt; PyValueError::new_err(format!(\"{}\", err)),\n        }\n    }\n}\n</code></pre>"},{"location":"architecture/python-bindings/#using-pyresult","title":"Using PyResult","text":"<p>Functions that can fail return <code>PyResult&lt;T&gt;</code>:</p> <pre><code>#[pyfunction]\nfn fit_glm(...) -&gt; PyResult&lt;PyGLMResults&gt; {\n    // The ? operator converts RustyStatsError to PyErr\n    let result = rustystats_core::fit_glm_full(...)?;\n    Ok(wrap_result(result))\n}\n</code></pre>"},{"location":"architecture/python-bindings/#releasing-the-gil","title":"Releasing the GIL","text":"<p>For long computations, release the GIL to allow other Python threads:</p> <pre><code>#[pyfunction]\nfn fit_glm&lt;'py&gt;(py: Python&lt;'py&gt;, ...) -&gt; PyResult&lt;PyGLMResults&gt; {\n    // Convert inputs first (needs GIL)\n    let y_owned = y.as_array().to_owned();\n    let x_owned = x.as_array().to_owned();\n\n    // Release GIL for computation\n    let result = py.allow_threads(|| {\n        rustystats_core::fit_glm_full(&amp;y_owned, &amp;x_owned, ...)\n    })?;\n\n    // Re-acquire GIL for output conversion\n    Ok(wrap_result(result))\n}\n</code></pre>"},{"location":"architecture/python-bindings/#the-python-layer","title":"The Python Layer","text":"<p>The compiled Rust module is imported by the Python package:</p> <pre><code># python/rustystats/__init__.py\nfrom rustystats._rustystats import (\n    IdentityLink, LogLink, LogitLink,\n    GaussianFamily, PoissonFamily, BinomialFamily,\n    # ...\n)\n\nfrom rustystats.glm import fit_glm, GLM, GLMResults\nfrom rustystats.formula import glm\n</code></pre> <p>The Python layer adds: - Higher-level APIs (formula parsing, DataFrame handling) - Documentation and type hints - Convenience functions</p>"},{"location":"architecture/python-bindings/#adding-a-new-binding","title":"Adding a New Binding","text":""},{"location":"architecture/python-bindings/#step-1-implement-in-rustystats-core","title":"Step 1: Implement in rustystats-core","text":"<pre><code>// In rustystats-core/src/mymodule.rs\npub fn my_new_function(x: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n    // Implementation\n}\n</code></pre>"},{"location":"architecture/python-bindings/#step-2-add-pyo3-wrapper","title":"Step 2: Add PyO3 Wrapper","text":"<pre><code>// In rustystats/src/lib.rs\n\n#[pyfunction]\nfn my_new_function&lt;'py&gt;(\n    py: Python&lt;'py&gt;,\n    x: PyReadonlyArray1&lt;f64&gt;,\n) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n    let x_owned = x.as_array().to_owned();\n    let result = rustystats_core::my_new_function(&amp;x_owned);\n    result.into_pyarray_bound(py)\n}\n</code></pre>"},{"location":"architecture/python-bindings/#step-3-register-in-module","title":"Step 3: Register in Module","text":"<pre><code>#[pymodule]\nfn _rustystats(m: &amp;Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    // ... existing registrations\n    m.add_function(wrap_pyfunction!(my_new_function, m)?)?;\n    Ok(())\n}\n</code></pre>"},{"location":"architecture/python-bindings/#step-4-export-from-python","title":"Step 4: Export from Python","text":"<pre><code># python/rustystats/__init__.py\nfrom rustystats._rustystats import my_new_function\n</code></pre>"},{"location":"architecture/python-bindings/#step-5-rebuild","title":"Step 5: Rebuild","text":"<pre><code>maturin develop\n</code></pre>"},{"location":"architecture/python-bindings/#testing-bindings","title":"Testing Bindings","text":"<pre><code># tests/python/test_bindings.py\ndef test_my_new_function():\n    import rustystats as rs\n    import numpy as np\n\n    x = np.array([1.0, 2.0, 3.0])\n    result = rs.my_new_function(x)\n\n    assert result.shape == x.shape\n    # More assertions...\n</code></pre>"},{"location":"architecture/python-bindings/#common-patterns","title":"Common Patterns","text":""},{"location":"architecture/python-bindings/#constructor-with-validation","title":"Constructor with Validation","text":"<pre><code>#[pymethods]\nimpl PyTweedieFamily {\n    #[new]\n    #[pyo3(signature = (var_power=1.5))]\n    fn new(var_power: f64) -&gt; PyResult&lt;Self&gt; {\n        if var_power &gt; 0.0 &amp;&amp; var_power &lt; 1.0 {\n            return Err(PyValueError::new_err(\n                format!(\"var_power must be &lt;= 0 or &gt;= 1, got {}\", var_power)\n            ));\n        }\n        Ok(Self { inner: TweedieFamily::new(var_power) })\n    }\n}\n</code></pre>"},{"location":"architecture/python-bindings/#method-returning-multiple-values","title":"Method Returning Multiple Values","text":"<pre><code>fn conf_int&lt;'py&gt;(\n    &amp;self,\n    py: Python&lt;'py&gt;,\n    alpha: f64,\n) -&gt; (Bound&lt;'py, PyArray1&lt;f64&gt;&gt;, Bound&lt;'py, PyArray1&lt;f64&gt;&gt;) {\n    let (lower, upper) = compute_ci(&amp;self.coefficients, alpha);\n    (lower.into_pyarray_bound(py), upper.into_pyarray_bound(py))\n}\n</code></pre> <p>In Python: <code>lower, upper = result.conf_int(0.05)</code></p>"},{"location":"architecture/python-bindings/#property-with-computed-value","title":"Property with Computed Value","text":"<pre><code>#[getter]\nfn df_resid(&amp;self) -&gt; usize {\n    self.n_obs.saturating_sub(self.n_params)\n}\n</code></pre>"},{"location":"architecture/rust-core/","title":"Rust Core Library","text":"<p>The <code>rustystats-core</code> crate contains all mathematical computation. This chapter provides a deep dive into its structure and implementation.</p>"},{"location":"architecture/rust-core/#crate-overview","title":"Crate Overview","text":"<pre><code>crates/rustystats-core/\n\u251c\u2500\u2500 Cargo.toml          # Dependencies and metadata\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 lib.rs          # Entry point, re-exports\n    \u251c\u2500\u2500 error.rs        # Error types\n    \u251c\u2500\u2500 families/       # Distribution families\n    \u251c\u2500\u2500 links/          # Link functions\n    \u251c\u2500\u2500 solvers/        # IRLS, coordinate descent\n    \u251c\u2500\u2500 inference/      # Standard errors, p-values\n    \u251c\u2500\u2500 diagnostics/    # Residuals, calibration\n    \u251c\u2500\u2500 splines/        # B-splines, natural splines\n    \u251c\u2500\u2500 formula/        # Formula parsing\n    \u251c\u2500\u2500 design_matrix/  # Design matrix construction\n    \u251c\u2500\u2500 target_encoding/# CatBoost-style encoding\n    \u251c\u2500\u2500 regularization/ # Penalty configuration\n    \u2514\u2500\u2500 interactions/   # Interaction term handling\n</code></pre>"},{"location":"architecture/rust-core/#dependencies","title":"Dependencies","text":"<p>From <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\nndarray = \"0.15\"           # N-dimensional arrays\nnalgebra = \"0.32\"          # Linear algebra (Cholesky, etc.)\nrayon = \"1.8\"              # Parallel iterators\nstatrs = \"0.16\"            # Statistical distributions\nthiserror = \"1.0\"          # Error handling\n</code></pre>"},{"location":"architecture/rust-core/#why-these-crates","title":"Why These Crates?","text":"Crate Purpose Why Chosen <code>ndarray</code> Array operations NumPy-like API, good performance <code>nalgebra</code> Linear algebra Robust decompositions <code>rayon</code> Parallelism Simple, efficient data parallelism <code>statrs</code> Statistics p-values from distributions <code>thiserror</code> Errors Clean error type derivation"},{"location":"architecture/rust-core/#module-librs","title":"Module: lib.rs","text":"<p>The entry point declares modules and re-exports public items:</p> <pre><code>// Module declarations\npub mod error;\npub mod families;\npub mod links;\npub mod solvers;\npub mod inference;\npub mod diagnostics;\npub mod splines;\npub mod formula;\npub mod design_matrix;\npub mod target_encoding;\npub mod regularization;\npub mod interactions;\n\n// Re-exports for convenience\npub use error::{RustyStatsError, Result};\npub use families::Family;\npub use links::Link;\npub use solvers::{IRLSConfig, IRLSResult, fit_glm, fit_glm_full};\n// ... more re-exports\n</code></pre> <p>This allows users to write <code>use rustystats_core::Family</code> instead of <code>use rustystats_core::families::Family</code>.</p>"},{"location":"architecture/rust-core/#module-errorrs","title":"Module: error.rs","text":"<p>Custom error types using <code>thiserror</code>:</p> <pre><code>use thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum RustyStatsError {\n    #[error(\"Invalid input: {0}\")]\n    InvalidInput(String),\n\n    #[error(\"Convergence failed after {iterations} iterations (tolerance: {tolerance})\")]\n    ConvergenceFailure { iterations: usize, tolerance: f64 },\n\n    #[error(\"Numerical error: {0}\")]\n    NumericalError(String),\n\n    #[error(\"Dimension mismatch: expected {expected}, got {got}\")]\n    DimensionMismatch { expected: usize, got: usize },\n}\n\npub type Result&lt;T&gt; = std::result::Result&lt;T, RustyStatsError&gt;;\n</code></pre>"},{"location":"architecture/rust-core/#module-families","title":"Module: families/","text":""},{"location":"architecture/rust-core/#the-family-trait","title":"The Family Trait","text":"<pre><code>pub trait Family: Send + Sync {\n    fn name(&amp;self) -&gt; &amp;str;\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n    fn deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;, \n                weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;) -&gt; f64;\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt;;\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool;\n}\n</code></pre>"},{"location":"architecture/rust-core/#example-implementation-poisson","title":"Example Implementation: Poisson","text":"<pre><code>// families/poisson.rs\npub struct PoissonFamily;\n\nimpl Family for PoissonFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Poisson\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.clone()  // V(\u03bc) = \u03bc\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Zip::from(y).and(mu)\n            .map_collect(|&amp;yi, &amp;mui| {\n                if yi &gt; 0.0 {\n                    2.0 * (yi * (yi / mui).ln() - (yi - mui))\n                } else {\n                    2.0 * mui  // Limit as y \u2192 0\n                }\n            })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.mapv(|yi| (yi + 0.1).max(0.1))  // Avoid log(0)\n    }\n\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;m| m &gt; 0.0)\n    }\n}\n</code></pre>"},{"location":"architecture/rust-core/#module-links","title":"Module: links/","text":""},{"location":"architecture/rust-core/#the-link-trait","title":"The Link Trait","text":"<pre><code>pub trait Link: Send + Sync {\n    fn name(&amp;self) -&gt; &amp;str;\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n}\n</code></pre>"},{"location":"architecture/rust-core/#example-implementation-log","title":"Example Implementation: Log","text":"<pre><code>// links/log.rs\npub struct LogLink;\n\nimpl Link for LogLink {\n    fn name(&amp;self) -&gt; &amp;str { \"Log\" }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| m.max(1e-10).ln())\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        eta.mapv(|e| e.exp())\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| 1.0 / m.max(1e-10))\n    }\n}\n</code></pre>"},{"location":"architecture/rust-core/#module-solvers","title":"Module: solvers/","text":""},{"location":"architecture/rust-core/#irls-implementation","title":"IRLS Implementation","text":"<p>The IRLS solver is the heart of GLM fitting:</p> <pre><code>pub fn fit_glm_full(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,\n    link: &amp;dyn Link,\n    config: &amp;IRLSConfig,\n    offset: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; Result&lt;IRLSResult&gt; {\n    let n = y.len();\n    let p = x.ncols();\n\n    // Initialize\n    let mut mu = family.initialize_mu(y);\n    let mut eta = link.link(&amp;mu);\n\n    // Add offset if present\n    if let Some(off) = offset {\n        eta = &amp;eta + off;\n        mu = link.inverse(&amp;eta);\n    }\n\n    let mut prev_deviance = family.deviance(y, &amp;mu, weights);\n\n    for iter in 0..config.max_iterations {\n        // Step 1: Compute working weights\n        let var = family.variance(&amp;mu);\n        let link_deriv = link.derivative(&amp;mu);\n        let w = compute_weights(&amp;var, &amp;link_deriv, weights, config.min_weight);\n\n        // Step 2: Compute working response\n        let z = compute_working_response(y, &amp;mu, &amp;eta, &amp;link_deriv);\n\n        // Step 3: Solve weighted least squares (X'WX)\u03b2 = X'Wz\n        let (beta, cov_unscaled) = solve_wls(x, &amp;w, &amp;z)?;\n\n        // Step 4: Update predictions\n        eta = x.dot(&amp;beta);\n        if let Some(off) = offset {\n            eta = &amp;eta + off;\n        }\n        mu = link.inverse(&amp;eta);\n\n        // Step 5: Check convergence\n        let deviance = family.deviance(y, &amp;mu, weights);\n        let rel_change = (deviance - prev_deviance).abs() / prev_deviance.max(1e-10);\n\n        if rel_change &lt; config.tolerance {\n            return Ok(IRLSResult { \n                coefficients: beta,\n                fitted_values: mu,\n                deviance,\n                iterations: iter + 1,\n                converged: true,\n                covariance_unscaled: cov_unscaled,\n                // ...\n            });\n        }\n\n        prev_deviance = deviance;\n    }\n\n    // Did not converge\n    Ok(IRLSResult { converged: false, ... })\n}\n</code></pre>"},{"location":"architecture/rust-core/#parallel-wls","title":"Parallel WLS","text":"<p>The weighted least squares step is parallelized:</p> <pre><code>fn solve_wls(\n    x: &amp;Array2&lt;f64&gt;,\n    w: &amp;Array1&lt;f64&gt;,\n    z: &amp;Array1&lt;f64&gt;,\n) -&gt; Result&lt;(Array1&lt;f64&gt;, Array2&lt;f64&gt;)&gt; {\n    let n = x.nrows();\n    let p = x.ncols();\n\n    // Parallel computation of X'WX and X'Wz\n    let (xtwx, xtwz) = (0..n).into_par_iter()\n        .fold(\n            || (Array2::&lt;f64&gt;::zeros((p, p)), Array1::&lt;f64&gt;::zeros(p)),\n            |(mut acc_xtwx, mut acc_xtwz), i| {\n                let xi = x.row(i);\n                let wi = w[i];\n                let wz_i = wi * z[i];\n\n                // X'Wz contribution\n                for j in 0..p {\n                    acc_xtwz[j] += xi[j] * wz_i;\n                }\n\n                // X'WX contribution (only upper triangle for efficiency)\n                for j in 0..p {\n                    for k in j..p {\n                        acc_xtwx[[j, k]] += wi * xi[j] * xi[k];\n                    }\n                }\n\n                (acc_xtwx, acc_xtwz)\n            }\n        )\n        .reduce(\n            || (Array2::zeros((p, p)), Array1::zeros(p)),\n            |(a1, b1), (a2, b2)| (a1 + a2, b1 + b2)\n        );\n\n    // Fill lower triangle\n    for j in 0..p {\n        for k in (j+1)..p {\n            xtwx[[k, j]] = xtwx[[j, k]];\n        }\n    }\n\n    // Solve using Cholesky decomposition\n    let beta = cholesky_solve(&amp;xtwx, &amp;xtwz)?;\n    let cov = cholesky_inverse(&amp;xtwx)?;\n\n    Ok((beta, cov))\n}\n</code></pre>"},{"location":"architecture/rust-core/#coordinate-descent","title":"Coordinate Descent","text":"<p>For regularized models:</p> <pre><code>pub fn fit_glm_coordinate_descent(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,\n    link: &amp;dyn Link,\n    config: &amp;IRLSConfig,\n    reg_config: &amp;RegularizationConfig,\n    offset: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; Result&lt;IRLSResult&gt; {\n    // Outer loop: IRLS for working response/weights\n    // Inner loop: Coordinate descent for penalized WLS\n\n    for outer_iter in 0..config.max_iterations {\n        // Compute working response and weights (same as IRLS)\n        let (z, w) = compute_working_response_and_weights(...);\n\n        // Precompute Gram matrix X'WX (done once per outer iteration)\n        let gram = compute_gram_matrix(x, &amp;w);\n\n        // Coordinate descent inner loop\n        for inner_iter in 0..max_inner_iterations {\n            for j in 0..p {\n                // Compute partial residual\n                let r_j = compute_partial_residual(&amp;z, x, &amp;beta, j, &amp;w);\n\n                // Update \u03b2_j with soft thresholding\n                let xtwx_jj = gram[[j, j]];\n                let xtwz_j = weighted_dot(&amp;x.column(j), &amp;w, &amp;r_j);\n\n                beta[j] = soft_threshold(\n                    xtwz_j,\n                    reg_config.alpha * reg_config.l1_ratio\n                ) / (xtwx_jj + reg_config.alpha * (1.0 - reg_config.l1_ratio));\n            }\n        }\n\n        // Update predictions and check convergence\n        // ...\n    }\n}\n</code></pre>"},{"location":"architecture/rust-core/#module-inference","title":"Module: inference/","text":""},{"location":"architecture/rust-core/#standard-errors-and-p-values","title":"Standard Errors and P-values","text":"<pre><code>pub fn pvalue_z(z: f64) -&gt; f64 {\n    let normal = Normal::new(0.0, 1.0).unwrap();\n    2.0 * (1.0 - normal.cdf(z.abs()))\n}\n\npub fn confidence_interval_z(\n    estimate: f64,\n    se: f64,\n    alpha: f64,\n) -&gt; (f64, f64) {\n    let normal = Normal::new(0.0, 1.0).unwrap();\n    let z_crit = normal.inverse_cdf(1.0 - alpha / 2.0);\n    (estimate - z_crit * se, estimate + z_crit * se)\n}\n</code></pre>"},{"location":"architecture/rust-core/#robust-standard-errors","title":"Robust Standard Errors","text":"<p>HC0, HC1, HC2, HC3 sandwich estimators:</p> <pre><code>pub fn robust_covariance(\n    x: &amp;Array2&lt;f64&gt;,\n    resid: &amp;Array1&lt;f64&gt;,\n    irls_weights: &amp;Array1&lt;f64&gt;,\n    prior_weights: &amp;Array1&lt;f64&gt;,\n    cov_unscaled: &amp;Array2&lt;f64&gt;,\n    hc_type: HCType,\n) -&gt; Array2&lt;f64&gt; {\n    let n = x.nrows();\n    let p = x.ncols();\n    let df = n - p;\n\n    // Compute \"meat\" matrix: X' diag(u\u00b2) X\n    // where u depends on HC type\n    let meat = compute_meat_matrix(x, resid, irls_weights, prior_weights, hc_type);\n\n    // Sandwich: (X'WX)\u207b\u00b9 Meat (X'WX)\u207b\u00b9\n    cov_unscaled.dot(&amp;meat).dot(cov_unscaled)\n}\n</code></pre>"},{"location":"architecture/rust-core/#module-splines","title":"Module: splines/","text":"<p>B-spline and natural spline basis functions:</p> <pre><code>pub fn bs_basis(\n    x: &amp;Array1&lt;f64&gt;,\n    knots: &amp;[f64],\n    degree: usize,\n) -&gt; Array2&lt;f64&gt; {\n    // Cox-de Boor recursive algorithm\n    let n = x.len();\n    let n_basis = knots.len() - degree - 1;\n    let mut basis = Array2::zeros((n, n_basis));\n\n    for i in 0..n {\n        for j in 0..n_basis {\n            basis[[i, j]] = b_spline_basis(x[i], j, degree, knots);\n        }\n    }\n\n    basis\n}\n\nfn b_spline_basis(x: f64, i: usize, k: usize, knots: &amp;[f64]) -&gt; f64 {\n    if k == 0 {\n        // Base case: indicator function\n        if knots[i] &lt;= x &amp;&amp; x &lt; knots[i + 1] {\n            1.0\n        } else {\n            0.0\n        }\n    } else {\n        // Recursive case: Cox-de Boor\n        let left = if (knots[i + k] - knots[i]).abs() &gt; 1e-10 {\n            (x - knots[i]) / (knots[i + k] - knots[i]) \n                * b_spline_basis(x, i, k - 1, knots)\n        } else {\n            0.0\n        };\n\n        let right = if (knots[i + k + 1] - knots[i + 1]).abs() &gt; 1e-10 {\n            (knots[i + k + 1] - x) / (knots[i + k + 1] - knots[i + 1])\n                * b_spline_basis(x, i + 1, k - 1, knots)\n        } else {\n            0.0\n        };\n\n        left + right\n    }\n}\n</code></pre>"},{"location":"architecture/rust-core/#testing","title":"Testing","text":"<p>Each module includes tests:</p> <pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n    use approx::assert_relative_eq;\n\n    #[test]\n    fn test_poisson_variance() {\n        let family = PoissonFamily;\n        let mu = array![1.0, 2.0, 5.0];\n        let var = family.variance(&amp;mu);\n        assert_eq!(var, mu);\n    }\n\n    #[test]\n    fn test_log_link_inverse() {\n        let link = LogLink;\n        let eta = array![0.0, 1.0, 2.0];\n        let mu = link.inverse(&amp;eta);\n        assert_relative_eq!(mu[0], 1.0, epsilon = 1e-10);\n        assert_relative_eq!(mu[1], E, epsilon = 1e-10);\n    }\n}\n</code></pre> <p>Run tests: <code>cargo test -p rustystats-core</code></p>"},{"location":"components/design-matrix/","title":"Design Matrix Component","text":"<p>The design matrix module handles construction of the feature matrix X from raw data, including categorical encoding, interactions, and splines.</p>"},{"location":"components/design-matrix/#code-location","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/design_matrix/\n\u2514\u2500\u2500 mod.rs  # All design matrix functionality\n</code></pre>"},{"location":"components/design-matrix/#overview","title":"Overview","text":"<p>The design matrix X is the n \u00d7 p matrix where: - n = number of observations - p = number of features (including intercept)</p> <p>Each row represents an observation, each column a feature.</p>"},{"location":"components/design-matrix/#categorical-encoding","title":"Categorical Encoding","text":""},{"location":"components/design-matrix/#dummy-coding-reference-level","title":"Dummy Coding (Reference Level)","text":"<p>Default encoding drops the first level to avoid collinearity:</p> <pre><code>pub fn encode_categorical(\n    values: &amp;[String],\n    reference_level: Option&lt;&amp;str&gt;,\n) -&gt; (Array2&lt;f64&gt;, Vec&lt;String&gt;) {\n    // Get unique levels\n    let mut levels: Vec&lt;String&gt; = values.iter()\n        .cloned()\n        .collect::&lt;HashSet&lt;_&gt;&gt;()\n        .into_iter()\n        .collect();\n    levels.sort();  // Consistent ordering\n\n    // Determine reference level\n    let ref_level = reference_level\n        .unwrap_or(&amp;levels[0]);\n\n    // Create column for each non-reference level\n    let non_ref_levels: Vec&lt;_&gt; = levels.iter()\n        .filter(|l| l.as_str() != ref_level)\n        .collect();\n\n    let n = values.len();\n    let k = non_ref_levels.len();\n    let mut encoded = Array2::zeros((n, k));\n\n    for (i, val) in values.iter().enumerate() {\n        if let Some(j) = non_ref_levels.iter().position(|l| *l == val) {\n            encoded[[i, j]] = 1.0;\n        }\n    }\n\n    // Column names: \"Category_Level\"\n    let names = non_ref_levels.iter()\n        .map(|l| format!(\"{}_{}\", \"Category\", l))\n        .collect();\n\n    (encoded, names)\n}\n</code></pre>"},{"location":"components/design-matrix/#example","title":"Example","text":"<p>For <code>Region = [\"A\", \"B\", \"C\", \"A\", \"B\"]</code>:</p> Row Region Region_B Region_C 0 A 0 0 1 B 1 0 2 C 0 1 3 A 0 0 4 B 1 0 <p>Level \"A\" is the reference (coefficient absorbed into intercept).</p>"},{"location":"components/design-matrix/#index-based-encoding","title":"Index-Based Encoding","text":"<p>For efficiency with large datasets, encode from pre-computed indices:</p> <pre><code>pub fn encode_categorical_from_indices(\n    indices: &amp;Array1&lt;usize&gt;,\n    n_levels: usize,\n    include_reference: bool,\n) -&gt; Array2&lt;f64&gt; {\n    let n = indices.len();\n    let k = if include_reference { n_levels } else { n_levels - 1 };\n    let offset = if include_reference { 0 } else { 1 };\n\n    let mut encoded = Array2::zeros((n, k));\n\n    for (i, &amp;idx) in indices.iter().enumerate() {\n        if idx &gt;= offset {\n            encoded[[i, idx - offset]] = 1.0;\n        }\n    }\n\n    encoded\n}\n</code></pre>"},{"location":"components/design-matrix/#interaction-terms","title":"Interaction Terms","text":""},{"location":"components/design-matrix/#categorical-categorical","title":"Categorical \u00d7 Categorical","text":"<p>Creates all pairwise combinations:</p> <pre><code>pub fn build_categorical_categorical_interaction(\n    cat1: &amp;Array2&lt;f64&gt;,\n    cat2: &amp;Array2&lt;f64&gt;,\n    names1: &amp;[String],\n    names2: &amp;[String],\n) -&gt; (Array2&lt;f64&gt;, Vec&lt;String&gt;) {\n    let n = cat1.nrows();\n    let k1 = cat1.ncols();\n    let k2 = cat2.ncols();\n    let k = k1 * k2;\n\n    let mut interaction = Array2::zeros((n, k));\n    let mut names = Vec::with_capacity(k);\n\n    for (j1, name1) in names1.iter().enumerate() {\n        for (j2, name2) in names2.iter().enumerate() {\n            let col_idx = j1 * k2 + j2;\n\n            // Element-wise product of columns\n            for i in 0..n {\n                interaction[[i, col_idx]] = cat1[[i, j1]] * cat2[[i, j2]];\n            }\n\n            names.push(format!(\"{}:{}\", name1, name2));\n        }\n    }\n\n    (interaction, names)\n}\n</code></pre>"},{"location":"components/design-matrix/#categorical-continuous","title":"Categorical \u00d7 Continuous","text":"<p>Each categorical level gets its own slope:</p> <pre><code>pub fn build_categorical_continuous_interaction(\n    categorical: &amp;Array2&lt;f64&gt;,\n    continuous: &amp;Array1&lt;f64&gt;,\n    cat_names: &amp;[String],\n    cont_name: &amp;str,\n) -&gt; (Array2&lt;f64&gt;, Vec&lt;String&gt;) {\n    let n = categorical.nrows();\n    let k = categorical.ncols();\n\n    let mut interaction = Array2::zeros((n, k));\n    let mut names = Vec::with_capacity(k);\n\n    for j in 0..k {\n        // Multiply categorical indicator by continuous variable\n        for i in 0..n {\n            interaction[[i, j]] = categorical[[i, j]] * continuous[i];\n        }\n\n        names.push(format!(\"{}:{}\", cat_names[j], cont_name));\n    }\n\n    (interaction, names)\n}\n</code></pre>"},{"location":"components/design-matrix/#continuous-continuous","title":"Continuous \u00d7 Continuous","text":"<p>Simple product:</p> <pre><code>pub fn build_continuous_continuous_interaction(\n    x1: &amp;Array1&lt;f64&gt;,\n    x2: &amp;Array1&lt;f64&gt;,\n    name1: &amp;str,\n    name2: &amp;str,\n) -&gt; (Array1&lt;f64&gt;, String) {\n    let interaction = x1 * x2;\n    let name = format!(\"{}:{}\", name1, name2);\n    (interaction, name)\n}\n</code></pre>"},{"location":"components/design-matrix/#the-full-design-matrix-builder","title":"The Full Design Matrix Builder","text":"<p>Combines all components:</p> <pre><code>pub fn build_design_matrix(\n    data: &amp;DataSource,\n    terms: &amp;[Term],\n    include_intercept: bool,\n) -&gt; Result&lt;(Array2&lt;f64&gt;, Vec&lt;String&gt;)&gt; {\n    let n = data.n_rows();\n    let mut columns: Vec&lt;Array1&lt;f64&gt;&gt; = Vec::new();\n    let mut names: Vec&lt;String&gt; = Vec::new();\n\n    // 1. Add intercept\n    if include_intercept {\n        columns.push(Array1::ones(n));\n        names.push(\"Intercept\".to_string());\n    }\n\n    // 2. Process each term\n    for term in terms {\n        match term {\n            Term::Continuous(name) =&gt; {\n                columns.push(data.get_column(name)?);\n                names.push(name.clone());\n            }\n\n            Term::Categorical(name) =&gt; {\n                let values = data.get_categorical(name)?;\n                let (encoded, term_names) = encode_categorical(&amp;values, None);\n                for col in encoded.axis_iter(Axis(1)) {\n                    columns.push(col.to_owned());\n                }\n                names.extend(term_names);\n            }\n\n            Term::Spline { name, kind, df } =&gt; {\n                let x = data.get_column(name)?;\n                let basis = match kind {\n                    SplineKind::BSpline =&gt; bs(&amp;x, *df, 3, None),\n                    SplineKind::NaturalSpline =&gt; ns(&amp;x, *df, None),\n                };\n                for (j, col) in basis.axis_iter(Axis(1)).enumerate() {\n                    columns.push(col.to_owned());\n                    names.push(format!(\"{}({}, df={})_{}\", kind, name, df, j));\n                }\n            }\n\n            Term::Interaction(t1, t2) =&gt; {\n                // Build interaction from sub-terms\n                // ...\n            }\n        }\n    }\n\n    // 3. Stack columns\n    let p = columns.len();\n    let mut x = Array2::zeros((n, p));\n    for (j, col) in columns.into_iter().enumerate() {\n        x.column_mut(j).assign(&amp;col);\n    }\n\n    Ok((x, names))\n}\n</code></pre>"},{"location":"components/design-matrix/#data-source-abstraction","title":"Data Source Abstraction","text":"<p>Supports both NumPy arrays and DataFrames:</p> <pre><code>pub enum DataSource&lt;'a&gt; {\n    Arrays {\n        y: &amp;'a Array1&lt;f64&gt;,\n        x: &amp;'a Array2&lt;f64&gt;,\n        column_names: Option&lt;&amp;'a [String]&gt;,\n    },\n    DataFrame {\n        columns: HashMap&lt;String, ColumnData&gt;,\n    },\n}\n\npub enum ColumnData {\n    Float64(Array1&lt;f64&gt;),\n    String(Vec&lt;String&gt;),\n    Int64(Array1&lt;i64&gt;),\n}\n</code></pre>"},{"location":"components/design-matrix/#performance-considerations","title":"Performance Considerations","text":""},{"location":"components/design-matrix/#1-pre-allocation","title":"1. Pre-allocation","text":"<p>Avoid repeated allocations:</p> <pre><code>// Pre-calculate total columns\nlet total_cols = terms.iter().map(|t| t.n_columns()).sum();\nlet mut x = Array2::zeros((n, total_cols));\n</code></pre>"},{"location":"components/design-matrix/#2-parallel-construction","title":"2. Parallel Construction","text":"<p>For large datasets:</p> <pre><code>use rayon::prelude::*;\n\nlet columns: Vec&lt;_&gt; = terms.par_iter()\n    .map(|term| build_term_columns(term, data))\n    .collect();\n</code></pre>"},{"location":"components/design-matrix/#3-sparse-consideration","title":"3. Sparse Consideration","text":"<p>Categorical encoding produces sparse columns (mostly zeros). For very high cardinality, consider sparse matrix representations.</p>"},{"location":"components/design-matrix/#testing","title":"Testing","text":"<pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_categorical_encoding() {\n        let values = vec![\"A\".into(), \"B\".into(), \"A\".into(), \"C\".into()];\n        let (encoded, names) = encode_categorical(&amp;values, None);\n\n        assert_eq!(encoded.shape(), &amp;[4, 2]);  // 3 levels - 1\n        assert_eq!(names, vec![\"Category_B\", \"Category_C\"]);\n\n        // Check encoding\n        assert_eq!(encoded[[0, 0]], 0.0);  // A \u2192 [0, 0]\n        assert_eq!(encoded[[1, 0]], 1.0);  // B \u2192 [1, 0]\n        assert_eq!(encoded[[3, 1]], 1.0);  // C \u2192 [0, 1]\n    }\n\n    #[test]\n    fn test_interaction_dimensions() {\n        let cat1 = Array2::zeros((10, 3));  // 4 levels\n        let cat2 = Array2::zeros((10, 2));  // 3 levels\n\n        let (interaction, names) = build_categorical_categorical_interaction(\n            &amp;cat1, &amp;cat2, &amp;[\"A\", \"B\", \"C\"], &amp;[\"X\", \"Y\"]\n        );\n\n        assert_eq!(interaction.shape(), &amp;[10, 6]);  // 3 \u00d7 2\n        assert_eq!(names.len(), 6);\n    }\n}\n</code></pre>"},{"location":"components/diagnostics/","title":"Diagnostics Component","text":"<p>The diagnostics module provides comprehensive model assessment tools including residuals, calibration metrics, discrimination measures, and interaction detection.</p>"},{"location":"components/diagnostics/#code-location","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/diagnostics/\n\u251c\u2500\u2500 mod.rs           # Re-exports\n\u251c\u2500\u2500 residuals.rs     # Residual computations\n\u251c\u2500\u2500 dispersion.rs    # Dispersion estimation\n\u251c\u2500\u2500 likelihood.rs    # Log-likelihood, AIC, BIC\n\u251c\u2500\u2500 calibration.rs   # A/E ratios, calibration curves\n\u251c\u2500\u2500 discrimination.rs # Gini, AUC, lift\n\u251c\u2500\u2500 loss.rs          # Loss functions (MSE, MAE)\n\u251c\u2500\u2500 interactions.rs  # Interaction detection\n\u2514\u2500\u2500 factor_analysis.rs # Per-factor diagnostics\n\npython/rustystats/diagnostics.py  # Python API\n</code></pre>"},{"location":"components/diagnostics/#residuals","title":"Residuals","text":""},{"location":"components/diagnostics/#types-of-residuals","title":"Types of Residuals","text":"<pre><code>/// Response residuals: y - \u03bc\npub fn resid_response(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n) -&gt; Array1&lt;f64&gt; {\n    y - mu\n}\n\n/// Pearson residuals: (y - \u03bc) / \u221aV(\u03bc)\npub fn resid_pearson(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    family: &amp;dyn Family,\n) -&gt; Array1&lt;f64&gt; {\n    let var = family.variance(mu);\n    (y - mu) / var.mapv(|v| v.sqrt())\n}\n\n/// Deviance residuals: sign(y - \u03bc) \u00d7 \u221ad\npub fn resid_deviance(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    family: &amp;dyn Family,\n) -&gt; Array1&lt;f64&gt; {\n    let unit_dev = family.unit_deviance(y, mu);\n    Zip::from(y).and(mu).and(&amp;unit_dev)\n        .map_collect(|&amp;yi, &amp;mui, &amp;di| {\n            let sign = if yi &gt; mui { 1.0 } else { -1.0 };\n            sign * di.sqrt()\n        })\n}\n\n/// Working residuals: (y - \u03bc) \u00d7 g'(\u03bc)\npub fn resid_working(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    link: &amp;dyn Link,\n) -&gt; Array1&lt;f64&gt; {\n    let deriv = link.derivative(mu);\n    (y - mu) * deriv\n}\n</code></pre>"},{"location":"components/diagnostics/#when-to-use-each","title":"When to Use Each","text":"Residual Use Case Response Simple interpretation Pearson Detecting outliers, checking overdispersion Deviance Most diagnostic plots, Q-Q plots Working Partial residual plots"},{"location":"components/diagnostics/#dispersion-estimation","title":"Dispersion Estimation","text":"<pre><code>/// Pearson-based dispersion estimate\npub fn estimate_dispersion_pearson(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    family: &amp;dyn Family,\n    df_resid: usize,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; f64 {\n    let chi2 = pearson_chi2(y, mu, family, weights);\n    chi2 / df_resid as f64\n}\n\n/// Pearson chi-squared statistic\npub fn pearson_chi2(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    family: &amp;dyn Family,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; f64 {\n    let var = family.variance(mu);\n\n    let mut chi2 = 0.0;\n    for i in 0..y.len() {\n        let r = (y[i] - mu[i]).powi(2) / var[i];\n        let w = weights.map_or(1.0, |w| w[i]);\n        chi2 += w * r;\n    }\n\n    chi2\n}\n</code></pre>"},{"location":"components/diagnostics/#likelihood-and-information-criteria","title":"Likelihood and Information Criteria","text":"<pre><code>/// Log-likelihood for Poisson\npub fn log_likelihood_poisson(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; f64 {\n    let mut ll = 0.0;\n    for i in 0..y.len() {\n        let w = weights.map_or(1.0, |w| w[i]);\n        // log P(Y=y) = y*log(\u03bc) - \u03bc - log(y!)\n        let log_factorial = (1..=(y[i] as usize))\n            .map(|k| (k as f64).ln())\n            .sum::&lt;f64&gt;();\n        ll += w * (y[i] * mu[i].ln() - mu[i] - log_factorial);\n    }\n    ll\n}\n\n/// AIC = -2*loglik + 2*p\npub fn aic(log_likelihood: f64, n_params: usize) -&gt; f64 {\n    -2.0 * log_likelihood + 2.0 * n_params as f64\n}\n\n/// BIC = -2*loglik + p*log(n)\npub fn bic(log_likelihood: f64, n_params: usize, n_obs: usize) -&gt; f64 {\n    -2.0 * log_likelihood + (n_params as f64) * (n_obs as f64).ln()\n}\n\n/// Null deviance (intercept-only model)\npub fn null_deviance(\n    y: &amp;Array1&lt;f64&gt;,\n    family: &amp;dyn Family,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; f64 {\n    // Fit intercept-only model: \u03bc = weighted mean of y\n    let mu_null = match weights {\n        Some(w) =&gt; {\n            let sum_wy: f64 = y.iter().zip(w.iter()).map(|(y, w)| y * w).sum();\n            let sum_w: f64 = w.sum();\n            sum_wy / sum_w\n        }\n        None =&gt; y.mean().unwrap(),\n    };\n\n    let mu_null_vec = Array1::from_elem(y.len(), mu_null);\n    family.deviance(y, &amp;mu_null_vec, weights)\n}\n</code></pre>"},{"location":"components/diagnostics/#calibration-metrics","title":"Calibration Metrics","text":""},{"location":"components/diagnostics/#actual-vs-expected","title":"Actual vs Expected","text":"<pre><code>pub struct CalibrationResult {\n    pub overall_ae: f64,\n    pub by_decile: Vec&lt;DecileStats&gt;,\n    pub hosmer_lemeshow: Option&lt;HLTest&gt;,\n}\n\npub fn compute_calibration_curve(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    n_bins: usize,\n) -&gt; CalibrationResult {\n    let n = y.len();\n\n    // Overall A/E\n    let total_actual: f64 = match weights {\n        Some(w) =&gt; y.iter().zip(w.iter()).map(|(y, w)| y * w).sum(),\n        None =&gt; y.sum(),\n    };\n    let total_expected: f64 = match weights {\n        Some(w) =&gt; mu.iter().zip(w.iter()).map(|(m, w)| m * w).sum(),\n        None =&gt; mu.sum(),\n    };\n    let overall_ae = total_actual / total_expected;\n\n    // Sort by predicted risk\n    let mut indices: Vec&lt;usize&gt; = (0..n).collect();\n    indices.sort_by(|&amp;a, &amp;b| mu[a].partial_cmp(&amp;mu[b]).unwrap());\n\n    // Compute A/E by decile\n    let bin_size = n / n_bins;\n    let mut by_decile = Vec::with_capacity(n_bins);\n\n    for bin in 0..n_bins {\n        let start = bin * bin_size;\n        let end = if bin == n_bins - 1 { n } else { (bin + 1) * bin_size };\n\n        let mut actual = 0.0;\n        let mut expected = 0.0;\n        let mut exposure = 0.0;\n\n        for &amp;i in &amp;indices[start..end] {\n            let w = weights.map_or(1.0, |w| w[i]);\n            actual += y[i] * w;\n            expected += mu[i] * w;\n            exposure += w;\n        }\n\n        by_decile.push(DecileStats {\n            decile: bin + 1,\n            actual,\n            expected,\n            ae_ratio: actual / expected,\n            exposure,\n        });\n    }\n\n    CalibrationResult { overall_ae, by_decile, hosmer_lemeshow: None }\n}\n</code></pre>"},{"location":"components/diagnostics/#discrimination-metrics","title":"Discrimination Metrics","text":""},{"location":"components/diagnostics/#gini-coefficient","title":"Gini Coefficient","text":"<pre><code>pub fn compute_gini(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; f64 {\n    // Gini = 2 * AUC - 1\n    let auc = compute_auc(y, mu, weights);\n    2.0 * auc - 1.0\n}\n\npub fn compute_auc(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; f64 {\n    let n = y.len();\n\n    // Sort by predicted risk (descending)\n    let mut indices: Vec&lt;usize&gt; = (0..n).collect();\n    indices.sort_by(|&amp;a, &amp;b| mu[b].partial_cmp(&amp;mu[a]).unwrap());\n\n    // Compute AUC using trapezoidal rule\n    let mut cum_actual = 0.0;\n    let mut cum_exposure = 0.0;\n    let total_actual: f64 = y.sum();\n    let total_exposure: f64 = weights.map_or(n as f64, |w| w.sum());\n\n    let mut auc = 0.0;\n    let mut prev_x = 0.0;\n    let mut prev_y = 0.0;\n\n    for &amp;i in &amp;indices {\n        let w = weights.map_or(1.0, |w| w[i]);\n        cum_actual += y[i];\n        cum_exposure += w;\n\n        let x = cum_exposure / total_exposure;\n        let y_val = cum_actual / total_actual;\n\n        // Trapezoidal area\n        auc += (x - prev_x) * (y_val + prev_y) / 2.0;\n\n        prev_x = x;\n        prev_y = y_val;\n    }\n\n    auc\n}\n</code></pre>"},{"location":"components/diagnostics/#lorenz-curve","title":"Lorenz Curve","text":"<pre><code>pub fn compute_lorenz_curve(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    n_points: usize,\n) -&gt; Vec&lt;(f64, f64)&gt; {\n    let n = y.len();\n\n    // Sort by predicted risk\n    let mut indices: Vec&lt;usize&gt; = (0..n).collect();\n    indices.sort_by(|&amp;a, &amp;b| mu[a].partial_cmp(&amp;mu[b]).unwrap());\n\n    let total_actual: f64 = y.sum();\n    let total_exposure: f64 = weights.map_or(n as f64, |w| w.sum());\n\n    let mut curve = Vec::with_capacity(n_points + 1);\n    curve.push((0.0, 0.0));\n\n    let step = n / n_points;\n    let mut cum_actual = 0.0;\n    let mut cum_exposure = 0.0;\n\n    for (idx, &amp;i) in indices.iter().enumerate() {\n        let w = weights.map_or(1.0, |w| w[i]);\n        cum_actual += y[i];\n        cum_exposure += w;\n\n        if (idx + 1) % step == 0 || idx == n - 1 {\n            curve.push((\n                cum_exposure / total_exposure,\n                cum_actual / total_actual,\n            ));\n        }\n    }\n\n    curve\n}\n</code></pre>"},{"location":"components/diagnostics/#interaction-detection","title":"Interaction Detection","text":"<p>Greedy residual-based detection of potential interactions:</p> <pre><code>pub struct InteractionConfig {\n    pub max_candidates: usize,\n    pub min_strength: f64,\n}\n\npub fn detect_interactions(\n    residuals: &amp;Array1&lt;f64&gt;,\n    factors: &amp;[FactorData],\n    config: &amp;InteractionConfig,\n) -&gt; Vec&lt;InteractionCandidate&gt; {\n    let n_factors = factors.len();\n    let mut candidates = Vec::new();\n\n    // Test all pairs\n    for i in 0..n_factors {\n        for j in (i + 1)..n_factors {\n            let strength = compute_interaction_strength(\n                residuals,\n                &amp;factors[i],\n                &amp;factors[j],\n            );\n\n            if strength &gt; config.min_strength {\n                candidates.push(InteractionCandidate {\n                    factor1: factors[i].name.clone(),\n                    factor2: factors[j].name.clone(),\n                    strength,\n                });\n            }\n        }\n    }\n\n    // Sort by strength\n    candidates.sort_by(|a, b| b.strength.partial_cmp(&amp;a.strength).unwrap());\n    candidates.truncate(config.max_candidates);\n\n    candidates\n}\n\nfn compute_interaction_strength(\n    residuals: &amp;Array1&lt;f64&gt;,\n    factor1: &amp;FactorData,\n    factor2: &amp;FactorData,\n) -&gt; f64 {\n    // Compute variance of residuals explained by interaction\n    // Uses grouping by factor combinations\n\n    // ... implementation details\n\n    explained_variance / total_variance\n}\n</code></pre>"},{"location":"components/diagnostics/#python-api-modeldiagnostics","title":"Python API: ModelDiagnostics","text":"<pre><code>class ModelDiagnostics:\n    \"\"\"Comprehensive model diagnostics with JSON export.\"\"\"\n\n    def __init__(self, result, data, categorical_factors, continuous_factors):\n        self.model_summary = self._compute_model_summary(result)\n        self.fit_statistics = self._compute_fit_stats(result)\n        self.calibration = self._compute_calibration(result, data)\n        self.discrimination = self._compute_discrimination(result, data)\n        self.factors = self._compute_factor_diagnostics(\n            result, data, categorical_factors, continuous_factors\n        )\n        self.interaction_candidates = self._detect_interactions(result, data)\n        self.warnings = self._generate_warnings()\n\n    def to_json(self) -&gt; str:\n        \"\"\"Export as compact JSON for LLM consumption.\"\"\"\n        return json.dumps({\n            'model_summary': self.model_summary,\n            'fit_statistics': self.fit_statistics,\n            'calibration': self.calibration,\n            'discrimination': self.discrimination,\n            'factors': [f.to_dict() for f in self.factors],\n            'interaction_candidates': self.interaction_candidates,\n            'warnings': self.warnings,\n        }, indent=2)\n</code></pre>"},{"location":"components/diagnostics/#usage","title":"Usage","text":"<pre><code>result = rs.glm(\"y ~ x1 + C(region)\", data, family=\"poisson\").fit()\n\ndiagnostics = result.diagnostics(\n    data=data,\n    categorical_factors=[\"region\", \"brand\"],\n    continuous_factors=[\"age\", \"income\"],\n)\n\n# View summary\nprint(diagnostics.to_json())\n\n# Check specific metrics\nprint(f\"Gini: {diagnostics.discrimination['gini_coefficient']:.3f}\")\nprint(f\"A/E: {diagnostics.calibration['overall_ae']:.3f}\")\n\n# Check for issues\nfor warning in diagnostics.warnings:\n    print(f\"\u26a0\ufe0f {warning['message']}\")\n</code></pre>"},{"location":"components/diagnostics/#warning-generation","title":"Warning Generation","text":"<pre><code>pub fn generate_warnings(diagnostics: &amp;Diagnostics) -&gt; Vec&lt;Warning&gt; {\n    let mut warnings = Vec::new();\n\n    // Check dispersion\n    if diagnostics.dispersion_pearson &gt; 2.0 {\n        warnings.push(Warning {\n            warning_type: \"overdispersion\".into(),\n            message: format!(\n                \"High dispersion ({:.2}). Consider QuasiPoisson or NegBinomial.\",\n                diagnostics.dispersion_pearson\n            ),\n        });\n    }\n\n    // Check calibration\n    if (diagnostics.overall_ae - 1.0).abs() &gt; 0.05 {\n        warnings.push(Warning {\n            warning_type: \"calibration\".into(),\n            message: format!(\n                \"A/E ratio is {:.3}. Model {} overall.\",\n                diagnostics.overall_ae,\n                if diagnostics.overall_ae &gt; 1.0 { \"underpredicts\" } else { \"overpredicts\" }\n            ),\n        });\n    }\n\n    // Check for non-fitted factors with signal\n    for factor in &amp;diagnostics.factors {\n        if !factor.in_model &amp;&amp; factor.residual_correlation.abs() &gt; 0.05 {\n            warnings.push(Warning {\n                warning_type: \"missing_factor\".into(),\n                message: format!(\n                    \"Factor '{}' not in model but explains {:.1}% of residual variance.\",\n                    factor.name,\n                    factor.residual_correlation.abs() * 100.0\n                ),\n            });\n        }\n    }\n\n    warnings\n}\n</code></pre>"},{"location":"components/diagnostics/#testing","title":"Testing","text":"<pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_gini_perfect_model() {\n        // Perfect predictions should give Gini = 1\n        let y = array![0.0, 0.0, 1.0, 1.0];\n        let mu = array![0.1, 0.2, 0.8, 0.9];  // Perfect ranking\n\n        let gini = compute_gini(&amp;y, &amp;mu, None);\n        assert!((gini - 1.0).abs() &lt; 0.01);\n    }\n\n    #[test]\n    fn test_gini_random_model() {\n        // Random predictions should give Gini \u2248 0\n        let y = array![0.0, 1.0, 0.0, 1.0];\n        let mu = array![0.5, 0.5, 0.5, 0.5];  // No discrimination\n\n        let gini = compute_gini(&amp;y, &amp;mu, None);\n        assert!(gini.abs() &lt; 0.01);\n    }\n\n    #[test]\n    fn test_ae_ratio() {\n        let y = array![1.0, 2.0, 3.0];\n        let mu = array![1.0, 2.0, 3.0];  // Perfect predictions\n\n        let cal = compute_calibration_curve(&amp;y, &amp;mu, None, 3);\n        assert!((cal.overall_ae - 1.0).abs() &lt; 1e-10);\n    }\n}\n</code></pre>"},{"location":"components/families/","title":"Families Component","text":"<p>This chapter provides implementation details for each distribution family in RustyStats.</p>"},{"location":"components/families/#code-location","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/families/\n\u251c\u2500\u2500 mod.rs              # Family trait definition\n\u251c\u2500\u2500 gaussian.rs         # Gaussian (Normal)\n\u251c\u2500\u2500 poisson.rs          # Poisson\n\u251c\u2500\u2500 binomial.rs         # Binomial\n\u251c\u2500\u2500 gamma.rs            # Gamma\n\u251c\u2500\u2500 tweedie.rs          # Tweedie\n\u251c\u2500\u2500 quasi.rs            # QuasiPoisson, QuasiBinomial\n\u2514\u2500\u2500 negative_binomial.rs # Negative Binomial\n</code></pre>"},{"location":"components/families/#the-family-trait","title":"The Family Trait","text":"<p>Every family must implement:</p> <pre><code>pub trait Family: Send + Sync {\n    /// Display name\n    fn name(&amp;self) -&gt; &amp;str;\n\n    /// Variance function V(\u03bc)\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Unit deviance d(y, \u03bc) per observation\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Total deviance (sum of weighted unit deviances)\n    fn deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;, \n                weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;) -&gt; f64;\n\n    /// Canonical link function\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt;;\n\n    /// Starting values for \u03bc\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Check if \u03bc values are valid\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool;\n}\n</code></pre>"},{"location":"components/families/#gaussian-family","title":"Gaussian Family","text":"<p>File: <code>gaussian.rs</code></p>"},{"location":"components/families/#properties","title":"Properties","text":"Property Value Variance \\(V(\\mu) = 1\\) Deviance \\(d(y, \\mu) = (y - \\mu)^2\\) Canonical link Identity Valid \u03bc range \\((-\\infty, +\\infty)\\)"},{"location":"components/families/#implementation","title":"Implementation","text":"<pre><code>pub struct GaussianFamily;\n\nimpl Family for GaussianFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Gaussian\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Array1::ones(mu.len())  // Constant variance\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        (y - mu).mapv(|r| r * r)  // (y - \u03bc)\u00b2\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(IdentityLink)\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.clone()  // Start at observed values\n    }\n\n    fn is_valid_mu(&amp;self, _mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        true  // Any real number is valid\n    }\n}\n</code></pre>"},{"location":"components/families/#poisson-family","title":"Poisson Family","text":"<p>File: <code>poisson.rs</code></p>"},{"location":"components/families/#properties_1","title":"Properties","text":"Property Value Variance \\(V(\\mu) = \\mu\\) Deviance \\(d(y, \\mu) = 2[y \\log(y/\\mu) - (y - \\mu)]\\) Canonical link Log Valid \u03bc range \\((0, +\\infty)\\)"},{"location":"components/families/#implementation_1","title":"Implementation","text":"<pre><code>pub struct PoissonFamily;\n\nimpl Family for PoissonFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Poisson\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.clone()  // V(\u03bc) = \u03bc\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            let mui_safe = mui.max(1e-10);\n            if yi &gt; 0.0 {\n                2.0 * (yi * (yi / mui_safe).ln() - (yi - mui_safe))\n            } else {\n                2.0 * mui_safe  // Limit as y \u2192 0\n            }\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.mapv(|yi| (yi + 0.1).max(0.1))  // Avoid log(0)\n    }\n\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;m| m &gt; 0.0)\n    }\n}\n</code></pre>"},{"location":"components/families/#note-handling-y-0","title":"Note: Handling y = 0","text":"<p>When \\(y = 0\\), the term \\(y \\log(y/\\mu)\\) requires special handling:</p> \\[ \\lim_{y \\to 0} y \\log(y/\\mu) = 0 \\] <p>So the unit deviance simplifies to \\(2\\mu\\).</p>"},{"location":"components/families/#binomial-family","title":"Binomial Family","text":"<p>File: <code>binomial.rs</code></p>"},{"location":"components/families/#properties_2","title":"Properties","text":"Property Value Variance \\(V(\\mu) = \\mu(1-\\mu)\\) Deviance \\(d(y, \\mu) = 2[y \\log(y/\\mu) + (1-y)\\log((1-y)/(1-\\mu))]\\) Canonical link Logit Valid \u03bc range \\((0, 1)\\)"},{"location":"components/families/#implementation_2","title":"Implementation","text":"<pre><code>pub struct BinomialFamily;\n\nimpl Family for BinomialFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Binomial\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| {\n            let m_safe = m.clamp(1e-10, 1.0 - 1e-10);\n            m_safe * (1.0 - m_safe)\n        })\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            let mui_safe = mui.clamp(1e-10, 1.0 - 1e-10);\n            let yi_safe = yi.clamp(1e-10, 1.0 - 1e-10);\n\n            2.0 * (yi_safe * (yi_safe / mui_safe).ln() \n                   + (1.0 - yi_safe) * ((1.0 - yi_safe) / (1.0 - mui_safe)).ln())\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogitLink)\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.mapv(|yi| (yi + 0.5) / 2.0)  // Shrink toward 0.5\n    }\n\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;m| m &gt; 0.0 &amp;&amp; m &lt; 1.0)\n    }\n}\n</code></pre>"},{"location":"components/families/#gamma-family","title":"Gamma Family","text":"<p>File: <code>gamma.rs</code></p>"},{"location":"components/families/#properties_3","title":"Properties","text":"Property Value Variance \\(V(\\mu) = \\mu^2\\) Deviance \\(d(y, \\mu) = 2[-\\log(y/\\mu) + (y-\\mu)/\\mu]\\) Canonical link Inverse (\\(-1/\\mu\\)) Common link Log (used in RustyStats) Valid \u03bc range \\((0, +\\infty)\\)"},{"location":"components/families/#implementation_3","title":"Implementation","text":"<pre><code>pub struct GammaFamily;\n\nimpl Family for GammaFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Gamma\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| m * m)  // V(\u03bc) = \u03bc\u00b2\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            let mui_safe = mui.max(1e-10);\n            let yi_safe = yi.max(1e-10);\n            2.0 * (-(yi_safe / mui_safe).ln() + (yi_safe - mui_safe) / mui_safe)\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)  // Log link, not canonical inverse\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.mapv(|yi| yi.max(0.1))\n    }\n\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;m| m &gt; 0.0)\n    }\n}\n</code></pre>"},{"location":"components/families/#tweedie-family","title":"Tweedie Family","text":"<p>File: <code>tweedie.rs</code></p>"},{"location":"components/families/#properties_4","title":"Properties","text":"Property Value Variance \\(V(\\mu) = \\mu^p\\) where \\(p\\) is variance power Valid \\(p\\) \\(p \\leq 0\\) or \\(p \\geq 1\\) Valid \u03bc range \\((0, +\\infty)\\) for \\(p &gt; 0\\)"},{"location":"components/families/#implementation_4","title":"Implementation","text":"<pre><code>pub struct TweedieFamily {\n    pub var_power: f64,\n}\n\nimpl TweedieFamily {\n    pub fn new(var_power: f64) -&gt; Self {\n        assert!(var_power &lt;= 0.0 || var_power &gt;= 1.0,\n                \"var_power must be &lt;= 0 or &gt;= 1\");\n        Self { var_power }\n    }\n}\n\nimpl Family for TweedieFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Tweedie\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        let p = self.var_power;\n        mu.mapv(|m| m.powf(p))\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        let p = self.var_power;\n\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            let mui_safe = mui.max(1e-10);\n\n            if (p - 1.0).abs() &lt; 1e-10 {\n                // p = 1: Poisson limit\n                2.0 * (yi * (yi / mui_safe).ln() - (yi - mui_safe))\n            } else if (p - 2.0).abs() &lt; 1e-10 {\n                // p = 2: Gamma\n                2.0 * (-(yi / mui_safe).ln() + (yi - mui_safe) / mui_safe)\n            } else {\n                // General case\n                let term1 = yi.powf(2.0 - p) / ((1.0 - p) * (2.0 - p));\n                let term2 = yi * mui_safe.powf(1.0 - p) / (1.0 - p);\n                let term3 = mui_safe.powf(2.0 - p) / (2.0 - p);\n                2.0 * (term1 - term2 + term3)\n            }\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)\n    }\n\n    // ...\n}\n</code></pre>"},{"location":"components/families/#quasi-families","title":"Quasi-Families","text":"<p>File: <code>quasi.rs</code></p> <p>Quasi-families estimate dispersion from data instead of fixing it at 1.</p>"},{"location":"components/families/#quasipoisson","title":"QuasiPoisson","text":"<pre><code>pub struct QuasiPoissonFamily;\n\nimpl Family for QuasiPoissonFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"QuasiPoisson\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.clone()  // Same as Poisson\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        // Same deviance formula as Poisson\n        // Dispersion is estimated separately\n        // ...\n    }\n\n    // Same initialization and validation as Poisson\n}\n</code></pre> <p>The key difference is in how dispersion is handled in <code>PyGLMResults</code>:</p> <pre><code>fn scale(&amp;self) -&gt; f64 {\n    match self.family_name.as_str() {\n        \"Poisson\" | \"Binomial\" =&gt; 1.0,  // Fixed\n        \"QuasiPoisson\" | \"QuasiBinomial\" =&gt; {\n            // Estimate from Pearson residuals\n            estimate_dispersion_pearson(...)\n        }\n        _ =&gt; self.deviance / self.df_resid() as f64,\n    }\n}\n</code></pre>"},{"location":"components/families/#negative-binomial-family","title":"Negative Binomial Family","text":"<p>File: <code>negative_binomial.rs</code></p>"},{"location":"components/families/#properties_5","title":"Properties","text":"Property Value Variance \\(V(\\mu) = \\mu + \\mu^2/\\theta\\) Parameter \\(\\theta &gt; 0\\) controls overdispersion Valid \u03bc range \\((0, +\\infty)\\)"},{"location":"components/families/#implementation_5","title":"Implementation","text":"<pre><code>pub struct NegativeBinomialFamily {\n    pub theta: f64,\n}\n\nimpl NegativeBinomialFamily {\n    pub fn new(theta: f64) -&gt; Self {\n        assert!(theta &gt; 0.0, \"theta must be positive\");\n        Self { theta }\n    }\n\n    pub fn alpha(&amp;self) -&gt; f64 {\n        1.0 / self.theta  // Alternative parameterization\n    }\n}\n\nimpl Family for NegativeBinomialFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"NegativeBinomial\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        let theta = self.theta;\n        mu.mapv(|m| m + m * m / theta)\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        let theta = self.theta;\n\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            let mui_safe = mui.max(1e-10);\n\n            let term1 = if yi &gt; 0.0 {\n                2.0 * yi * (yi / mui_safe).ln()\n            } else {\n                0.0\n            };\n\n            let term2 = 2.0 * (yi + theta) * \n                ((yi + theta) / (mui_safe + theta)).ln();\n\n            term1 - term2\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)\n    }\n\n    // ...\n}\n</code></pre>"},{"location":"components/families/#theta-estimation","title":"Theta Estimation","text":"<p>RustyStats can automatically estimate \u03b8:</p> <pre><code>// In diagnostics module\npub fn estimate_theta_moments(\n    y: &amp;Array1&lt;f64&gt;,\n    mu: &amp;Array1&lt;f64&gt;,\n) -&gt; f64 {\n    // Method of moments estimator\n    let n = y.len() as f64;\n    let mean = mu.mean().unwrap();\n\n    let var = y.iter().zip(mu.iter())\n        .map(|(yi, mui)| (yi - mui).powi(2) / mui)\n        .sum::&lt;f64&gt;() / n;\n\n    // Var = \u03bc + \u03bc\u00b2/\u03b8  \u2192  \u03b8 = \u03bc\u00b2 / (Var - \u03bc)\n    let excess_var = (var - 1.0).max(0.01);\n    mean / excess_var\n}\n</code></pre>"},{"location":"components/families/#testing-families","title":"Testing Families","text":"<p>Each family has unit tests:</p> <pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n    use approx::assert_relative_eq;\n\n    #[test]\n    fn test_poisson_variance() {\n        let family = PoissonFamily;\n        let mu = array![1.0, 2.0, 5.0];\n        let var = family.variance(&amp;mu);\n\n        assert_eq!(var, mu);  // V(\u03bc) = \u03bc\n    }\n\n    #[test]\n    fn test_poisson_deviance_zero() {\n        let family = PoissonFamily;\n        let y = array![0.0];\n        let mu = array![1.0];\n        let dev = family.unit_deviance(&amp;y, &amp;mu);\n\n        assert_relative_eq!(dev[0], 2.0, epsilon = 1e-10);\n    }\n\n    #[test]\n    fn test_binomial_variance_bounds() {\n        let family = BinomialFamily;\n        let mu = array![0.0001, 0.5, 0.9999];\n        let var = family.variance(&amp;mu);\n\n        // All variances should be positive and bounded\n        for v in var.iter() {\n            assert!(*v &gt; 0.0 &amp;&amp; *v &lt;= 0.25);\n        }\n    }\n}\n</code></pre>"},{"location":"components/links/","title":"Link Functions Component","text":"<p>This chapter covers the implementation of link functions in RustyStats.</p>"},{"location":"components/links/#code-location","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/links/\n\u251c\u2500\u2500 mod.rs       # Link trait definition\n\u251c\u2500\u2500 identity.rs  # Identity link\n\u251c\u2500\u2500 log.rs       # Log link\n\u2514\u2500\u2500 logit.rs     # Logit link\n</code></pre>"},{"location":"components/links/#the-link-trait","title":"The Link Trait","text":"<pre><code>pub trait Link: Send + Sync {\n    /// Display name\n    fn name(&amp;self) -&gt; &amp;str;\n\n    /// Forward transformation: \u03b7 = g(\u03bc)\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Inverse transformation: \u03bc = g\u207b\u00b9(\u03b7)\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Derivative: d\u03b7/d\u03bc\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n}\n</code></pre> <p>The <code>Send + Sync</code> bounds allow link functions to be used across threads (required for parallel IRLS).</p>"},{"location":"components/links/#identity-link","title":"Identity Link","text":"<p>File: <code>identity.rs</code></p> Property Formula Link \\(g(\\mu) = \\mu\\) Inverse \\(g^{-1}(\\eta) = \\eta\\) Derivative \\(g'(\\mu) = 1\\)"},{"location":"components/links/#implementation","title":"Implementation","text":"<pre><code>pub struct IdentityLink;\n\nimpl Link for IdentityLink {\n    fn name(&amp;self) -&gt; &amp;str { \"Identity\" }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.clone()\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        eta.clone()\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Array1::ones(mu.len())\n    }\n}\n</code></pre>"},{"location":"components/links/#notes","title":"Notes","text":"<ul> <li>Simplest link - no transformation</li> <li>Used with Gaussian family (linear regression)</li> <li>Predictions can be any real number (no bounds)</li> </ul>"},{"location":"components/links/#log-link","title":"Log Link","text":"<p>File: <code>log.rs</code></p> Property Formula Link \\(g(\\mu) = \\log(\\mu)\\) Inverse \\(g^{-1}(\\eta) = e^\\eta\\) Derivative \\(g'(\\mu) = 1/\\mu\\)"},{"location":"components/links/#implementation_1","title":"Implementation","text":"<pre><code>pub struct LogLink;\n\nimpl Link for LogLink {\n    fn name(&amp;self) -&gt; &amp;str { \"Log\" }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| {\n            // Clamp to avoid log(0) = -\u221e\n            m.max(1e-10).ln()\n        })\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        eta.mapv(|e| e.exp())\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| {\n            // Clamp to avoid division by zero\n            1.0 / m.max(1e-10)\n        })\n    }\n}\n</code></pre>"},{"location":"components/links/#numerical-considerations","title":"Numerical Considerations","text":"<ol> <li>Log of small values: Clamp \u03bc to avoid <code>log(0) = -\u221e</code></li> <li>Derivative at small \u03bc: Large derivative can cause numerical issues</li> <li>Exp overflow: Very large \u03b7 can cause <code>exp(\u03b7) = \u221e</code></li> </ol>"},{"location":"components/links/#overflow-protection","title":"Overflow Protection","text":"<p>For very large \u03b7, exp can overflow. Consider adding bounds:</p> <pre><code>fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n    eta.mapv(|e| {\n        // Prevent overflow: exp(700) \u2248 1e304\n        e.min(700.0).exp()\n    })\n}\n</code></pre>"},{"location":"components/links/#logit-link","title":"Logit Link","text":"<p>File: <code>logit.rs</code></p> Property Formula Link \\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\) Inverse \\(g^{-1}(\\eta) = \\frac{1}{1+e^{-\\eta}}\\) Derivative \\(g'(\\mu) = \\frac{1}{\\mu(1-\\mu)}\\)"},{"location":"components/links/#implementation_2","title":"Implementation","text":"<pre><code>pub struct LogitLink;\n\nimpl Link for LogitLink {\n    fn name(&amp;self) -&gt; &amp;str { \"Logit\" }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| {\n            // Clamp to (\u03b5, 1-\u03b5) to avoid log(0) and log(-x)\n            let m_safe = m.clamp(1e-10, 1.0 - 1e-10);\n            (m_safe / (1.0 - m_safe)).ln()\n        })\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        eta.mapv(|e| {\n            // Sigmoid function\n            // Use stable formulation to avoid overflow\n            if e &gt;= 0.0 {\n                1.0 / (1.0 + (-e).exp())\n            } else {\n                let exp_e = e.exp();\n                exp_e / (1.0 + exp_e)\n            }\n        })\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| {\n            let m_safe = m.clamp(1e-10, 1.0 - 1e-10);\n            1.0 / (m_safe * (1.0 - m_safe))\n        })\n    }\n}\n</code></pre>"},{"location":"components/links/#numerical-stability","title":"Numerical Stability","text":"<p>The sigmoid function can be computed stably:</p> <pre><code>// Naive (can overflow):\nlet mu = 1.0 / (1.0 + (-eta).exp());  // exp(-eta) overflows if eta &lt;&lt; 0\n\n// Stable:\nlet mu = if eta &gt;= 0.0 {\n    1.0 / (1.0 + (-eta).exp())\n} else {\n    let exp_eta = eta.exp();\n    exp_eta / (1.0 + exp_eta)\n};\n</code></pre>"},{"location":"components/links/#role-in-irls","title":"Role in IRLS","text":"<p>Link functions appear in two key places in IRLS:</p>"},{"location":"components/links/#1-working-weights","title":"1. Working Weights","text":"\\[ W = \\frac{1}{V(\\mu) \\cdot [g'(\\mu)]^2} \\] <pre><code>let link_deriv = link.derivative(&amp;mu);\nlet weights = variance.iter().zip(link_deriv.iter())\n    .map(|(v, d)| 1.0 / (v * d * d))\n    .collect();\n</code></pre>"},{"location":"components/links/#2-working-response","title":"2. Working Response","text":"\\[ z = \\eta + (y - \\mu) \\cdot g'(\\mu) \\] <pre><code>let z = Zip::from(&amp;eta).and(y).and(&amp;mu).and(&amp;link_deriv)\n    .map_collect(|&amp;e, &amp;yi, &amp;mui, &amp;d| {\n        e + (yi - mui) * d\n    });\n</code></pre>"},{"location":"components/links/#testing-link-functions","title":"Testing Link Functions","text":"<pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n    use approx::assert_relative_eq;\n\n    #[test]\n    fn test_log_link_roundtrip() {\n        let link = LogLink;\n        let mu = array![1.0, 2.0, 5.0];\n\n        let eta = link.link(&amp;mu);\n        let mu_back = link.inverse(&amp;eta);\n\n        for i in 0..mu.len() {\n            assert_relative_eq!(mu[i], mu_back[i], epsilon = 1e-10);\n        }\n    }\n\n    #[test]\n    fn test_logit_bounds() {\n        let link = LogitLink;\n\n        // Very negative eta \u2192 mu near 0\n        let eta_neg = array![-100.0];\n        let mu = link.inverse(&amp;eta_neg);\n        assert!(mu[0] &lt; 1e-40);\n        assert!(mu[0] &gt; 0.0);\n\n        // Very positive eta \u2192 mu near 1\n        let eta_pos = array![100.0];\n        let mu = link.inverse(&amp;eta_pos);\n        assert!(mu[0] &gt; 1.0 - 1e-40);\n        assert!(mu[0] &lt; 1.0);\n    }\n\n    #[test]\n    fn test_derivative_numerical() {\n        let link = LogLink;\n        let mu = array![1.0, 2.0, 5.0];\n        let deriv = link.derivative(&amp;mu);\n\n        // Compare to numerical derivative\n        let eps = 1e-7;\n        for i in 0..mu.len() {\n            let mu_plus = mu[i] + eps;\n            let mu_minus = mu[i] - eps;\n            let numerical = (mu_plus.ln() - mu_minus.ln()) / (2.0 * eps);\n            assert_relative_eq!(deriv[i], numerical, epsilon = 1e-5);\n        }\n    }\n}\n</code></pre>"},{"location":"components/links/#adding-a-new-link-function","title":"Adding a New Link Function","text":"<p>To add a new link (e.g., Probit):</p>"},{"location":"components/links/#1-create-the-file","title":"1. Create the File","text":"<pre><code>// links/probit.rs\nuse ndarray::Array1;\nuse statrs::distribution::{Normal, ContinuousCDF};\n\npub struct ProbitLink;\n\nimpl Link for ProbitLink {\n    fn name(&amp;self) -&gt; &amp;str { \"Probit\" }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        let normal = Normal::new(0.0, 1.0).unwrap();\n        mu.mapv(|m| {\n            let m_safe = m.clamp(1e-10, 1.0 - 1e-10);\n            normal.inverse_cdf(m_safe)\n        })\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        let normal = Normal::new(0.0, 1.0).unwrap();\n        eta.mapv(|e| normal.cdf(e))\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        let normal = Normal::new(0.0, 1.0).unwrap();\n        mu.mapv(|m| {\n            let m_safe = m.clamp(1e-10, 1.0 - 1e-10);\n            let z = normal.inverse_cdf(m_safe);\n            1.0 / normal.pdf(z)\n        })\n    }\n}\n</code></pre>"},{"location":"components/links/#2-export-from-modrs","title":"2. Export from mod.rs","text":"<pre><code>mod probit;\npub use probit::ProbitLink;\n</code></pre>"},{"location":"components/links/#3-add-python-binding","title":"3. Add Python Binding","text":"<p>See Adding a New Link for complete instructions.</p>"},{"location":"components/solvers/","title":"Solvers","text":"<p>RustyStats includes two main solvers: IRLS for standard GLMs and Coordinate Descent for regularized models. This chapter covers implementation details and optimization strategies.</p>"},{"location":"components/solvers/#code-location","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/solvers/\n\u251c\u2500\u2500 mod.rs                  # Re-exports\n\u251c\u2500\u2500 irls.rs                 # IRLS implementation (~1300 lines)\n\u2514\u2500\u2500 coordinate_descent.rs   # Regularized solver (~900 lines)\n</code></pre>"},{"location":"components/solvers/#irls-solver","title":"IRLS Solver","text":""},{"location":"components/solvers/#configuration","title":"Configuration","text":"<pre><code>pub struct IRLSConfig {\n    pub max_iterations: usize,  // Default: 25\n    pub tolerance: f64,         // Default: 1e-8\n    pub min_weight: f64,        // Default: 1e-10\n    pub verbose: bool,          // Default: false\n}\n</code></pre>"},{"location":"components/solvers/#main-function-signature","title":"Main Function Signature","text":"<pre><code>pub fn fit_glm_full(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,\n    link: &amp;dyn Link,\n    config: &amp;IRLSConfig,\n    offset: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; Result&lt;IRLSResult&gt;\n</code></pre>"},{"location":"components/solvers/#algorithm-steps","title":"Algorithm Steps","text":"<pre><code>// 1. Initialize\nlet mut mu = family.initialize_mu(y);\nlet mut eta = link.link(&amp;mu);\n\n// 2. Main loop\nfor iter in 0..config.max_iterations {\n    // 2a. Compute variance and link derivative\n    let variance = family.variance(&amp;mu);\n    let link_deriv = link.derivative(&amp;mu);\n\n    // 2b. Compute working weights\n    //     W = 1 / [V(\u03bc) \u00d7 g'(\u03bc)\u00b2]\n    let w = compute_irls_weights(&amp;variance, &amp;link_deriv, weights);\n\n    // 2c. Compute working response\n    //     z = \u03b7 + (y - \u03bc) \u00d7 g'(\u03bc)\n    let z = compute_working_response(y, &amp;mu, &amp;eta, &amp;link_deriv);\n\n    // 2d. Solve weighted least squares\n    //     (X'WX)\u03b2 = X'Wz\n    let (beta, cov) = solve_wls_parallel(x, &amp;w, &amp;z)?;\n\n    // 2e. Update predictions\n    eta = x.dot(&amp;beta);\n    if let Some(off) = offset {\n        eta = &amp;eta + off;\n    }\n    mu = link.inverse(&amp;eta);\n\n    // 2f. Check convergence\n    let deviance = family.deviance(y, &amp;mu, weights);\n    if converged(deviance, prev_deviance, config.tolerance) {\n        return Ok(build_result(..., converged: true));\n    }\n    prev_deviance = deviance;\n}\n</code></pre>"},{"location":"components/solvers/#parallel-wls-solver","title":"Parallel WLS Solver","text":"<p>The weighted least squares step dominates computation time. We parallelize it:</p> <pre><code>fn solve_wls_parallel(\n    x: &amp;Array2&lt;f64&gt;,\n    w: &amp;Array1&lt;f64&gt;,\n    z: &amp;Array1&lt;f64&gt;,\n) -&gt; Result&lt;(Array1&lt;f64&gt;, Array2&lt;f64&gt;)&gt; {\n    let n = x.nrows();\n    let p = x.ncols();\n\n    // Parallel fold-reduce for X'WX and X'Wz\n    let (xtwx, xtwz) = (0..n).into_par_iter()\n        .fold(\n            || (vec![0.0; p * p], vec![0.0; p]),\n            |(mut gram, mut moment), i| {\n                let wi = w[i];\n                let wzi = wi * z[i];\n\n                for j in 0..p {\n                    let xij = x[[i, j]];\n                    moment[j] += xij * wzi;\n\n                    // Upper triangle only (symmetric)\n                    for k in j..p {\n                        gram[j * p + k] += wi * xij * x[[i, k]];\n                    }\n                }\n                (gram, moment)\n            }\n        )\n        .reduce(\n            || (vec![0.0; p * p], vec![0.0; p]),\n            |(mut g1, mut m1), (g2, m2)| {\n                for i in 0..g1.len() { g1[i] += g2[i]; }\n                for i in 0..m1.len() { m1[i] += m2[i]; }\n                (g1, m1)\n            }\n        );\n\n    // Convert to matrices and solve\n    let gram_matrix = reshape_to_symmetric(xtwx, p);\n    let moment_vector = Array1::from_vec(xtwz);\n\n    cholesky_solve(&amp;gram_matrix, &amp;moment_vector)\n}\n</code></pre>"},{"location":"components/solvers/#numerical-stability","title":"Numerical Stability","text":"<pre><code>// Clamp weights to avoid numerical issues\nfn compute_irls_weights(\n    variance: &amp;Array1&lt;f64&gt;,\n    link_deriv: &amp;Array1&lt;f64&gt;,\n    prior_weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    min_weight: f64,\n) -&gt; Array1&lt;f64&gt; {\n    let n = variance.len();\n    let mut w = Array1::zeros(n);\n\n    for i in 0..n {\n        let v = variance[i].max(min_weight);\n        let d = link_deriv[i];\n        let pw = prior_weights.map_or(1.0, |pw| pw[i]);\n\n        w[i] = (pw / (v * d * d)).max(min_weight);\n    }\n\n    w\n}\n</code></pre>"},{"location":"components/solvers/#coordinate-descent-solver","title":"Coordinate Descent Solver","text":"<p>For regularized GLMs (Lasso, Ridge, Elastic Net).</p>"},{"location":"components/solvers/#configuration_1","title":"Configuration","text":"<pre><code>pub struct RegularizationConfig {\n    pub alpha: f64,        // Overall penalty strength\n    pub l1_ratio: f64,     // Mix: 1.0 = Lasso, 0.0 = Ridge\n    pub standardize: bool, // Standardize features\n}\n</code></pre>"},{"location":"components/solvers/#algorithm-overview","title":"Algorithm Overview","text":"<p>Coordinate descent optimizes one coefficient at a time while holding others fixed:</p> <pre><code>pub fn fit_glm_coordinate_descent(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,\n    link: &amp;dyn Link,\n    irls_config: &amp;IRLSConfig,\n    reg_config: &amp;RegularizationConfig,\n    offset: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; Result&lt;IRLSResult&gt; {\n    // Outer loop: IRLS for working response\n    for outer in 0..irls_config.max_iterations {\n        // Compute working response and weights\n        let (z, w) = compute_working_response_weights(...);\n\n        // Precompute Gram matrix (X'WX) once per outer iteration\n        let gram = compute_gram_matrix(x, &amp;w);\n\n        // Inner loop: Coordinate descent\n        for inner in 0..max_inner_iterations {\n            let max_change = 0.0;\n\n            for j in 0..p {\n                // Skip intercept from penalization\n                if j == 0 &amp;&amp; has_intercept {\n                    // Update without penalty\n                    beta[j] = update_intercept(...);\n                    continue;\n                }\n\n                // Compute partial residual\n                let r_j = z - x_without_j.dot(&amp;beta_without_j);\n\n                // Coordinate update with soft thresholding\n                let xwz_j = weighted_dot(&amp;x.column(j), &amp;w, &amp;r_j);\n                let xwx_jj = gram[[j, j]];\n\n                let beta_old = beta[j];\n                beta[j] = soft_threshold(\n                    xwz_j,\n                    reg_config.alpha * reg_config.l1_ratio\n                ) / (xwx_jj + reg_config.alpha * (1.0 - reg_config.l1_ratio));\n\n                max_change = max_change.max((beta[j] - beta_old).abs());\n            }\n\n            if max_change &lt; inner_tolerance {\n                break;\n            }\n        }\n\n        // Update predictions\n        // Check outer convergence\n    }\n}\n</code></pre>"},{"location":"components/solvers/#soft-thresholding","title":"Soft Thresholding","text":"<p>The key operation for L1 penalty:</p> <pre><code>pub fn soft_threshold(z: f64, gamma: f64) -&gt; f64 {\n    if z &gt; gamma {\n        z - gamma\n    } else if z &lt; -gamma {\n        z + gamma\n    } else {\n        0.0  // Sets coefficient to exactly zero\n    }\n}\n</code></pre> <p>Visualization:</p> <pre><code>    Output\n      \u2191\n      \u2502      /\n      \u2502     /\n      \u2502    /\n\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Input\n     /\u2502    \u03b3\n    / \u2502\n   /  \u2502\n      \u2502\n</code></pre>"},{"location":"components/solvers/#active-set-strategy","title":"Active Set Strategy","text":"<p>For efficiency with many zero coefficients:</p> <pre><code>// Track which coefficients are non-zero\nlet mut active_set: Vec&lt;usize&gt; = (0..p).collect();\n\nfor cycle in 0..max_cycles {\n    // First pass: update only active set\n    for &amp;j in &amp;active_set {\n        update_coefficient(j, ...);\n    }\n\n    // Periodically: check all coefficients\n    if cycle % 10 == 0 {\n        active_set.clear();\n        for j in 0..p {\n            if beta[j].abs() &gt; 1e-10 || should_enter(j, ...) {\n                active_set.push(j);\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"components/solvers/#warm-starts","title":"Warm Starts","text":"<p>When computing regularization paths, use previous solution as starting point:</p> <pre><code>pub fn lasso_path(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    alphas: &amp;[f64],  // Decreasing sequence\n    ...\n) -&gt; Vec&lt;IRLSResult&gt; {\n    let mut results = Vec::with_capacity(alphas.len());\n    let mut beta = Array1::zeros(p);  // Start from zero\n\n    for &amp;alpha in alphas {\n        // Warm start from previous solution\n        let config = RegularizationConfig { alpha, ... };\n        let result = fit_glm_coordinate_descent(\n            ...,\n            initial_beta: Some(&amp;beta),\n        )?;\n\n        beta = result.coefficients.clone();\n        results.push(result);\n    }\n\n    results\n}\n</code></pre>"},{"location":"components/solvers/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"components/solvers/#1-cache-friendly-access","title":"1. Cache-Friendly Access","text":"<p>Store Gram matrix in flat Vec for better cache performance:</p> <pre><code>// Instead of Array2&lt;f64&gt;\nlet gram_flat: Vec&lt;f64&gt; = vec![0.0; p * p];\n\n// Access with manual indexing\nlet value = gram_flat[i * p + j];\n</code></pre>"},{"location":"components/solvers/#2-simd-friendly-operations","title":"2. SIMD-Friendly Operations","text":"<p>Structure loops for auto-vectorization:</p> <pre><code>// Good: contiguous memory access\nfor i in 0..n {\n    result += x[i] * w[i];\n}\n\n// Bad: strided access\nfor i in 0..n {\n    result += x[[i, j]] * w[i];  // Column access is strided\n}\n</code></pre>"},{"location":"components/solvers/#3-parallel-reduction","title":"3. Parallel Reduction","text":"<p>Use Rayon's parallel fold-reduce pattern:</p> <pre><code>let sum = (0..n).into_par_iter()\n    .map(|i| compute(i))\n    .reduce(|| 0.0, |a, b| a + b);\n</code></pre>"},{"location":"components/solvers/#4-avoid-allocations-in-hot-loops","title":"4. Avoid Allocations in Hot Loops","text":"<p>Pre-allocate and reuse buffers:</p> <pre><code>// Pre-allocate outside loop\nlet mut working_buffer = Array1::zeros(n);\n\nfor iter in 0..max_iterations {\n    // Reuse buffer\n    working_buffer.fill(0.0);\n    compute_into(&amp;mut working_buffer);\n}\n</code></pre>"},{"location":"components/solvers/#benchmarking","title":"Benchmarking","text":""},{"location":"components/solvers/#simple-benchmark","title":"Simple Benchmark","text":"<pre><code>#[cfg(test)]\nmod benchmarks {\n    use super::*;\n    use std::time::Instant;\n\n    #[test]\n    #[ignore]  // Run with: cargo test --release -- --ignored\n    fn bench_irls() {\n        let n = 100_000;\n        let p = 50;\n\n        let y = generate_poisson_data(n);\n        let x = generate_design_matrix(n, p);\n\n        let start = Instant::now();\n        let result = fit_glm(&amp;y, &amp;x, &amp;PoissonFamily, &amp;LogLink, &amp;IRLSConfig::default());\n        let elapsed = start.elapsed();\n\n        println!(\"IRLS: {:?} ({} iterations)\", elapsed, result.unwrap().iterations);\n    }\n}\n</code></pre>"},{"location":"components/solvers/#expected-performance","title":"Expected Performance","text":"Dataset Size Features Family Time (release) 100K 20 Poisson ~100ms 500K 50 Poisson ~800ms 1M 100 Gaussian ~2s <p>Performance scales roughly as O(n \u00d7 p\u00b2) for IRLS.</p>"},{"location":"components/splines/","title":"Splines Component","text":"<p>Spline basis functions allow modeling non-linear relationships while staying within the GLM framework. RustyStats implements B-splines and natural splines.</p>"},{"location":"components/splines/#code-location","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/splines/\n\u2514\u2500\u2500 mod.rs  # All spline functionality\n\npython/rustystats/splines.py  # Python API\n</code></pre>"},{"location":"components/splines/#why-splines","title":"Why Splines?","text":"<p>Instead of assuming a linear effect: [ \\eta = \\beta_0 + \\beta_1 x ]</p> <p>Splines allow flexible shapes: [ \\eta = \\beta_0 + \\sum_{j=1}^{k} \\beta_j B_j(x) ]</p> <p>where \\(B_j(x)\\) are basis functions.</p>"},{"location":"components/splines/#b-splines","title":"B-Splines","text":""},{"location":"components/splines/#mathematical-background","title":"Mathematical Background","text":"<p>B-splines are piecewise polynomials defined by: - Degree: Polynomial degree (default 3 = cubic) - Knots: Points where pieces join</p> <p>The Cox-de Boor recursion formula:</p> \\[ B_{i,0}(x) = \\begin{cases} 1 &amp; \\text{if } t_i \\leq x &lt; t_{i+1} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] \\[ B_{i,k}(x) = \\frac{x - t_i}{t_{i+k} - t_i} B_{i,k-1}(x) + \\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} B_{i+1,k-1}(x) \\]"},{"location":"components/splines/#implementation","title":"Implementation","text":"<pre><code>pub const DEFAULT_DEGREE: usize = 3;\n\n/// Compute B-spline basis matrix\npub fn bs_basis(\n    x: &amp;Array1&lt;f64&gt;,\n    knots: &amp;[f64],\n    degree: usize,\n) -&gt; Array2&lt;f64&gt; {\n    let n = x.len();\n    let n_basis = knots.len() - degree - 1;\n    let mut basis = Array2::zeros((n, n_basis));\n\n    for i in 0..n {\n        for j in 0..n_basis {\n            basis[[i, j]] = b_spline_value(x[i], j, degree, knots);\n        }\n    }\n\n    basis\n}\n\n/// Single B-spline basis function via Cox-de Boor\nfn b_spline_value(x: f64, i: usize, k: usize, knots: &amp;[f64]) -&gt; f64 {\n    if k == 0 {\n        // Base case: indicator function\n        if x &gt;= knots[i] &amp;&amp; x &lt; knots[i + 1] {\n            1.0\n        } else if i + 1 == knots.len() - 1 &amp;&amp; x == knots[i + 1] {\n            // Include right endpoint\n            1.0\n        } else {\n            0.0\n        }\n    } else {\n        // Recursive case\n        let left_denom = knots[i + k] - knots[i];\n        let left = if left_denom.abs() &gt; 1e-10 {\n            (x - knots[i]) / left_denom * b_spline_value(x, i, k - 1, knots)\n        } else {\n            0.0\n        };\n\n        let right_denom = knots[i + k + 1] - knots[i + 1];\n        let right = if right_denom.abs() &gt; 1e-10 {\n            (knots[i + k + 1] - x) / right_denom * b_spline_value(x, i + 1, k - 1, knots)\n        } else {\n            0.0\n        };\n\n        left + right\n    }\n}\n</code></pre>"},{"location":"components/splines/#knot-placement","title":"Knot Placement","text":"<p>Knots are placed at quantiles of the data:</p> <pre><code>pub fn compute_knots(\n    x: &amp;Array1&lt;f64&gt;,\n    n_interior_knots: usize,\n    degree: usize,\n    boundary_knots: Option&lt;(f64, f64)&gt;,\n) -&gt; Vec&lt;f64&gt; {\n    let (x_min, x_max) = boundary_knots.unwrap_or_else(|| {\n        let mut sorted = x.to_vec();\n        sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());\n        (sorted[0], sorted[sorted.len() - 1])\n    });\n\n    let mut knots = Vec::with_capacity(n_interior_knots + 2 * (degree + 1));\n\n    // Boundary knots (repeated degree+1 times)\n    for _ in 0..=degree {\n        knots.push(x_min);\n    }\n\n    // Interior knots at quantiles\n    for i in 1..=n_interior_knots {\n        let q = i as f64 / (n_interior_knots + 1) as f64;\n        knots.push(quantile(x, q));\n    }\n\n    // Right boundary knots\n    for _ in 0..=degree {\n        knots.push(x_max);\n    }\n\n    knots\n}\n</code></pre>"},{"location":"components/splines/#degrees-of-freedom","title":"Degrees of Freedom","text":"<p>The <code>df</code> parameter controls flexibility: - <code>df</code> = number of basis functions - Interior knots = <code>df - degree - 1</code> (for B-splines without intercept)</p> <pre><code>pub fn bs(\n    x: &amp;Array1&lt;f64&gt;,\n    df: usize,\n    degree: usize,\n    boundary_knots: Option&lt;(f64, f64)&gt;,\n) -&gt; Array2&lt;f64&gt; {\n    // df includes intercept basis\n    let n_interior = df - degree - 1;\n    let knots = compute_knots(x, n_interior, degree, boundary_knots);\n    bs_basis(x, &amp;knots, degree)\n}\n</code></pre>"},{"location":"components/splines/#natural-splines","title":"Natural Splines","text":"<p>Natural splines add constraints for better extrapolation: - Linear beyond the boundary knots - Reduces df by 2 compared to B-splines</p>"},{"location":"components/splines/#implementation_1","title":"Implementation","text":"<pre><code>pub fn ns_basis(\n    x: &amp;Array1&lt;f64&gt;,\n    df: usize,\n    boundary_knots: Option&lt;(f64, f64)&gt;,\n) -&gt; Array2&lt;f64&gt; {\n    let degree = 3;  // Natural splines are cubic\n\n    // Compute B-spline basis\n    let n_interior = df - 1;  // Natural splines use more interior knots\n    let knots = compute_knots(x, n_interior, degree, boundary_knots);\n    let bs = bs_basis(x, &amp;knots, degree);\n\n    // Apply natural spline constraints\n    // The constraint matrix ensures:\n    // f''(x_min) = 0 and f''(x_max) = 0\n    let constraint_matrix = compute_natural_spline_constraint(&amp;knots, degree);\n\n    // Project B-spline basis onto constrained space\n    bs.dot(&amp;constraint_matrix)\n}\n</code></pre>"},{"location":"components/splines/#properties","title":"Properties","text":"Property B-Spline Natural Spline Behavior at boundaries Polynomial Linear Extrapolation Unstable Stable df for same flexibility Higher Lower Recommended for Interpolation Prediction"},{"location":"components/splines/#python-api","title":"Python API","text":"<pre><code># python/rustystats/splines.py\n\ndef bs(x, df=None, knots=None, degree=3, boundary_knots=None, \n       include_intercept=False):\n    \"\"\"\n    B-spline basis matrix.\n\n    Parameters\n    ----------\n    x : array-like\n        Values at which to evaluate the spline basis\n    df : int, optional\n        Degrees of freedom (number of columns)\n    degree : int, default 3\n        Polynomial degree (3 = cubic)\n    boundary_knots : tuple, optional\n        (min, max) boundary knots\n    include_intercept : bool, default False\n        Whether to include constant basis function\n\n    Returns\n    -------\n    ndarray\n        Basis matrix of shape (len(x), df)\n    \"\"\"\n    x = np.asarray(x, dtype=np.float64)\n\n    if df is None:\n        df = 4  # Default\n\n    # Call Rust implementation\n    from rustystats._rustystats import bs_basis\n    basis = bs_basis(x, df, degree, boundary_knots)\n\n    if not include_intercept:\n        # Drop first column (intercept absorbed)\n        basis = basis[:, 1:]\n\n    return basis\n</code></pre>"},{"location":"components/splines/#formula-integration","title":"Formula Integration","text":"<p>Splines can be used directly in formulas:</p> <pre><code>result = rs.glm(\n    \"y ~ bs(age, df=5) + ns(income, df=4) + C(region)\",\n    data=data,\n    family=\"poisson\"\n).fit()\n</code></pre> <p>The formula parser recognizes <code>bs()</code> and <code>ns()</code> terms:</p> <pre><code># In formula.py\nclass SplineTerm:\n    def __init__(self, variable, df, kind='bs', degree=3):\n        self.variable = variable\n        self.df = df\n        self.kind = kind  # 'bs' or 'ns'\n        self.degree = degree\n\n    def build(self, data):\n        x = data[self.variable].to_numpy()\n        if self.kind == 'bs':\n            return bs(x, df=self.df, degree=self.degree)\n        else:\n            return ns(x, df=self.df)\n</code></pre>"},{"location":"components/splines/#parallel-computation","title":"Parallel Computation","text":"<p>For large datasets, parallelize over observations:</p> <pre><code>pub fn bs_basis_parallel(\n    x: &amp;Array1&lt;f64&gt;,\n    knots: &amp;[f64],\n    degree: usize,\n) -&gt; Array2&lt;f64&gt; {\n    let n = x.len();\n    let n_basis = knots.len() - degree - 1;\n\n    // Parallel over rows\n    let rows: Vec&lt;Vec&lt;f64&gt;&gt; = (0..n).into_par_iter()\n        .map(|i| {\n            (0..n_basis)\n                .map(|j| b_spline_value(x[i], j, degree, knots))\n                .collect()\n        })\n        .collect();\n\n    // Assemble matrix\n    let mut basis = Array2::zeros((n, n_basis));\n    for (i, row) in rows.into_iter().enumerate() {\n        for (j, val) in row.into_iter().enumerate() {\n            basis[[i, j]] = val;\n        }\n    }\n\n    basis\n}\n</code></pre>"},{"location":"components/splines/#testing","title":"Testing","text":"<pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_bs_partition_of_unity() {\n        // B-splines should sum to 1 at any point\n        let x = array![0.0, 0.25, 0.5, 0.75, 1.0];\n        let knots = vec![0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0];\n        let basis = bs_basis(&amp;x, &amp;knots, 3);\n\n        for i in 0..x.len() {\n            let sum: f64 = basis.row(i).sum();\n            assert!((sum - 1.0).abs() &lt; 1e-10);\n        }\n    }\n\n    #[test]\n    fn test_ns_linear_extrapolation() {\n        // Natural splines should extrapolate linearly\n        let x_train = Array1::linspace(0.0, 1.0, 100);\n        let x_test = array![-0.5, 1.5];\n\n        // Fit basis on training data\n        let basis_train = ns_basis(&amp;x_train, 4, Some((0.0, 1.0)));\n        let basis_test = ns_basis(&amp;x_test, 4, Some((0.0, 1.0)));\n\n        // Linear extrapolation means second derivative = 0\n        // (Would need derivative implementation to test properly)\n    }\n\n    #[test]\n    fn test_df_vs_columns() {\n        let x = Array1::linspace(0.0, 1.0, 50);\n\n        for df in 3..10 {\n            let basis = bs(&amp;x, df, 3, None);\n            assert_eq!(basis.ncols(), df);\n        }\n    }\n}\n</code></pre>"},{"location":"components/splines/#performance","title":"Performance","text":"Dataset Size df Time (release) 10K 5 ~5ms 100K 10 ~50ms 1M 10 ~500ms <p>Performance scales as O(n \u00d7 df \u00d7 degree).</p>"},{"location":"components/target-encoding/","title":"Target Encoding Component","text":"<p>Target encoding converts high-cardinality categorical variables into numerical features using the target variable. RustyStats implements CatBoost-style ordered target statistics to prevent target leakage.</p>"},{"location":"components/target-encoding/#code-location","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/target_encoding/\n\u2514\u2500\u2500 mod.rs  # All target encoding functionality\n\npython/rustystats/target_encoding.py  # Python API\n</code></pre>"},{"location":"components/target-encoding/#why-target-encoding","title":"Why Target Encoding?","text":""},{"location":"components/target-encoding/#the-problem-with-one-hot-encoding","title":"The Problem with One-Hot Encoding","text":"<p>For a categorical with k levels, one-hot creates k-1 columns: - 10 levels \u2192 9 columns - 1000 levels \u2192 999 columns - 100,000 levels (e.g., ZIP codes) \u2192 99,999 columns</p> <p>This causes: - Memory explosion - Overfitting - Slow training</p>"},{"location":"components/target-encoding/#target-encoding-solution","title":"Target Encoding Solution","text":"<p>Replace each category with a single number based on the target:</p> <pre><code>Category    Target    Encoded\n--------    ------    -------\nToyota      1.0       0.65\nFord        0.0       0.40\nBMW         1.0       0.75\nToyota      0.5       0.65\n...\n</code></pre>"},{"location":"components/target-encoding/#the-target-leakage-problem","title":"The Target Leakage Problem","text":"<p>Naive target encoding causes target leakage:</p> <pre><code># WRONG: Uses observation's own target in encoding\nfor category in categories:\n    encoded[category] = mean(target[category == categories])\n</code></pre> <p>The model sees information about the target it's trying to predict \u2192 overfitting.</p>"},{"location":"components/target-encoding/#catboost-solution-ordered-statistics","title":"CatBoost Solution: Ordered Statistics","text":"<p>CatBoost's approach computes encodings using only \"past\" observations:</p> <pre><code>For observation i in random order:\n    encoded[i] = (sum of target for this category BEFORE i) / (count BEFORE i)\n</code></pre>"},{"location":"components/target-encoding/#algorithm","title":"Algorithm","text":"<pre><code>pub fn target_encode(\n    categories: &amp;[String],\n    target: &amp;Array1&lt;f64&gt;,\n    prior_weight: f64,\n    n_permutations: usize,\n    seed: u64,\n) -&gt; (Array1&lt;f64&gt;, f64, HashMap&lt;String, LevelStatistics&gt;) {\n    let n = categories.len();\n    let prior = target.mean().unwrap();  // Global mean\n\n    // Average across multiple permutations for stability\n    let mut encoded_sum = Array1::zeros(n);\n\n    for perm_idx in 0..n_permutations {\n        // Random permutation of indices\n        let permutation = generate_permutation(n, seed + perm_idx as u64);\n\n        // Running statistics per category\n        let mut category_sum: HashMap&lt;String, f64&gt; = HashMap::new();\n        let mut category_count: HashMap&lt;String, usize&gt; = HashMap::new();\n\n        let mut encoded_perm = Array1::zeros(n);\n\n        for &amp;i in &amp;permutation {\n            let cat = &amp;categories[i];\n\n            // Get statistics from observations BEFORE this one\n            let sum_before = *category_sum.get(cat).unwrap_or(&amp;0.0);\n            let count_before = *category_count.get(cat).unwrap_or(&amp;0);\n\n            // Compute encoding with regularization toward prior\n            encoded_perm[i] = (sum_before + prior * prior_weight) \n                            / (count_before as f64 + prior_weight);\n\n            // Update running statistics\n            *category_sum.entry(cat.clone()).or_insert(0.0) += target[i];\n            *category_count.entry(cat.clone()).or_insert(0) += 1;\n        }\n\n        encoded_sum = encoded_sum + encoded_perm;\n    }\n\n    // Average across permutations\n    let encoded = encoded_sum / n_permutations as f64;\n\n    // Compute full statistics for prediction\n    let stats = compute_full_statistics(categories, target, prior, prior_weight);\n\n    (encoded, prior, stats)\n}\n</code></pre>"},{"location":"components/target-encoding/#key-properties","title":"Key Properties","text":"<ol> <li>No leakage: Each observation's encoding uses only prior observations</li> <li>Regularization: <code>prior_weight</code> shrinks rare categories toward global mean</li> <li>Stability: Multiple permutations reduce variance</li> </ol>"},{"location":"components/target-encoding/#regularization-with-prior-weight","title":"Regularization with Prior Weight","text":"<p>The encoding formula:</p> \\[ \\text{encoded}_i = \\frac{\\sum_{j &lt; i} y_j + \\mu \\cdot w}{\\text{count}_{j &lt; i} + w} \\] <p>where: - \\(\\mu\\) = global mean (prior) - \\(w\\) = prior weight</p> prior_weight Effect 0 No regularization (pure empirical mean) 1 Mild regularization 10 Strong regularization (rare categories \u2192 global mean)"},{"location":"components/target-encoding/#example","title":"Example","text":"<p>For a rare category with 2 observations (target = 1, 1):</p> prior_weight Encoded value (prior=0.5) 0 1.0 1 (2.0 + 0.5\u00d71) / (2 + 1) = 0.83 10 (2.0 + 0.5\u00d710) / (2 + 10) = 0.58"},{"location":"components/target-encoding/#applying-to-new-data","title":"Applying to New Data","text":"<p>For prediction, use full training statistics:</p> <pre><code>pub fn apply_target_encoding(\n    categories: &amp;[String],\n    stats: &amp;HashMap&lt;String, LevelStatistics&gt;,\n    prior: f64,\n) -&gt; Array1&lt;f64&gt; {\n    categories.iter()\n        .map(|cat| {\n            stats.get(cat)\n                .map(|s| s.mean)\n                .unwrap_or(prior)  // Unseen categories get prior\n        })\n        .collect()\n}\n</code></pre>"},{"location":"components/target-encoding/#python-api","title":"Python API","text":""},{"location":"components/target-encoding/#direct-api","title":"Direct API","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\ncategories = [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\"]\ntarget = np.array([1.0, 0.0, 0.5, 1.0, 0.2, 0.8])\n\n# Encode\nencoded, column_name, prior, stats = rs.target_encode(\n    categories, \n    target,\n    column_name=\"category\",\n    prior_weight=1.0,\n    n_permutations=4,\n    seed=42\n)\n\n# Apply to new data\nnew_cats = [\"A\", \"B\", \"D\"]  # D is unseen\nnew_encoded = rs.apply_target_encoding(new_cats, stats, prior)\n# D gets the prior (global mean)\n</code></pre>"},{"location":"components/target-encoding/#sklearn-style-api","title":"Sklearn-Style API","text":"<pre><code>encoder = rs.TargetEncoder(prior_weight=1.0, n_permutations=4)\n\n# Fit and transform training data\ntrain_encoded = encoder.fit_transform(train_categories, train_target)\n\n# Transform test data (uses full training statistics)\ntest_encoded = encoder.transform(test_categories)\n</code></pre>"},{"location":"components/target-encoding/#formula-api","title":"Formula API","text":"<pre><code>result = rs.glm(\n    \"ClaimNb ~ TE(VehicleBrand) + TE(ZipCode, prior_weight=2.0) + Age\",\n    data=data,\n    family=\"poisson\",\n    offset=\"Exposure\"\n).fit()\n</code></pre>"},{"location":"components/target-encoding/#data-structures","title":"Data Structures","text":"<pre><code>pub struct LevelStatistics {\n    pub level: String,\n    pub count: usize,\n    pub sum: f64,\n    pub mean: f64,\n}\n\npub struct TargetEncoding {\n    pub encoded_values: Array1&lt;f64&gt;,\n    pub prior: f64,\n    pub level_stats: HashMap&lt;String, LevelStatistics&gt;,\n    pub column_name: String,\n}\n\npub struct TargetEncodingConfig {\n    pub prior_weight: f64,\n    pub n_permutations: usize,\n    pub seed: u64,\n}\n</code></pre>"},{"location":"components/target-encoding/#testing","title":"Testing","text":"<pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_no_leakage() {\n        // With 1 observation per category, encoding should equal prior\n        // (no past observations to use)\n        let categories = vec![\"A\".into(), \"B\".into(), \"C\".into()];\n        let target = array![1.0, 0.0, 0.5];\n\n        let (encoded, prior, _) = target_encode(\n            &amp;categories, &amp;target, 1.0, 1, 42\n        );\n\n        // First observation of each category should be regularized toward prior\n        let expected_prior = target.mean().unwrap();\n        for &amp;e in encoded.iter() {\n            // With prior_weight=1, first obs: (0 + prior*1) / (0 + 1) = prior\n            assert!((e - expected_prior).abs() &lt; 0.01);\n        }\n    }\n\n    #[test]\n    fn test_unseen_category() {\n        let categories = vec![\"A\".into(), \"A\".into()];\n        let target = array![1.0, 1.0];\n\n        let (_, prior, stats) = target_encode(&amp;categories, &amp;target, 1.0, 1, 42);\n\n        // Apply to unseen category\n        let new_cats = vec![\"B\".into()];\n        let new_encoded = apply_target_encoding(&amp;new_cats, &amp;stats, prior);\n\n        // Unseen category gets prior\n        assert!((new_encoded[0] - prior).abs() &lt; 1e-10);\n    }\n\n    #[test]\n    fn test_prior_weight_effect() {\n        let categories = vec![\"A\".into(), \"A\".into()];\n        let target = array![1.0, 1.0];\n\n        // Low prior weight\n        let (_, _, stats_low) = target_encode(&amp;categories, &amp;target, 0.1, 10, 42);\n\n        // High prior weight\n        let (_, _, stats_high) = target_encode(&amp;categories, &amp;target, 10.0, 10, 42);\n\n        // High prior weight should pull mean toward prior\n        let prior = 1.0;  // mean of [1.0, 1.0]\n        assert!(stats_high.get(\"A\").unwrap().mean &lt; stats_low.get(\"A\").unwrap().mean);\n    }\n}\n</code></pre>"},{"location":"components/target-encoding/#when-to-use-target-encoding","title":"When to Use Target Encoding","text":""},{"location":"components/target-encoding/#good-use-cases","title":"Good Use Cases","text":"<ul> <li>High-cardinality categoricals (100s or 1000s of levels)</li> <li>ZIP codes, product IDs, user IDs</li> <li>When feature count would explode with one-hot</li> </ul>"},{"location":"components/target-encoding/#avoid-when","title":"Avoid When","text":"<ul> <li>Low-cardinality (&lt; 10 levels): one-hot is fine</li> <li>Need interpretable per-level effects</li> <li>Target is unavailable at prediction time</li> </ul>"},{"location":"components/target-encoding/#comparison","title":"Comparison","text":"Encoding Columns Leakage Risk Interpretability One-Hot k-1 None High Target (naive) 1 High Medium Target (CatBoost) 1 Low Medium Ordinal 1 None Low"},{"location":"getting-started/installation/","title":"Installation","text":"<p>RustyStats requires Python 3.9+ and a Rust toolchain for building from source.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#python","title":"Python","text":"<p>Ensure you have Python 3.9 or later: <pre><code>python --version  # Should be 3.9+\n</code></pre></p>"},{"location":"getting-started/installation/#rust","title":"Rust","text":"<p>Install Rust via rustup if not already installed: <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre></p> <p>Verify installation: <pre><code>rustc --version  # Should show version 1.70+\ncargo --version\n</code></pre></p>"},{"location":"getting-started/installation/#uv-recommended","title":"uv (Recommended)","text":"<p>We use <code>uv</code> for Python package management: <pre><code>pip install uv\n</code></pre></p>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>Clone and build from source:</p> <pre><code># Clone the repository\ngit clone https://github.com/your-org/rustystats.git\ncd rustystats\n\n# Install in development mode\nuv run maturin develop\n\n# Or with pip\npip install maturin\nmaturin develop\n</code></pre> <p>Development Mode</p> <p><code>maturin develop</code> compiles the Rust code and installs it in your current Python environment. Changes to Rust code require re-running this command.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\n# Quick test\ny = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\nX = np.column_stack([np.ones(5), np.array([1, 2, 3, 4, 5])])\n\nresult = rs.fit_glm(y, X, family=\"gaussian\")\nprint(f\"Coefficients: {result.params}\")\nprint(f\"RustyStats is working!\")\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":""},{"location":"getting-started/installation/#core-required","title":"Core (Required)","text":"<ul> <li><code>numpy&gt;=1.20</code> - Array operations</li> </ul>"},{"location":"getting-started/installation/#optional","title":"Optional","text":"<ul> <li><code>polars&gt;=1.0</code> - DataFrame support for formula API</li> <li><code>pyarrow</code> - Parquet file support</li> </ul>"},{"location":"getting-started/installation/#development","title":"Development","text":"<ul> <li><code>pytest&gt;=7.0</code> - Testing</li> <li><code>maturin&gt;=1.4</code> - Build tool</li> <li><code>statsmodels&gt;=0.14</code> - Comparison testing</li> </ul>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#compilation-errors","title":"Compilation Errors","text":"<p>If Rust compilation fails:</p> <ol> <li>Update Rust: <code>rustup update</code></li> <li>Check Cargo.toml: Ensure all dependencies are available</li> <li>Clean build: <code>cargo clean</code> then rebuild</li> </ol>"},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If <code>import rustystats</code> fails:</p> <ol> <li>Ensure you ran <code>maturin develop</code> (not just <code>maturin build</code>)</li> <li>Check you're in the correct Python environment</li> <li>Verify with <code>pip list | grep rustystats</code></li> </ol>"},{"location":"getting-started/installation/#performance-issues","title":"Performance Issues","text":"<p>For optimal performance:</p> <ol> <li>Build in release mode: <code>maturin develop --release</code></li> <li>Ensure Rayon parallelism isn't disabled by environment variables</li> </ol>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide walks through the essential RustyStats functionality in 10 minutes.</p>"},{"location":"getting-started/quickstart/#your-first-glm","title":"Your First GLM","text":""},{"location":"getting-started/quickstart/#using-numpy-arrays","title":"Using NumPy Arrays","text":"<pre><code>import rustystats as rs\nimport numpy as np\n\n# Generate sample data\nnp.random.seed(42)\nn = 1000\nx1 = np.random.normal(0, 1, n)\nx2 = np.random.normal(0, 1, n)\n\n# True model: y ~ Poisson(exp(0.5 + 0.3*x1 - 0.2*x2))\neta = 0.5 + 0.3 * x1 - 0.2 * x2\ny = np.random.poisson(np.exp(eta))\n\n# Build design matrix (include intercept!)\nX = np.column_stack([np.ones(n), x1, x2])\n\n# Fit the model\nresult = rs.fit_glm(y, X, family=\"poisson\")\n\n# View results\nprint(f\"Coefficients: {result.params}\")      # [0.50, 0.30, -0.20] approximately\nprint(f\"Standard Errors: {result.bse()}\")\nprint(f\"P-values: {result.pvalues()}\")\nprint(f\"Deviance: {result.deviance:.2f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#using-the-formula-api","title":"Using the Formula API","text":"<p>The formula API is more intuitive for real-world data:</p> <pre><code>import rustystats as rs\nimport polars as pl\n\n# Load data\ndata = pl.DataFrame({\n    \"claims\": [0, 1, 0, 2, 0, 1, 3, 0, 1, 0],\n    \"age\": [25, 35, 45, 55, 28, 38, 48, 58, 32, 42],\n    \"region\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"],\n    \"exposure\": [1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.2, 0.9, 1.0, 0.7]\n})\n\n# Fit model with formula\nresult = rs.glm(\n    formula=\"claims ~ age + C(region)\",\n    data=data,\n    family=\"poisson\",\n    offset=\"exposure\"  # log(exposure) applied automatically\n).fit()\n\n# Rich output\nprint(result.summary())\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-the-output","title":"Understanding the Output","text":"<pre><code># Coefficients and inference\nresult.params              # Coefficient estimates\nresult.bse()               # Standard errors\nresult.tvalues()           # z-statistics (coef / SE)\nresult.pvalues()           # Two-sided p-values\nresult.conf_int(0.05)      # 95% confidence intervals\n\n# Model fit\nresult.deviance            # Model deviance\nresult.aic()               # Akaike Information Criterion\nresult.bic()               # Bayesian Information Criterion\nresult.llf()               # Log-likelihood\n\n# Predictions\nresult.fittedvalues        # Fitted values (\u03bc)\nresult.linear_predictor    # Linear predictor (\u03b7 = X\u03b2)\n</code></pre>"},{"location":"getting-started/quickstart/#choosing-a-family","title":"Choosing a Family","text":"Data Type Family Example Continuous (any value) <code>\"gaussian\"</code> Claim amounts, temperatures Counts (0, 1, 2, ...) <code>\"poisson\"</code> Claim frequency, event counts Binary (0 or 1) <code>\"binomial\"</code> Claim occurrence, churn Positive continuous <code>\"gamma\"</code> Claim severity, durations Mixed zeros + positive <code>\"tweedie\"</code> Pure premium (frequency \u00d7 severity) <pre><code># Examples\nresult = rs.fit_glm(y, X, family=\"gaussian\")     # Linear regression\nresult = rs.fit_glm(y, X, family=\"poisson\")      # Count data\nresult = rs.fit_glm(y, X, family=\"binomial\")     # Binary outcomes\nresult = rs.fit_glm(y, X, family=\"gamma\")        # Positive continuous\nresult = rs.fit_glm(y, X, family=\"tweedie\", var_power=1.5)  # Pure premium\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-categorical-variables","title":"Working with Categorical Variables","text":"<p>Use <code>C()</code> to mark categorical variables in formulas:</p> <pre><code>result = rs.glm(\n    formula=\"claims ~ age + C(region) + C(vehicle_type)\",\n    data=data,\n    family=\"poisson\"\n).fit()\n\n# View relativities (exp(coef) for multiplicative interpretation)\nprint(result.relativities())\n</code></pre>"},{"location":"getting-started/quickstart/#adding-regularization","title":"Adding Regularization","text":"<pre><code># Lasso (L1) - variable selection\nresult = rs.fit_glm(y, X, family=\"poisson\", alpha=0.1, l1_ratio=1.0)\nprint(f\"Non-zero coefficients: {result.n_nonzero()}\")\n\n# Ridge (L2) - shrinkage without selection\nresult = rs.fit_glm(y, X, family=\"gaussian\", alpha=0.1, l1_ratio=0.0)\n\n# Elastic Net - mix of both\nresult = rs.fit_glm(y, X, family=\"gaussian\", alpha=0.1, l1_ratio=0.5)\n\n# Cross-validation for optimal alpha\ncv_result = rs.cv_glm(y, X, family=\"poisson\", l1_ratio=1.0, cv=5)\nprint(f\"Best alpha: {cv_result.alpha_best}\")\n</code></pre>"},{"location":"getting-started/quickstart/#non-linear-effects-with-splines","title":"Non-linear Effects with Splines","text":"<pre><code># Add smooth age effect\nresult = rs.glm(\n    formula=\"claims ~ bs(age, df=5) + C(region)\",\n    data=data,\n    family=\"poisson\"\n).fit()\n\n# Natural splines (linear at boundaries - better extrapolation)\nresult = rs.glm(\n    formula=\"claims ~ ns(age, df=4) + C(region)\",\n    data=data,\n    family=\"poisson\"\n).fit()\n</code></pre>"},{"location":"getting-started/quickstart/#handling-overdispersion","title":"Handling Overdispersion","text":"<p>When variance exceeds what the model predicts:</p> <pre><code># Check for overdispersion\nresult = rs.fit_glm(y, X, family=\"poisson\")\ndispersion = result.pearson_chi2() / result.df_resid\nprint(f\"Dispersion ratio: {dispersion:.2f}\")  # &gt;&gt; 1 indicates overdispersion\n\n# Use QuasiPoisson for inflated standard errors\nresult_quasi = rs.fit_glm(y, X, family=\"quasipoisson\")\n\n# Or Negative Binomial for proper likelihood\nresult_nb = rs.fit_glm(y, X, family=\"negbinomial\", theta=1.0)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>GLM Theory - Understand the mathematics</li> <li>Architecture - How the code is organized</li> <li>Python API - Complete API reference</li> </ul>"},{"location":"guides/glm-workflow/","title":"GLM Modeling Workflow","text":"<p>This guide walks through a complete GLM modeling workflow using RustyStats, showing how to use diagnostics at each step to make informed decisions about model development.</p> <p>Target audience: Data scientists and actuaries building production GLM models.</p>"},{"location":"guides/glm-workflow/#overview-the-iterative-modeling-process","title":"Overview: The Iterative Modeling Process","text":"<p>Building a good GLM is iterative. At each stage, diagnostics guide your next decision:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        GLM MODELING WORKFLOW                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                      \u2502\n\u2502  1. DATA EXPLORATION                                                 \u2502\n\u2502     \u2514\u2500\u25ba explore_data() \u2192 Understand distributions, spot issues       \u2502\n\u2502                                                                      \u2502\n\u2502  2. FAMILY &amp; LINK SELECTION                                          \u2502\n\u2502     \u2514\u2500\u25ba Response distribution \u2192 Choose appropriate family/link       \u2502\n\u2502                                                                      \u2502\n\u2502  3. INITIAL MODEL FIT                                                \u2502\n\u2502     \u2514\u2500\u25ba fit_glm() \u2192 Baseline model with key predictors              \u2502\n\u2502                                                                      \u2502\n\u2502  4. DIAGNOSTICS CHECK                                                \u2502\n\u2502     \u2514\u2500\u25ba result.diagnostics() \u2192 Assess fit quality                   \u2502\n\u2502         \u251c\u2500\u25ba Calibration OK? \u2192 Continue                              \u2502\n\u2502         \u251c\u2500\u25ba Overdispersion? \u2192 Consider QuasiPoisson/NegBin          \u2502\n\u2502         \u2514\u2500\u25ba Poor discrimination? \u2192 Add predictors/interactions      \u2502\n\u2502                                                                      \u2502\n\u2502  5. FACTOR ANALYSIS                                                  \u2502\n\u2502     \u2514\u2500\u25ba Per-factor A/E \u2192 Identify non-linear effects, missing vars  \u2502\n\u2502                                                                      \u2502\n\u2502  6. MODEL REFINEMENT                                                 \u2502\n\u2502     \u2514\u2500\u25ba Add splines, interactions, transformations                  \u2502\n\u2502                                                                      \u2502\n\u2502  7. REGULARIZATION (optional)                                        \u2502\n\u2502     \u2514\u2500\u25ba cv_lasso() \u2192 Variable selection for many predictors         \u2502\n\u2502                                                                      \u2502\n\u2502  8. FINAL VALIDATION                                                 \u2502\n\u2502     \u2514\u2500\u25ba Out-of-sample testing, stability checks                     \u2502\n\u2502                                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/glm-workflow/#step-1-data-exploration","title":"Step 1: Data Exploration","text":"<p>Before fitting any model, understand your data.</p>"},{"location":"guides/glm-workflow/#11-load-and-inspect-data","title":"1.1 Load and Inspect Data","text":"<pre><code>import rustystats as rs\nimport polars as pl\nimport numpy as np\n\n# Load your data\ndata = pl.read_csv(\"insurance_claims.csv\")\n\n# Basic summary\nprint(f\"Rows: {len(data)}\")\nprint(f\"Columns: {data.columns}\")\nprint(data.describe())\n</code></pre>"},{"location":"guides/glm-workflow/#12-use-explore_data-for-pre-fit-analysis","title":"1.2 Use <code>explore_data()</code> for Pre-Fit Analysis","text":"<pre><code>exploration = rs.explore_data(\n    data=data,\n    response=\"claim_amount\",\n    exposure=\"exposure\",\n    categorical_factors=[\"region\", \"vehicle_type\", \"driver_age_band\"],\n    continuous_factors=[\"vehicle_age\", \"driver_experience\", \"credit_score\"],\n)\n\n# View as JSON (useful for LLM analysis or dashboards)\nprint(exploration.to_json(indent=2))\n</code></pre>"},{"location":"guides/glm-workflow/#13-key-diagnostics-to-check","title":"1.3 Key Diagnostics to Check","text":"Diagnostic What to Look For Action if Problem Response distribution Zeros, skewness, outliers Choose appropriate family Missing values High % missing in key factors Impute or exclude Rare categories Levels with &lt; 1% of data Combine into \"Other\" Correlation with response Factors with no signal Consider excluding <p>Decision Point: Based on response distribution:</p> <pre><code>response_stats = exploration.response_stats\n\n# Check for zeros\nif response_stats[\"pct_zero\"] &gt; 10:\n    print(\"\u26a0\ufe0f Many zeros \u2192 Consider Tweedie or zero-inflated model\")\n\n# Check for overdispersion (counts)\nif response_stats[\"variance\"] &gt; response_stats[\"mean\"] * 1.5:\n    print(\"\u26a0\ufe0f Overdispersion \u2192 Consider QuasiPoisson or NegBin\")\n</code></pre>"},{"location":"guides/glm-workflow/#step-2-choose-family-and-link","title":"Step 2: Choose Family and Link","text":"<p>The response distribution determines your family choice.</p>"},{"location":"guides/glm-workflow/#21-decision-tree","title":"2.1 Decision Tree","text":"<pre><code>def suggest_family(y: np.ndarray) -&gt; str:\n    \"\"\"Suggest a GLM family based on response characteristics.\"\"\"\n\n    # Check data type and range\n    is_binary = set(np.unique(y)) &lt;= {0, 1}\n    is_count = np.all(y == np.floor(y)) and np.all(y &gt;= 0)\n    is_positive = np.all(y &gt; 0)\n    has_zeros = np.any(y == 0)\n\n    if is_binary:\n        return \"binomial\"\n\n    if is_count:\n        # Check dispersion\n        mean_y = np.mean(y)\n        var_y = np.var(y)\n        dispersion_ratio = var_y / mean_y\n\n        if 0.8 &lt;= dispersion_ratio &lt;= 1.2:\n            return \"poisson\"\n        elif dispersion_ratio &gt; 1.2:\n            return \"quasipoisson\"  # or \"negbinomial\"\n        else:\n            return \"poisson\"  # underdispersion is rare\n\n    # Continuous\n    if has_zeros and is_positive or np.any(y == 0):\n        return \"tweedie\"  # Can handle zeros\n\n    if is_positive:\n        return \"gamma\"\n\n    return \"gaussian\"  # Default for continuous data\n\n# Usage\ny = data[\"claim_amount\"].to_numpy()\nsuggested = suggest_family(y)\nprint(f\"Suggested family: {suggested}\")\n</code></pre>"},{"location":"guides/glm-workflow/#22-family-link-combinations","title":"2.2 Family-Link Combinations","text":"Family Default Link When to Use <code>gaussian</code> identity Continuous, can be negative <code>poisson</code> log Counts with mean \u2248 variance <code>binomial</code> logit Binary outcomes <code>gamma</code> log Positive continuous, CV constant <code>tweedie</code> log Positive with exact zeros <code>quasipoisson</code> log Overdispersed counts <code>negbinomial</code> log Overdispersed counts (proper likelihood)"},{"location":"guides/glm-workflow/#step-3-fit-initial-model","title":"Step 3: Fit Initial Model","text":"<p>Start with a baseline model using your most important predictors.</p>"},{"location":"guides/glm-workflow/#31-prepare-design-matrix","title":"3.1 Prepare Design Matrix","text":"<pre><code># Option A: Array API (explicit control)\nfrom rustystats import fit_glm\n\nX = np.column_stack([\n    np.ones(len(data)),  # Intercept\n    data[\"vehicle_age\"].to_numpy(),\n    data[\"driver_experience\"].to_numpy(),\n    # Add dummy variables for categoricals\n])\n\nresult = fit_glm(\n    y=data[\"claim_amount\"].to_numpy(),\n    X=X,\n    family=\"gamma\",\n    offset=np.log(data[\"exposure\"].to_numpy()),\n)\n</code></pre> <pre><code># Option B: Formula API (recommended for most cases)\nresult = rs.glm(\n    \"claim_amount ~ vehicle_age + driver_experience + C(region)\",\n    data=data,\n    family=\"gamma\",\n    exposure=\"exposure\",\n).fit()\n</code></pre>"},{"location":"guides/glm-workflow/#32-quick-convergence-check","title":"3.2 Quick Convergence Check","text":"<pre><code>print(f\"Converged: {result.converged}\")\nprint(f\"Iterations: {result.iterations}\")\nprint(f\"Deviance: {result.deviance:.2f}\")\n\nif not result.converged:\n    print(\"\u26a0\ufe0f Model did not converge!\")\n    print(\"   \u2192 Try: increase max_iter, check for separation, add regularization\")\n</code></pre>"},{"location":"guides/glm-workflow/#step-4-comprehensive-diagnostics","title":"Step 4: Comprehensive Diagnostics","text":"<p>This is the critical step. Use <code>result.diagnostics()</code> to assess model quality.</p>"},{"location":"guides/glm-workflow/#41-compute-full-diagnostics","title":"4.1 Compute Full Diagnostics","text":"<pre><code>diagnostics = result.diagnostics(\n    data=data,\n    categorical_factors=[\"region\", \"vehicle_type\", \"driver_age_band\"],\n    continuous_factors=[\"vehicle_age\", \"driver_experience\", \"credit_score\"],\n    n_bins=10,\n)\n</code></pre>"},{"location":"guides/glm-workflow/#42-overall-fit-statistics","title":"4.2 Overall Fit Statistics","text":"<pre><code>fit_stats = diagnostics.fit_statistics\n\nprint(\"=== FIT STATISTICS ===\")\nprint(f\"Deviance:       {fit_stats['deviance']:.2f}\")\nprint(f\"Null Deviance:  {fit_stats['null_deviance']:.2f}\")\nprint(f\"Pseudo R\u00b2:      {fit_stats['pseudo_r2']:.4f}\")\nprint(f\"AIC:            {fit_stats['aic']:.2f}\")\nprint(f\"BIC:            {fit_stats['bic']:.2f}\")\n</code></pre> <p>Decision Points:</p> Metric Good Range Action if Bad Pseudo R\u00b2 &gt; 0.1 for GLMs Add predictors, check family AIC/BIC Lower is better Use for model comparison Deviance/df \u2248 1 for Poisson/Binomial Check overdispersion"},{"location":"guides/glm-workflow/#43-calibration-diagnostics","title":"4.3 Calibration Diagnostics","text":"<p>Calibration measures whether predictions match actuals on average.</p> <pre><code>calibration = diagnostics.calibration\n\nprint(\"=== CALIBRATION ===\")\nprint(f\"Overall A/E:     {calibration['actual_expected_ratio']:.4f}\")\nprint(f\"Calibration Err: {calibration['calibration_error']:.4f}\")\nprint(f\"H-L p-value:     {calibration['hosmer_lemeshow_pvalue']:.4f}\")\n\n# Check calibration by decile\nprint(\"\\nCalibration by Decile:\")\nprint(\"Decile | Predicted | Actual | A/E Ratio | Count\")\nprint(\"-\" * 55)\nfor bin_data in calibration['by_decile']:\n    print(f\"  {bin_data['bin_index']:2d}   | {bin_data['predicted_mean']:9.2f} | \"\n          f\"{bin_data['actual_mean']:6.2f} | {bin_data['actual_expected_ratio']:9.3f} | \"\n          f\"{bin_data['count']:5d}\")\n</code></pre> <p>Decision Points:</p> Diagnostic Threshold Interpretation Action Overall A/E 0.95 - 1.05 Good calibration Continue Overall A/E &lt; 0.95 or &gt; 1.05 Systematic bias Check model specification H-L p-value &gt; 0.05 No evidence of poor fit Continue H-L p-value &lt; 0.05 Significant miscalibration Add predictors or interactions Decile A/E All near 1.0 Uniform calibration Good Decile A/E Trend (e.g., low\u2192high) Non-linearity Add splines or transforms <pre><code># Automated calibration check\ndef check_calibration(diagnostics) -&gt; dict:\n    \"\"\"Check calibration and return recommendations.\"\"\"\n    cal = diagnostics.calibration\n    issues = []\n\n    ae = cal['actual_expected_ratio']\n    if ae &lt; 0.95 or ae &gt; 1.05:\n        issues.append(f\"Overall A/E = {ae:.3f}, outside [0.95, 1.05]\")\n\n    hl_p = cal['hosmer_lemeshow_pvalue']\n    if hl_p &lt; 0.05:\n        issues.append(f\"Hosmer-Lemeshow p = {hl_p:.4f} &lt; 0.05\")\n\n    # Check for trend in deciles\n    decile_ae = [b['actual_expected_ratio'] for b in cal['by_decile']]\n    if len(decile_ae) &gt;= 5:\n        correlation = np.corrcoef(range(len(decile_ae)), decile_ae)[0, 1]\n        if abs(correlation) &gt; 0.7:\n            issues.append(f\"Strong trend in decile A/E (r = {correlation:.2f})\")\n\n    return {\n        \"passed\": len(issues) == 0,\n        \"issues\": issues,\n        \"recommendation\": \"Add interactions or non-linear terms\" if issues else \"Calibration OK\"\n    }\n\ncal_check = check_calibration(diagnostics)\nprint(f\"\\nCalibration Check: {'\u2713 PASS' if cal_check['passed'] else '\u2717 FAIL'}\")\nfor issue in cal_check['issues']:\n    print(f\"  \u2192 {issue}\")\n</code></pre>"},{"location":"guides/glm-workflow/#44-discrimination-diagnostics","title":"4.4 Discrimination Diagnostics","text":"<p>Discrimination measures how well the model separates high from low risk.</p> <pre><code>discrimination = diagnostics.discrimination\n\nprint(\"=== DISCRIMINATION ===\")\nprint(f\"Gini Coefficient: {discrimination['gini_coefficient']:.4f}\")\nprint(f\"AUC:              {discrimination['auc']:.4f}\")\nprint(f\"KS Statistic:     {discrimination['ks_statistic']:.4f}\")\nprint(f\"Lift @ 10%:       {discrimination['lift_at_10pct']:.2f}x\")\nprint(f\"Lift @ 20%:       {discrimination['lift_at_20pct']:.2f}x\")\n</code></pre> <p>Interpretation Guide:</p> Gini Model Quality &lt; 0.20 Poor discrimination 0.20 - 0.40 Fair 0.40 - 0.60 Good &gt; 0.60 Excellent <p>Decision Points:</p> <pre><code>def check_discrimination(diagnostics) -&gt; dict:\n    \"\"\"Check discrimination and return recommendations.\"\"\"\n    disc = diagnostics.discrimination\n    if disc is None:\n        return {\"passed\": True, \"issues\": [], \"recommendation\": \"N/A for this model type\"}\n\n    issues = []\n    gini = disc['gini_coefficient']\n\n    if gini &lt; 0.20:\n        issues.append(f\"Poor Gini = {gini:.3f}, need better predictors\")\n    elif gini &lt; 0.30:\n        issues.append(f\"Fair Gini = {gini:.3f}, consider adding features\")\n\n    lift_10 = disc['lift_at_10pct']\n    if lift_10 &lt; 1.5:\n        issues.append(f\"Low lift@10% = {lift_10:.2f}x, model not separating well\")\n\n    return {\n        \"passed\": len(issues) == 0,\n        \"gini\": gini,\n        \"issues\": issues,\n        \"recommendation\": \"Add more predictive features or interactions\" if issues else \"Discrimination OK\"\n    }\n</code></pre>"},{"location":"guides/glm-workflow/#45-residual-analysis","title":"4.5 Residual Analysis","text":"<p>Residuals reveal systematic patterns the model is missing.</p> <pre><code>resid_summary = diagnostics.residual_summary\n\nprint(\"=== RESIDUAL SUMMARY ===\")\nfor resid_type in ['pearson', 'deviance']:\n    r = resid_summary[resid_type]\n    print(f\"\\n{resid_type.upper()} Residuals:\")\n    print(f\"  Mean:     {r.mean:8.4f}  (should be \u2248 0)\")\n    print(f\"  Std:      {r.std:8.4f}  (should be \u2248 1 for Pearson)\")\n    print(f\"  Skewness: {r.skewness:8.4f}  (should be \u2248 0)\")\n    print(f\"  Kurtosis: {r.kurtosis:8.4f}  (should be \u2248 3)\")\n    print(f\"  Range:    [{r.min:.2f}, {r.max:.2f}]\")\n</code></pre> <p>Decision Points:</p> Symptom Possible Cause Action Mean \u2260 0 Bias in predictions Check calibration, add intercept Std &gt;&gt; 1 Overdispersion Use quasi-family or negative binomial High skewness Non-normality, outliers Check for outliers, consider robust fitting Large outliers Extreme observations Investigate, possibly winsorize"},{"location":"guides/glm-workflow/#46-overdispersion-check","title":"4.6 Overdispersion Check","text":"<p>Critical for Poisson and Binomial models.</p> <pre><code>def check_overdispersion(result, diagnostics) -&gt; dict:\n    \"\"\"Check for overdispersion in count/binary models.\"\"\"\n\n    family = result.family.lower()\n    if family not in ['poisson', 'binomial']:\n        return {\"applicable\": False}\n\n    # Dispersion ratio = Pearson \u03c7\u00b2 / df_resid\n    pearson_chi2 = result.pearson_chi2()\n    df_resid = result.df_resid\n    dispersion = pearson_chi2 / df_resid\n\n    print(f\"Dispersion ratio: {dispersion:.3f}\")\n\n    if dispersion &gt; 1.5:\n        return {\n            \"applicable\": True,\n            \"overdispersed\": True,\n            \"dispersion\": dispersion,\n            \"recommendation\": f\"Significant overdispersion ({dispersion:.2f}). \"\n                            f\"Switch to {'quasipoisson' if family == 'poisson' else 'quasibinomial'} \"\n                            f\"or negative binomial.\"\n        }\n    elif dispersion &lt; 0.7:\n        return {\n            \"applicable\": True,\n            \"overdispersed\": False,\n            \"underdispersed\": True,\n            \"dispersion\": dispersion,\n            \"recommendation\": f\"Possible underdispersion ({dispersion:.2f}). Check model specification.\"\n        }\n    else:\n        return {\n            \"applicable\": True,\n            \"overdispersed\": False,\n            \"dispersion\": dispersion,\n            \"recommendation\": f\"Dispersion OK ({dispersion:.2f})\"\n        }\n\ndispersion_check = check_overdispersion(result, diagnostics)\nprint(dispersion_check['recommendation'])\n</code></pre>"},{"location":"guides/glm-workflow/#step-5-per-factor-analysis","title":"Step 5: Per-Factor Analysis","text":"<p>Examine each predictor's relationship with residuals.</p>"},{"location":"guides/glm-workflow/#51-review-factor-diagnostics","title":"5.1 Review Factor Diagnostics","text":"<pre><code>print(\"=== FACTOR DIAGNOSTICS ===\")\nfor factor in diagnostics.factors:\n    print(f\"\\n{factor.name} ({factor.factor_type})\")\n    print(f\"  In model: {factor.in_model}\")\n\n    # Residual pattern\n    rp = factor.residual_pattern\n    print(f\"  Residual correlation: {rp.correlation_with_residuals:.4f}\")\n    print(f\"  Variance explained:   {rp.residual_variance_explained:.4f}\")\n\n    # A/E summary\n    ae_ratios = [b.actual_expected_ratio for b in factor.actual_vs_expected]\n    ae_range = max(ae_ratios) - min(ae_ratios)\n    print(f\"  A/E range: {min(ae_ratios):.3f} - {max(ae_ratios):.3f} (spread: {ae_range:.3f})\")\n</code></pre>"},{"location":"guides/glm-workflow/#52-identify-problem-factors","title":"5.2 Identify Problem Factors","text":"<pre><code>def identify_problem_factors(diagnostics, \n                              ae_threshold: float = 0.15,\n                              resid_corr_threshold: float = 0.05) -&gt; list:\n    \"\"\"Identify factors that need attention.\"\"\"\n    problems = []\n\n    for factor in diagnostics.factors:\n        issues = []\n\n        # Check A/E spread\n        ae_ratios = [b.actual_expected_ratio for b in factor.actual_vs_expected]\n        ae_range = max(ae_ratios) - min(ae_ratios)\n\n        if ae_range &gt; ae_threshold:\n            issues.append(f\"Large A/E spread ({ae_range:.3f})\")\n\n        # Check residual correlation\n        resid_corr = abs(factor.residual_pattern.correlation_with_residuals)\n        if resid_corr &gt; resid_corr_threshold:\n            issues.append(f\"Residual correlation ({resid_corr:.3f})\")\n\n        # Check if factor is NOT in model but shows signal\n        if not factor.in_model and (ae_range &gt; ae_threshold / 2 or resid_corr &gt; resid_corr_threshold / 2):\n            issues.append(\"NOT in model but shows predictive signal\")\n\n        if issues:\n            problems.append({\n                \"factor\": factor.name,\n                \"type\": factor.factor_type,\n                \"in_model\": factor.in_model,\n                \"issues\": issues,\n                \"recommendation\": get_factor_recommendation(factor, issues)\n            })\n\n    return problems\n\n\ndef get_factor_recommendation(factor, issues) -&gt; str:\n    \"\"\"Generate recommendation for a problem factor.\"\"\"\n    if not factor.in_model:\n        return \"Consider adding to model\"\n\n    if factor.factor_type == \"continuous\":\n        ae_ratios = [b.actual_expected_ratio for b in factor.actual_vs_expected]\n        # Check for non-monotonic pattern\n        diffs = np.diff(ae_ratios)\n        sign_changes = np.sum(np.diff(np.sign(diffs)) != 0)\n        if sign_changes &gt; 2:\n            return \"Add spline: ns(factor, df=4) or bs(factor, df=5)\"\n        else:\n            return \"Consider polynomial or log transformation\"\n    else:\n        return \"Check category groupings, consider interactions\"\n\n\n# Run analysis\nproblems = identify_problem_factors(diagnostics)\n\nprint(\"\\n=== FACTORS NEEDING ATTENTION ===\")\nfor p in problems:\n    print(f\"\\n{p['factor']} ({p['type']}, in_model={p['in_model']})\")\n    for issue in p['issues']:\n        print(f\"  \u26a0\ufe0f {issue}\")\n    print(f\"  \u2192 {p['recommendation']}\")\n</code></pre>"},{"location":"guides/glm-workflow/#53-visualize-ae-by-factor","title":"5.3 Visualize A/E by Factor","text":"<pre><code>import matplotlib.pyplot as plt\n\ndef plot_ae_by_factor(factor_diagnostics, figsize=(10, 6)):\n    \"\"\"Plot A/E ratios for a factor.\"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n\n    bins = factor_diagnostics.actual_vs_expected\n    labels = [b.bin_label for b in bins]\n    ae_ratios = [b.actual_expected_ratio for b in bins]\n    ci_lower = [b.ae_confidence_interval_lower for b in bins]\n    ci_upper = [b.ae_confidence_interval_upper for b in bins]\n\n    x = range(len(bins))\n\n    # Plot bars\n    bars = ax.bar(x, ae_ratios, color='steelblue', alpha=0.7)\n\n    # Add error bars for confidence intervals\n    ax.errorbar(x, ae_ratios, \n                yerr=[np.array(ae_ratios) - np.array(ci_lower), \n                      np.array(ci_upper) - np.array(ae_ratios)],\n                fmt='none', color='black', capsize=3)\n\n    # Reference line at 1.0\n    ax.axhline(y=1.0, color='red', linestyle='--', linewidth=1.5, label='Perfect calibration')\n    ax.axhline(y=1.05, color='orange', linestyle=':', alpha=0.7)\n    ax.axhline(y=0.95, color='orange', linestyle=':', alpha=0.7)\n\n    ax.set_xlabel(factor_diagnostics.name)\n    ax.set_ylabel('Actual / Expected Ratio')\n    ax.set_title(f'Calibration by {factor_diagnostics.name}')\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels, rotation=45, ha='right')\n    ax.legend()\n\n    plt.tight_layout()\n    return fig, ax\n\n# Plot for each problem factor\nfor p in problems[:3]:  # Top 3 problems\n    factor = next(f for f in diagnostics.factors if f.name == p['factor'])\n    plot_ae_by_factor(factor)\n    plt.show()\n</code></pre>"},{"location":"guides/glm-workflow/#step-6-model-refinement","title":"Step 6: Model Refinement","text":"<p>Based on diagnostics, refine your model.</p>"},{"location":"guides/glm-workflow/#61-adding-non-linear-effects-splines","title":"6.1 Adding Non-Linear Effects (Splines)","text":"<p>If continuous factors show non-linear A/E patterns:</p> <pre><code># Before: linear effect\nresult_v1 = rs.glm(\n    \"claim_amount ~ vehicle_age + driver_experience + C(region)\",\n    data=data,\n    family=\"gamma\",\n    exposure=\"exposure\",\n).fit()\n\n# After: non-linear effects with natural splines\nresult_v2 = rs.glm(\n    \"claim_amount ~ ns(vehicle_age, df=4) + ns(driver_experience, df=3) + C(region)\",\n    data=data,\n    family=\"gamma\", \n    exposure=\"exposure\",\n).fit()\n\n# Compare\nprint(f\"Model v1 AIC: {result_v1.aic():.2f}\")\nprint(f\"Model v2 AIC: {result_v2.aic():.2f}\")\nprint(f\"Improvement: {result_v1.aic() - result_v2.aic():.2f} (lower is better)\")\n</code></pre>"},{"location":"guides/glm-workflow/#62-adding-interactions","title":"6.2 Adding Interactions","text":"<p>If <code>detect_interactions()</code> finds significant pairs:</p> <pre><code># Check diagnostics for interaction candidates\ninteractions = diagnostics.interaction_candidates\n\nprint(\"=== INTERACTION CANDIDATES ===\")\nfor ic in interactions:\n    sig = \"***\" if ic.pvalue &lt; 0.001 else \"**\" if ic.pvalue &lt; 0.01 else \"*\" if ic.pvalue &lt; 0.05 else \"\"\n    print(f\"{ic.factor1} \u00d7 {ic.factor2}: strength={ic.interaction_strength:.4f}, p={ic.pvalue:.4f}{sig}\")\n\n# Add top interaction to model\nif interactions and interactions[0].pvalue &lt; 0.05:\n    f1, f2 = interactions[0].factor1, interactions[0].factor2\n    formula = f\"claim_amount ~ vehicle_age + driver_experience + C(region) + {f1}:{f2}\"\n    result_v3 = rs.glm(formula, data=data, family=\"gamma\", exposure=\"exposure\").fit()\n    print(f\"Model with interaction AIC: {result_v3.aic():.2f}\")\n</code></pre>"},{"location":"guides/glm-workflow/#63-handling-overdispersion","title":"6.3 Handling Overdispersion","text":"<p>If dispersion check showed overdispersion:</p> <pre><code># Original Poisson\nresult_pois = rs.glm(\n    \"claim_count ~ vehicle_age + C(region)\",\n    data=data,\n    family=\"poisson\",\n    exposure=\"exposure\",\n).fit()\n\n# QuasiPoisson (adjusts standard errors)\nresult_quasi = rs.glm(\n    \"claim_count ~ vehicle_age + C(region)\",\n    data=data,\n    family=\"quasipoisson\",\n    exposure=\"exposure\",\n).fit()\n\n# Negative Binomial (estimates dispersion parameter)\nresult_nb = rs.glm(\n    \"claim_count ~ vehicle_age + C(region)\",\n    data=data,\n    family=\"negbinomial\",\n    exposure=\"exposure\",\n).fit()\n\n# Compare standard errors\nprint(\"\\nCoefficient Standard Errors:\")\nprint(f\"Poisson:      {result_pois.bse()[1]:.4f}\")\nprint(f\"QuasiPoisson: {result_quasi.bse()[1]:.4f}\")\nprint(f\"NegBinomial:  {result_nb.bse()[1]:.4f}\")\n</code></pre>"},{"location":"guides/glm-workflow/#64-re-run-diagnostics-after-refinement","title":"6.4 Re-Run Diagnostics After Refinement","text":"<p>Always re-check diagnostics after changes:</p> <pre><code># Fit refined model\nresult_refined = rs.glm(\n    \"claim_amount ~ ns(vehicle_age, df=4) + ns(driver_experience, df=3) + \"\n    \"C(region) + vehicle_age:C(region)\",\n    data=data,\n    family=\"gamma\",\n    exposure=\"exposure\",\n).fit()\n\n# Re-compute diagnostics\ndiagnostics_refined = result_refined.diagnostics(\n    data=data,\n    categorical_factors=[\"region\", \"vehicle_type\"],\n    continuous_factors=[\"vehicle_age\", \"driver_experience\"],\n)\n\n# Compare calibration\ncal_before = diagnostics.calibration['actual_expected_ratio']\ncal_after = diagnostics_refined.calibration['actual_expected_ratio']\nprint(f\"Overall A/E: {cal_before:.4f} \u2192 {cal_after:.4f}\")\n\n# Compare discrimination\ngini_before = diagnostics.discrimination['gini_coefficient']\ngini_after = diagnostics_refined.discrimination['gini_coefficient']\nprint(f\"Gini:        {gini_before:.4f} \u2192 {gini_after:.4f}\")\n</code></pre>"},{"location":"guides/glm-workflow/#step-7-regularization-for-variable-selection","title":"Step 7: Regularization for Variable Selection","text":"<p>When you have many predictors, use regularization to select the important ones.</p>"},{"location":"guides/glm-workflow/#71-lasso-path-visualization","title":"7.1 Lasso Path Visualization","text":"<pre><code># Compute Lasso path\npath = rs.lasso_path(\n    y=data[\"claim_amount\"].to_numpy(),\n    X=X,  # Design matrix with all candidate features\n    family=\"gamma\",\n    n_alphas=50,\n)\n\n# Plot coefficient paths\npath.plot()\nplt.title(\"Lasso Coefficient Paths\")\nplt.show()\n\n# See which features are selected at each sparsity level\nprint(\"\\nFeatures by regularization strength:\")\nfor i in [0, len(path.alphas)//4, len(path.alphas)//2, -1]:\n    n_nonzero = path.n_nonzero[i]\n    print(f\"  \u03b1={path.alphas[i]:.4f}: {n_nonzero} features\")\n</code></pre>"},{"location":"guides/glm-workflow/#72-cross-validated-selection","title":"7.2 Cross-Validated Selection","text":"<pre><code># Find optimal regularization via CV\ncv_result = rs.cv_lasso(\n    y=data[\"claim_amount\"].to_numpy(),\n    X=X,\n    family=\"gamma\",\n    cv=5,\n    n_alphas=30,\n)\n\nprint(f\"Best \u03b1:        {cv_result.alpha_best:.6f}\")\nprint(f\"1-SE \u03b1:        {cv_result.alpha_1se:.6f}\")\nprint(f\"Features (best): {cv_result.n_nonzero_best}\")\n\n# Plot CV curve\ncv_result.plot()\nplt.show()\n\n# Get the selected model\nbest_model = cv_result.best_result\nselected_features = [i for i, c in enumerate(best_model.params) if abs(c) &gt; 1e-8]\nprint(f\"Selected feature indices: {selected_features}\")\n</code></pre>"},{"location":"guides/glm-workflow/#73-using-regularized-model","title":"7.3 Using Regularized Model","text":"<pre><code># Fit with optimal regularization\nresult_lasso = rs.fit_glm(\n    y=data[\"claim_amount\"].to_numpy(),\n    X=X,\n    family=\"gamma\",\n    alpha=cv_result.alpha_1se,  # Use 1-SE rule for more parsimony\n    l1_ratio=1.0,\n)\n\nprint(f\"Non-zero coefficients: {result_lasso.n_nonzero()}\")\n\n# Compare to unregularized\nprint(f\"\\nUnregularized deviance: {result.deviance:.2f}\")\nprint(f\"Lasso deviance:         {result_lasso.deviance:.2f}\")\n</code></pre>"},{"location":"guides/glm-workflow/#step-8-final-validation","title":"Step 8: Final Validation","text":"<p>Before deploying, validate on held-out data.</p>"},{"location":"guides/glm-workflow/#81-traintest-split","title":"8.1 Train/Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\n# Split data\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Fit on training data\nresult_train = rs.glm(\n    \"claim_amount ~ ns(vehicle_age, df=4) + C(region)\",\n    data=train_data,\n    family=\"gamma\",\n    exposure=\"exposure\",\n).fit()\n\n# Predict on test data\ntest_predictions = result_train.predict(test_data)\n\n# Compute test diagnostics\ntest_y = test_data[\"claim_amount\"].to_numpy()\ntest_exposure = test_data[\"exposure\"].to_numpy()\n\n# Manual A/E calculation\ntest_ae = np.sum(test_y) / np.sum(test_predictions)\nprint(f\"Test set A/E: {test_ae:.4f}\")\n</code></pre>"},{"location":"guides/glm-workflow/#82-stability-check","title":"8.2 Stability Check","text":"<pre><code>def stability_check(data, formula, family, exposure_col, n_bootstrap=10, sample_frac=0.8):\n    \"\"\"Check coefficient stability via bootstrap.\"\"\"\n\n    coef_samples = []\n\n    for i in range(n_bootstrap):\n        # Bootstrap sample\n        sample = data.sample(fraction=sample_frac, shuffle=True, seed=i)\n\n        # Fit model\n        result = rs.glm(formula, data=sample, family=family, exposure=exposure_col).fit()\n        coef_samples.append(result.params)\n\n    coef_array = np.vstack(coef_samples)\n\n    # Compute coefficient variation\n    coef_mean = np.mean(coef_array, axis=0)\n    coef_std = np.std(coef_array, axis=0)\n    coef_cv = np.abs(coef_std / (coef_mean + 1e-10))\n\n    return {\n        \"mean\": coef_mean,\n        \"std\": coef_std,\n        \"cv\": coef_cv,\n        \"stable\": np.all(coef_cv &lt; 0.5),  # CV &lt; 50% considered stable\n    }\n\nstability = stability_check(\n    data, \n    \"claim_amount ~ vehicle_age + C(region)\",\n    \"gamma\",\n    \"exposure\",\n    n_bootstrap=10\n)\n\nprint(f\"Coefficients stable: {stability['stable']}\")\nprint(f\"Coefficient CVs: {stability['cv']}\")\n</code></pre>"},{"location":"guides/glm-workflow/#complete-workflow-summary","title":"Complete Workflow Summary","text":"<pre><code>def full_glm_workflow(data, response, exposure, categorical_vars, continuous_vars):\n    \"\"\"Complete GLM workflow with decision points.\"\"\"\n\n    results = {\"steps\": []}\n\n    # Step 1: Explore\n    exploration = rs.explore_data(\n        data=data,\n        response=response,\n        exposure=exposure,\n        categorical_factors=categorical_vars,\n        continuous_factors=continuous_vars,\n    )\n    results[\"exploration\"] = exploration\n    results[\"steps\"].append(\"1. Data exploration complete\")\n\n    # Step 2: Choose family\n    y = data[response].to_numpy()\n    family = suggest_family(y)\n    results[\"suggested_family\"] = family\n    results[\"steps\"].append(f\"2. Suggested family: {family}\")\n\n    # Step 3: Initial fit\n    formula = f\"{response} ~ \" + \" + \".join(\n        [f\"C({v})\" for v in categorical_vars] + continuous_vars\n    )\n    result = rs.glm(formula, data=data, family=family, exposure=exposure).fit()\n    results[\"initial_result\"] = result\n    results[\"steps\"].append(f\"3. Initial model fit (converged={result.converged})\")\n\n    # Step 4: Diagnostics\n    diagnostics = result.diagnostics(\n        data=data,\n        categorical_factors=categorical_vars,\n        continuous_factors=continuous_vars,\n    )\n    results[\"diagnostics\"] = diagnostics\n\n    # Calibration check\n    cal_check = check_calibration(diagnostics)\n    results[\"calibration_check\"] = cal_check\n    results[\"steps\"].append(f\"4. Calibration: {'PASS' if cal_check['passed'] else 'FAIL'}\")\n\n    # Discrimination check  \n    disc_check = check_discrimination(diagnostics)\n    results[\"discrimination_check\"] = disc_check\n    results[\"steps\"].append(f\"5. Discrimination (Gini={disc_check.get('gini', 'N/A'):.3f})\")\n\n    # Step 5: Factor analysis\n    problem_factors = identify_problem_factors(diagnostics)\n    results[\"problem_factors\"] = problem_factors\n    results[\"steps\"].append(f\"6. Identified {len(problem_factors)} factors needing attention\")\n\n    # Step 6: Recommendations\n    recommendations = []\n\n    if not cal_check[\"passed\"]:\n        recommendations.append(\"Improve calibration: add interactions or non-linear terms\")\n\n    if disc_check.get(\"gini\", 1.0) &lt; 0.3:\n        recommendations.append(\"Improve discrimination: add more predictive features\")\n\n    for pf in problem_factors:\n        recommendations.append(f\"Fix {pf['factor']}: {pf['recommendation']}\")\n\n    results[\"recommendations\"] = recommendations\n\n    return results\n\n\n# Run workflow\nworkflow_results = full_glm_workflow(\n    data=data,\n    response=\"claim_amount\",\n    exposure=\"exposure\",\n    categorical_vars=[\"region\", \"vehicle_type\"],\n    continuous_vars=[\"vehicle_age\", \"driver_experience\"],\n)\n\nprint(\"\\n=== WORKFLOW SUMMARY ===\")\nfor step in workflow_results[\"steps\"]:\n    print(f\"  {step}\")\n\nprint(\"\\n=== RECOMMENDATIONS ===\")\nfor rec in workflow_results[\"recommendations\"]:\n    print(f\"  \u2192 {rec}\")\n</code></pre>"},{"location":"guides/glm-workflow/#quick-reference-diagnostic-decision-table","title":"Quick Reference: Diagnostic Decision Table","text":"Diagnostic Good Warning Action Convergence converged=True converged=False \u2191 max_iter, add regularization Overall A/E 0.95-1.05 &lt;0.95 or &gt;1.05 Check model, add predictors H-L p-value &gt;0.05 &lt;0.05 Add interactions/splines Gini &gt;0.30 &lt;0.20 Add predictive features Dispersion 0.8-1.2 &gt;1.5 Use quasi-family or NegBin Factor A/E spread &lt;0.10 &gt;0.15 Add splines/interactions Residual correlation &lt;0.03 &gt;0.05 Factor not captured well"},{"location":"guides/glm-workflow/#next-steps","title":"Next Steps","text":"<ul> <li>Regularization Theory \u2014 Mathematical background</li> <li>Diagnostics API Reference \u2014 Full API documentation</li> <li>Adding Interactions \u2014 How interactions work</li> <li>Spline Basis Functions \u2014 Non-linear effects</li> </ul>"},{"location":"maintenance/adding-family/","title":"Adding a New Family","text":"<p>This guide walks through adding a new distribution family to RustyStats.</p>"},{"location":"maintenance/adding-family/#overview","title":"Overview","text":"<p>Adding a new family requires: 1. Implement the <code>Family</code> trait in Rust 2. Add Python bindings via PyO3 3. Register in the Python package 4. Add tests 5. Update documentation</p> <p>We'll use Inverse Gaussian as an example.</p>"},{"location":"maintenance/adding-family/#step-1-create-the-rust-implementation","title":"Step 1: Create the Rust Implementation","text":""},{"location":"maintenance/adding-family/#create-the-file","title":"Create the File","text":"<pre><code>touch crates/rustystats-core/src/families/inverse_gaussian.rs\n</code></pre>"},{"location":"maintenance/adding-family/#implement-the-family-trait","title":"Implement the Family Trait","text":"<pre><code>// crates/rustystats-core/src/families/inverse_gaussian.rs\n\nuse ndarray::Array1;\nuse crate::links::{Link, LogLink};\n\n/// Inverse Gaussian family for positive continuous data.\n///\n/// The Inverse Gaussian distribution is useful for modeling\n/// positive continuous data with positive skew.\n///\n/// Variance function: V(\u03bc) = \u03bc\u00b3\npub struct InverseGaussianFamily;\n\nimpl super::Family for InverseGaussianFamily {\n    fn name(&amp;self) -&gt; &amp;str {\n        \"InverseGaussian\"\n    }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        // V(\u03bc) = \u03bc\u00b3\n        mu.mapv(|m| m.powi(3))\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        // d(y, \u03bc) = (y - \u03bc)\u00b2 / (y \u00d7 \u03bc\u00b2)\n        use ndarray::Zip;\n\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            let yi_safe = yi.max(1e-10);\n            let mui_safe = mui.max(1e-10);\n            (yi_safe - mui_safe).powi(2) / (yi_safe * mui_safe.powi(2))\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        // Log link is common for positive data\n        Box::new(LogLink)\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        // Start at observed values, clamped to positive\n        y.mapv(|yi| yi.max(0.1))\n    }\n\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;m| m &gt; 0.0)\n    }\n}\n</code></pre>"},{"location":"maintenance/adding-family/#add-unit-tests","title":"Add Unit Tests","text":"<pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n    use ndarray::array;\n    use approx::assert_relative_eq;\n\n    #[test]\n    fn test_name() {\n        let family = InverseGaussianFamily;\n        assert_eq!(family.name(), \"InverseGaussian\");\n    }\n\n    #[test]\n    fn test_variance() {\n        let family = InverseGaussianFamily;\n        let mu = array![1.0, 2.0, 3.0];\n        let var = family.variance(&amp;mu);\n\n        assert_relative_eq!(var[0], 1.0, epsilon = 1e-10);  // 1\u00b3\n        assert_relative_eq!(var[1], 8.0, epsilon = 1e-10);  // 2\u00b3\n        assert_relative_eq!(var[2], 27.0, epsilon = 1e-10); // 3\u00b3\n    }\n\n    #[test]\n    fn test_deviance_perfect_fit() {\n        let family = InverseGaussianFamily;\n        let y = array![1.0, 2.0, 3.0];\n        let mu = array![1.0, 2.0, 3.0];\n        let dev = family.unit_deviance(&amp;y, &amp;mu);\n\n        // Perfect fit: deviance should be 0\n        for d in dev.iter() {\n            assert_relative_eq!(*d, 0.0, epsilon = 1e-10);\n        }\n    }\n\n    #[test]\n    fn test_valid_mu() {\n        let family = InverseGaussianFamily;\n\n        assert!(family.is_valid_mu(&amp;array![0.1, 1.0, 10.0]));\n        assert!(!family.is_valid_mu(&amp;array![0.0, 1.0, 2.0]));\n        assert!(!family.is_valid_mu(&amp;array![-1.0, 1.0, 2.0]));\n    }\n}\n</code></pre>"},{"location":"maintenance/adding-family/#export-from-module","title":"Export from Module","text":"<pre><code>// crates/rustystats-core/src/families/mod.rs\n\nmod inverse_gaussian;\npub use inverse_gaussian::InverseGaussianFamily;\n</code></pre>"},{"location":"maintenance/adding-family/#run-rust-tests","title":"Run Rust Tests","text":"<pre><code>cargo test -p rustystats-core inverse_gaussian\n</code></pre>"},{"location":"maintenance/adding-family/#step-2-add-python-bindings","title":"Step 2: Add Python Bindings","text":""},{"location":"maintenance/adding-family/#add-the-wrapper-class","title":"Add the Wrapper Class","text":"<pre><code>// crates/rustystats/src/lib.rs\n\nuse rustystats_core::families::InverseGaussianFamily;\n\n/// Inverse Gaussian family for positive continuous data.\n///\n/// Variance function: V(\u03bc) = \u03bc\u00b3\n/// Use for positive, right-skewed continuous data.\n#[pyclass(name = \"InverseGaussianFamily\")]\n#[derive(Clone)]\npub struct PyInverseGaussianFamily {\n    inner: InverseGaussianFamily,\n}\n\n#[pymethods]\nimpl PyInverseGaussianFamily {\n    #[new]\n    fn new() -&gt; Self {\n        Self { inner: InverseGaussianFamily }\n    }\n\n    fn name(&amp;self) -&gt; &amp;str {\n        self.inner.name()\n    }\n\n    fn variance&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;, mu: PyReadonlyArray1&lt;f64&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        let mu_array = mu.as_array().to_owned();\n        let result = self.inner.variance(&amp;mu_array);\n        result.into_pyarray_bound(py)\n    }\n\n    fn unit_deviance&lt;'py&gt;(\n        &amp;self,\n        py: Python&lt;'py&gt;,\n        y: PyReadonlyArray1&lt;f64&gt;,\n        mu: PyReadonlyArray1&lt;f64&gt;,\n    ) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        let y_array = y.as_array().to_owned();\n        let mu_array = mu.as_array().to_owned();\n        let result = self.inner.unit_deviance(&amp;y_array, &amp;mu_array);\n        result.into_pyarray_bound(py)\n    }\n\n    fn deviance(&amp;self, y: PyReadonlyArray1&lt;f64&gt;, mu: PyReadonlyArray1&lt;f64&gt;) -&gt; f64 {\n        let y_array = y.as_array().to_owned();\n        let mu_array = mu.as_array().to_owned();\n        self.inner.deviance(&amp;y_array, &amp;mu_array, None)\n    }\n\n    fn default_link(&amp;self) -&gt; PyLogLink {\n        PyLogLink::new()\n    }\n}\n</code></pre>"},{"location":"maintenance/adding-family/#register-in-module","title":"Register in Module","text":"<pre><code>// In the #[pymodule] function\n#[pymodule]\nfn _rustystats(m: &amp;Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    // ... existing registrations\n    m.add_class::&lt;PyInverseGaussianFamily&gt;()?;\n    Ok(())\n}\n</code></pre>"},{"location":"maintenance/adding-family/#add-to-fit_glm-family-matching","title":"Add to fit_glm Family Matching","text":"<pre><code>// In the fit_glm function\nlet (family_obj, link_obj): (Box&lt;dyn Family&gt;, Box&lt;dyn Link&gt;) = match family {\n    \"gaussian\" =&gt; (Box::new(GaussianFamily), Box::new(IdentityLink)),\n    \"poisson\" =&gt; (Box::new(PoissonFamily), Box::new(LogLink)),\n    // ... existing families\n    \"inversegaussian\" | \"inverse_gaussian\" =&gt; {\n        (Box::new(InverseGaussianFamily), Box::new(LogLink))\n    }\n    _ =&gt; return Err(PyValueError::new_err(format!(\"Unknown family: {}\", family))),\n};\n</code></pre>"},{"location":"maintenance/adding-family/#step-3-export-from-python-package","title":"Step 3: Export from Python Package","text":"<pre><code># python/rustystats/__init__.py\n\nfrom rustystats._rustystats import (\n    # ... existing imports\n    InverseGaussianFamily,\n)\n\n# Add to families module\nfrom rustystats import families\nfamilies.InverseGaussian = InverseGaussianFamily\n</code></pre> <pre><code># python/rustystats/families.py\n\nfrom rustystats._rustystats import (\n    # ... existing imports\n    InverseGaussianFamily as InverseGaussian,\n)\n</code></pre>"},{"location":"maintenance/adding-family/#step-4-add-python-tests","title":"Step 4: Add Python Tests","text":"<pre><code># tests/python/test_inverse_gaussian.py\n\nimport pytest\nimport numpy as np\nimport rustystats as rs\n\nclass TestInverseGaussianFamily:\n    \"\"\"Tests for Inverse Gaussian family.\"\"\"\n\n    def test_family_exists(self):\n        \"\"\"Test that family can be instantiated.\"\"\"\n        family = rs.families.InverseGaussian()\n        assert family.name() == \"InverseGaussian\"\n\n    def test_variance_function(self):\n        \"\"\"Test V(\u03bc) = \u03bc\u00b3.\"\"\"\n        family = rs.families.InverseGaussian()\n        mu = np.array([1.0, 2.0, 3.0])\n        var = family.variance(mu)\n\n        expected = np.array([1.0, 8.0, 27.0])\n        np.testing.assert_allclose(var, expected)\n\n    def test_fit_glm(self):\n        \"\"\"Test fitting with inverse gaussian family.\"\"\"\n        np.random.seed(42)\n        n = 100\n        x = np.random.randn(n)\n        mu = np.exp(1.0 + 0.5 * x)\n        y = np.random.wald(mu, 1.0)  # Inverse Gaussian samples\n\n        X = np.column_stack([np.ones(n), x])\n        result = rs.fit_glm(y, X, family=\"inversegaussian\")\n\n        assert result.converged\n        assert len(result.params) == 2\n\n    def test_predictions_positive(self):\n        \"\"\"Test that predictions are always positive.\"\"\"\n        np.random.seed(42)\n        n = 50\n        y = np.abs(np.random.randn(n)) + 0.1\n        X = np.column_stack([np.ones(n), np.random.randn(n)])\n\n        result = rs.fit_glm(y, X, family=\"inversegaussian\")\n\n        assert np.all(result.fittedvalues &gt; 0)\n</code></pre>"},{"location":"maintenance/adding-family/#run-python-tests","title":"Run Python Tests","text":"<pre><code>uv run pytest tests/python/test_inverse_gaussian.py -v\n</code></pre>"},{"location":"maintenance/adding-family/#step-5-update-documentation","title":"Step 5: Update Documentation","text":""},{"location":"maintenance/adding-family/#add-to-families-theory-page","title":"Add to Families Theory Page","text":"<pre><code>&lt;!-- docs/theory/families.md --&gt;\n\n## Inverse Gaussian Family\n\n**Use for**: Positive continuous data with right skew.\n\n### Properties\n\n| Property | Value |\n|----------|-------|\n| Variance | V(\u03bc) = \u03bc\u00b3 |\n| Canonical link | 1/\u03bc\u00b2 |\n| Common link | Log |\n| Valid \u03bc range | (0, +\u221e) |\n\n### Unit Deviance\n\n$$\nd(y, \\mu) = \\frac{(y - \\mu)^2}{y \\mu^2}\n$$\n</code></pre>"},{"location":"maintenance/adding-family/#add-to-quick-start","title":"Add to Quick Start","text":"<pre><code>&lt;!-- docs/getting-started/quickstart.md --&gt;\n\n| Data Type | Family | Example |\n|-----------|--------|---------|\n| ... | ... | ... |\n| Positive right-skewed | `\"inversegaussian\"` | Waiting times, durations |\n</code></pre>"},{"location":"maintenance/adding-family/#step-6-rebuild-and-test","title":"Step 6: Rebuild and Test","text":"<pre><code># Rebuild\nmaturin develop\n\n# Run all tests\ncargo test -p rustystats-core\nuv run pytest tests/python/ -v\n\n# Verify docs build\nmkdocs build\n</code></pre>"},{"location":"maintenance/adding-family/#checklist","title":"Checklist","text":"<ul> <li>[ ] Rust implementation in <code>families/</code></li> <li>[ ] Unit tests for variance, deviance, initialization</li> <li>[ ] Export from <code>families/mod.rs</code></li> <li>[ ] PyO3 wrapper class</li> <li>[ ] Register in <code>#[pymodule]</code></li> <li>[ ] Add to <code>fit_glm</code> family matching</li> <li>[ ] Export from Python package</li> <li>[ ] Python tests</li> <li>[ ] Documentation updates</li> <li>[ ] Full test suite passes</li> </ul>"},{"location":"maintenance/adding-link/","title":"Adding a New Link Function","text":"<p>This guide walks through adding a new link function to RustyStats.</p>"},{"location":"maintenance/adding-link/#overview","title":"Overview","text":"<p>Adding a new link requires: 1. Implement the <code>Link</code> trait in Rust 2. Add Python bindings via PyO3 3. Register in the Python package 4. Add tests 5. Update documentation</p> <p>We'll use Probit (inverse normal CDF) as an example.</p>"},{"location":"maintenance/adding-link/#step-1-create-the-rust-implementation","title":"Step 1: Create the Rust Implementation","text":""},{"location":"maintenance/adding-link/#create-the-file","title":"Create the File","text":"<pre><code>touch crates/rustystats-core/src/links/probit.rs\n</code></pre>"},{"location":"maintenance/adding-link/#implement-the-link-trait","title":"Implement the Link Trait","text":"<pre><code>// crates/rustystats-core/src/links/probit.rs\n\nuse ndarray::Array1;\nuse statrs::distribution::{ContinuousCDF, Normal};\n\n/// Probit link function: \u03b7 = \u03a6\u207b\u00b9(\u03bc)\n///\n/// Uses the inverse of the standard normal CDF.\n/// Alternative to logit for binary data.\npub struct ProbitLink {\n    normal: Normal,\n}\n\nimpl ProbitLink {\n    pub fn new() -&gt; Self {\n        Self {\n            normal: Normal::new(0.0, 1.0).unwrap(),\n        }\n    }\n}\n\nimpl Default for ProbitLink {\n    fn default() -&gt; Self {\n        Self::new()\n    }\n}\n\nimpl super::Link for ProbitLink {\n    fn name(&amp;self) -&gt; &amp;str {\n        \"Probit\"\n    }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        // \u03b7 = \u03a6\u207b\u00b9(\u03bc)\n        mu.mapv(|m| {\n            // Clamp to avoid infinite values\n            let m_safe = m.clamp(1e-10, 1.0 - 1e-10);\n            self.normal.inverse_cdf(m_safe)\n        })\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        // \u03bc = \u03a6(\u03b7)\n        eta.mapv(|e| self.normal.cdf(e))\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        // d\u03b7/d\u03bc = 1/\u03c6(\u03a6\u207b\u00b9(\u03bc))\n        // where \u03c6 is the standard normal PDF\n        use statrs::distribution::Continuous;\n\n        mu.mapv(|m| {\n            let m_safe = m.clamp(1e-10, 1.0 - 1e-10);\n            let z = self.normal.inverse_cdf(m_safe);\n            let pdf = self.normal.pdf(z);\n\n            if pdf &gt; 1e-10 {\n                1.0 / pdf\n            } else {\n                1e10  // Cap at large value\n            }\n        })\n    }\n}\n\n// Need Clone for PyO3\nimpl Clone for ProbitLink {\n    fn clone(&amp;self) -&gt; Self {\n        Self::new()\n    }\n}\n</code></pre>"},{"location":"maintenance/adding-link/#add-unit-tests","title":"Add Unit Tests","text":"<pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n    use ndarray::array;\n    use approx::assert_relative_eq;\n\n    #[test]\n    fn test_name() {\n        let link = ProbitLink::new();\n        assert_eq!(link.name(), \"Probit\");\n    }\n\n    #[test]\n    fn test_link_at_half() {\n        let link = ProbitLink::new();\n        let mu = array![0.5];\n        let eta = link.link(&amp;mu);\n\n        // \u03a6\u207b\u00b9(0.5) = 0\n        assert_relative_eq!(eta[0], 0.0, epsilon = 1e-10);\n    }\n\n    #[test]\n    fn test_inverse_at_zero() {\n        let link = ProbitLink::new();\n        let eta = array![0.0];\n        let mu = link.inverse(&amp;eta);\n\n        // \u03a6(0) = 0.5\n        assert_relative_eq!(mu[0], 0.5, epsilon = 1e-10);\n    }\n\n    #[test]\n    fn test_roundtrip() {\n        let link = ProbitLink::new();\n        let mu_original = array![0.1, 0.3, 0.5, 0.7, 0.9];\n\n        let eta = link.link(&amp;mu_original);\n        let mu_recovered = link.inverse(&amp;eta);\n\n        for i in 0..mu_original.len() {\n            assert_relative_eq!(mu_original[i], mu_recovered[i], epsilon = 1e-8);\n        }\n    }\n\n    #[test]\n    fn test_bounds() {\n        let link = ProbitLink::new();\n\n        // Very negative eta should give mu near 0\n        let eta_neg = array![-5.0];\n        let mu = link.inverse(&amp;eta_neg);\n        assert!(mu[0] &lt; 0.001);\n        assert!(mu[0] &gt; 0.0);\n\n        // Very positive eta should give mu near 1\n        let eta_pos = array![5.0];\n        let mu = link.inverse(&amp;eta_pos);\n        assert!(mu[0] &gt; 0.999);\n        assert!(mu[0] &lt; 1.0);\n    }\n\n    #[test]\n    fn test_derivative_positive() {\n        let link = ProbitLink::new();\n        let mu = array![0.1, 0.5, 0.9];\n        let deriv = link.derivative(&amp;mu);\n\n        // Derivative should always be positive\n        for d in deriv.iter() {\n            assert!(*d &gt; 0.0);\n        }\n    }\n}\n</code></pre>"},{"location":"maintenance/adding-link/#export-from-module","title":"Export from Module","text":"<pre><code>// crates/rustystats-core/src/links/mod.rs\n\nmod probit;\npub use probit::ProbitLink;\n</code></pre>"},{"location":"maintenance/adding-link/#run-rust-tests","title":"Run Rust Tests","text":"<pre><code>cargo test -p rustystats-core probit\n</code></pre>"},{"location":"maintenance/adding-link/#step-2-add-python-bindings","title":"Step 2: Add Python Bindings","text":""},{"location":"maintenance/adding-link/#add-the-wrapper-class","title":"Add the Wrapper Class","text":"<pre><code>// crates/rustystats/src/lib.rs\n\nuse rustystats_core::links::ProbitLink;\n\n/// Probit link function: \u03b7 = \u03a6\u207b\u00b9(\u03bc)\n///\n/// Uses the inverse standard normal CDF.\n/// Alternative to logit for Binomial family.\n#[pyclass(name = \"ProbitLink\")]\n#[derive(Clone)]\npub struct PyProbitLink {\n    inner: ProbitLink,\n}\n\n#[pymethods]\nimpl PyProbitLink {\n    #[new]\n    fn new() -&gt; Self {\n        Self { inner: ProbitLink::new() }\n    }\n\n    fn name(&amp;self) -&gt; &amp;str {\n        self.inner.name()\n    }\n\n    fn link&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;, mu: PyReadonlyArray1&lt;f64&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        let mu_array = mu.as_array().to_owned();\n        let result = self.inner.link(&amp;mu_array);\n        result.into_pyarray_bound(py)\n    }\n\n    fn inverse&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;, eta: PyReadonlyArray1&lt;f64&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        let eta_array = eta.as_array().to_owned();\n        let result = self.inner.inverse(&amp;eta_array);\n        result.into_pyarray_bound(py)\n    }\n\n    fn derivative&lt;'py&gt;(&amp;self, py: Python&lt;'py&gt;, mu: PyReadonlyArray1&lt;f64&gt;) -&gt; Bound&lt;'py, PyArray1&lt;f64&gt;&gt; {\n        let mu_array = mu.as_array().to_owned();\n        let result = self.inner.derivative(&amp;mu_array);\n        result.into_pyarray_bound(py)\n    }\n}\n</code></pre>"},{"location":"maintenance/adding-link/#register-in-module","title":"Register in Module","text":"<pre><code>// In the #[pymodule] function\n#[pymodule]\nfn _rustystats(m: &amp;Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    // ... existing registrations\n    m.add_class::&lt;PyProbitLink&gt;()?;\n    Ok(())\n}\n</code></pre>"},{"location":"maintenance/adding-link/#add-to-fit_glm-link-resolution","title":"Add to fit_glm Link Resolution","text":"<pre><code>// Helper function to get link from string\nfn get_link(link: Option&lt;&amp;str&gt;, family: &amp;str) -&gt; Box&lt;dyn Link&gt; {\n    match link {\n        Some(\"identity\") =&gt; Box::new(IdentityLink),\n        Some(\"log\") =&gt; Box::new(LogLink),\n        Some(\"logit\") =&gt; Box::new(LogitLink),\n        Some(\"probit\") =&gt; Box::new(ProbitLink::new()),\n        None =&gt; {\n            // Use family default\n            match family {\n                \"gaussian\" =&gt; Box::new(IdentityLink),\n                \"poisson\" | \"gamma\" =&gt; Box::new(LogLink),\n                \"binomial\" =&gt; Box::new(LogitLink),\n                _ =&gt; Box::new(IdentityLink),\n            }\n        }\n        Some(other) =&gt; panic!(\"Unknown link: {}\", other),\n    }\n}\n</code></pre>"},{"location":"maintenance/adding-link/#step-3-export-from-python-package","title":"Step 3: Export from Python Package","text":"<pre><code># python/rustystats/__init__.py\n\nfrom rustystats._rustystats import (\n    # ... existing imports\n    ProbitLink,\n)\n</code></pre> <pre><code># python/rustystats/links.py\n\nfrom rustystats._rustystats import (\n    IdentityLink as Identity,\n    LogLink as Log,\n    LogitLink as Logit,\n    ProbitLink as Probit,\n)\n</code></pre>"},{"location":"maintenance/adding-link/#step-4-add-python-tests","title":"Step 4: Add Python Tests","text":"<pre><code># tests/python/test_probit.py\n\nimport pytest\nimport numpy as np\nimport rustystats as rs\nfrom scipy import stats\n\nclass TestProbitLink:\n    \"\"\"Tests for Probit link function.\"\"\"\n\n    def test_link_exists(self):\n        \"\"\"Test that link can be instantiated.\"\"\"\n        link = rs.ProbitLink()\n        assert link.name() == \"Probit\"\n\n    def test_link_at_half(self):\n        \"\"\"Test \u03a6\u207b\u00b9(0.5) = 0.\"\"\"\n        link = rs.ProbitLink()\n        mu = np.array([0.5])\n        eta = link.link(mu)\n        np.testing.assert_allclose(eta, [0.0], atol=1e-10)\n\n    def test_inverse_at_zero(self):\n        \"\"\"Test \u03a6(0) = 0.5.\"\"\"\n        link = rs.ProbitLink()\n        eta = np.array([0.0])\n        mu = link.inverse(eta)\n        np.testing.assert_allclose(mu, [0.5], atol=1e-10)\n\n    def test_vs_scipy(self):\n        \"\"\"Compare to scipy implementation.\"\"\"\n        link = rs.ProbitLink()\n        mu = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n\n        eta_rustystats = link.link(mu)\n        eta_scipy = stats.norm.ppf(mu)\n\n        np.testing.assert_allclose(eta_rustystats, eta_scipy, rtol=1e-6)\n\n    def test_roundtrip(self):\n        \"\"\"Test link \u2192 inverse roundtrip.\"\"\"\n        link = rs.ProbitLink()\n        mu_original = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n\n        eta = link.link(mu_original)\n        mu_recovered = link.inverse(eta)\n\n        np.testing.assert_allclose(mu_original, mu_recovered, rtol=1e-8)\n\n    def test_fit_binomial_with_probit(self):\n        \"\"\"Test fitting binomial with probit link.\"\"\"\n        np.random.seed(42)\n        n = 200\n        x = np.random.randn(n)\n\n        # Generate probit model data\n        eta = 0.5 + 0.3 * x\n        p = stats.norm.cdf(eta)\n        y = np.random.binomial(1, p)\n\n        X = np.column_stack([np.ones(n), x])\n        result = rs.fit_glm(y, X, family=\"binomial\", link=\"probit\")\n\n        assert result.converged\n        # Coefficients should be close to true values\n        np.testing.assert_allclose(result.params, [0.5, 0.3], atol=0.2)\n</code></pre>"},{"location":"maintenance/adding-link/#run-python-tests","title":"Run Python Tests","text":"<pre><code>uv run pytest tests/python/test_probit.py -v\n</code></pre>"},{"location":"maintenance/adding-link/#step-5-update-documentation","title":"Step 5: Update Documentation","text":""},{"location":"maintenance/adding-link/#add-to-links-theory-page","title":"Add to Links Theory Page","text":"<pre><code>&lt;!-- docs/theory/links.md --&gt;\n\n## Probit Link\n\n**Formula**: \u03b7 = \u03a6\u207b\u00b9(\u03bc)\n\nUses the inverse of the standard normal CDF.\n\n| Property | Formula |\n|----------|---------|\n| Link | g(\u03bc) = \u03a6\u207b\u00b9(\u03bc) |\n| Inverse | g\u207b\u00b9(\u03b7) = \u03a6(\u03b7) |\n| Derivative | g'(\u03bc) = 1/\u03c6(\u03a6\u207b\u00b9(\u03bc)) |\n\n### When to Use\n\n- Alternative to logit for binomial data\n- When underlying process involves normal distribution\n- Dose-response models\n\n### Comparison to Logit\n\nBoth map (0,1) to (-\u221e, +\u221e), but:\n- Logit: heavier tails\n- Probit: lighter tails, closer to 0/1 more quickly\n</code></pre>"},{"location":"maintenance/adding-link/#step-6-rebuild-and-test","title":"Step 6: Rebuild and Test","text":"<pre><code># Rebuild\nmaturin develop\n\n# Run all tests\ncargo test -p rustystats-core\nuv run pytest tests/python/ -v\n\n# Verify docs build\nmkdocs build\n</code></pre>"},{"location":"maintenance/adding-link/#checklist","title":"Checklist","text":"<ul> <li>[ ] Rust implementation in <code>links/</code></li> <li>[ ] Unit tests for link, inverse, derivative, roundtrip</li> <li>[ ] Export from <code>links/mod.rs</code></li> <li>[ ] PyO3 wrapper class</li> <li>[ ] Register in <code>#[pymodule]</code></li> <li>[ ] Add to link resolution in <code>fit_glm</code></li> <li>[ ] Export from Python package</li> <li>[ ] Python tests (including comparison to scipy)</li> <li>[ ] Documentation updates</li> <li>[ ] Full test suite passes</li> </ul>"},{"location":"maintenance/performance/","title":"Performance Guide","text":"<p>This guide covers performance optimization, profiling, and benchmarking for RustyStats.</p>"},{"location":"maintenance/performance/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"maintenance/performance/#complexity-analysis","title":"Complexity Analysis","text":"Operation Complexity Notes IRLS iteration O(np\u00b2 + p\u00b3) X'WX computation + solve Single coordinate descent step O(np) Update one coefficient Full CD iteration (all p) O(np\u00b2) With Gram matrix Spline basis evaluation O(n \u00d7 df \u00d7 degree) Per-observation Target encoding O(n \u00d7 n_perms) With permutations"},{"location":"maintenance/performance/#typical-performance","title":"Typical Performance","text":"Dataset Features Family Time (release) 10K rows 20 Poisson ~20ms 100K rows 50 Poisson ~200ms 500K rows 50 Poisson ~800ms 1M rows 100 Gaussian ~2s"},{"location":"maintenance/performance/#build-optimization","title":"Build Optimization","text":""},{"location":"maintenance/performance/#always-use-release-mode","title":"Always Use Release Mode","text":"<pre><code># Development (fast compile, slow run)\nmaturin develop\n\n# Release (slow compile, fast run)\nmaturin develop --release\n</code></pre> <p>Release mode enables: - Optimizations (<code>-O3</code>) - LTO (Link-Time Optimization) - Inlining - SIMD auto-vectorization</p>"},{"location":"maintenance/performance/#cargo-profile","title":"Cargo Profile","text":"<pre><code># Cargo.toml\n[profile.release]\nlto = true\ncodegen-units = 1\nopt-level = 3\n</code></pre>"},{"location":"maintenance/performance/#parallelization","title":"Parallelization","text":""},{"location":"maintenance/performance/#rayon-configuration","title":"Rayon Configuration","text":"<p>Rayon automatically uses all available cores. Control with:</p> <pre><code>// Limit threads\nrayon::ThreadPoolBuilder::new()\n    .num_threads(4)\n    .build_global()\n    .unwrap();\n</code></pre> <p>Or environment variable: <pre><code>RAYON_NUM_THREADS=4 python script.py\n</code></pre></p>"},{"location":"maintenance/performance/#parallel-patterns","title":"Parallel Patterns","text":""},{"location":"maintenance/performance/#parallel-map","title":"Parallel Map","text":"<pre><code>let results: Vec&lt;_&gt; = data.par_iter()\n    .map(|x| expensive_compute(x))\n    .collect();\n</code></pre>"},{"location":"maintenance/performance/#parallel-fold-reduce","title":"Parallel Fold-Reduce","text":"<pre><code>let sum = (0..n).into_par_iter()\n    .fold(|| 0.0, |acc, i| acc + data[i])\n    .reduce(|| 0.0, |a, b| a + b);\n</code></pre>"},{"location":"maintenance/performance/#when-to-parallelize","title":"When to Parallelize","text":"<ul> <li>Good: Independent computations, large n</li> <li>Bad: Small n (overhead dominates), sequential dependencies</li> </ul> <p>Rule of thumb: Parallelize when n &gt; 1000.</p>"},{"location":"maintenance/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"maintenance/performance/#avoid-unnecessary-copies","title":"Avoid Unnecessary Copies","text":"<pre><code>// Bad: clones array\nfn process(data: Array1&lt;f64&gt;) { ... }\n\n// Good: borrows\nfn process(data: &amp;Array1&lt;f64&gt;) { ... }\n\n// Good: moves ownership when needed\nfn consume(data: Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; { ... }\n</code></pre>"},{"location":"maintenance/performance/#pre-allocate-buffers","title":"Pre-allocate Buffers","text":"<pre><code>// Bad: allocates in loop\nfor iter in 0..max_iter {\n    let buffer = Array1::zeros(n);\n    // ...\n}\n\n// Good: allocate once\nlet mut buffer = Array1::zeros(n);\nfor iter in 0..max_iter {\n    buffer.fill(0.0);\n    // ...\n}\n</code></pre>"},{"location":"maintenance/performance/#use-views","title":"Use Views","text":"<pre><code>// No copy - just a view\nlet column = matrix.column(j);\nlet slice = array.slice(s![10..20]);\n</code></pre>"},{"location":"maintenance/performance/#memory-layout","title":"Memory Layout","text":"<p>ndarray uses row-major by default. Access patterns matter:</p> <pre><code>// Good: row-wise access (contiguous)\nfor row in matrix.rows() {\n    for &amp;val in row.iter() { ... }\n}\n\n// Bad: column-wise access (strided)\nfor col in matrix.columns() {\n    for &amp;val in col.iter() { ... }\n}\n</code></pre>"},{"location":"maintenance/performance/#profiling","title":"Profiling","text":""},{"location":"maintenance/performance/#rust-profiling","title":"Rust Profiling","text":""},{"location":"maintenance/performance/#using-perf-linux","title":"Using perf (Linux)","text":"<pre><code># Build with debug symbols\ncargo build --release\n\n# Record profile\nperf record -g ./target/release/benchmark\n\n# View report\nperf report\n</code></pre>"},{"location":"maintenance/performance/#using-flamegraph","title":"Using flamegraph","text":"<pre><code># Install\ncargo install flamegraph\n\n# Generate\ncargo flamegraph --bin benchmark\n</code></pre>"},{"location":"maintenance/performance/#using-cargo-instruments-macos","title":"Using cargo-instruments (macOS)","text":"<pre><code>cargo install cargo-instruments\ncargo instruments -t \"Time Profiler\" --release\n</code></pre>"},{"location":"maintenance/performance/#python-profiling","title":"Python Profiling","text":""},{"location":"maintenance/performance/#cprofile","title":"cProfile","text":"<pre><code>import cProfile\nimport pstats\n\nwith cProfile.Profile() as pr:\n    result = rs.fit_glm(y, X, family=\"poisson\")\n\nstats = pstats.Stats(pr)\nstats.sort_stats('cumulative')\nstats.print_stats(10)\n</code></pre>"},{"location":"maintenance/performance/#line_profiler","title":"line_profiler","text":"<pre><code># Install: pip install line_profiler\n\n@profile\ndef benchmark():\n    result = rs.fit_glm(y, X, family=\"poisson\")\n    return result\n\n# Run: kernprof -l -v script.py\n</code></pre>"},{"location":"maintenance/performance/#benchmarking","title":"Benchmarking","text":""},{"location":"maintenance/performance/#rust-benchmarks","title":"Rust Benchmarks","text":"<pre><code>// benches/irls.rs\nuse criterion::{criterion_group, criterion_main, Criterion};\n\nfn bench_irls(c: &amp;mut Criterion) {\n    let (y, x) = generate_data(10000, 20);\n\n    c.bench_function(\"irls_10k_20\", |b| {\n        b.iter(|| {\n            fit_glm(&amp;y, &amp;x, &amp;PoissonFamily, &amp;LogLink, &amp;IRLSConfig::default())\n        })\n    });\n}\n\ncriterion_group!(benches, bench_irls);\ncriterion_main!(benches);\n</code></pre> <p>Run with: <pre><code>cargo bench\n</code></pre></p>"},{"location":"maintenance/performance/#python-benchmarks","title":"Python Benchmarks","text":"<pre><code>import time\nimport numpy as np\nimport rustystats as rs\n\ndef benchmark(n, p, family=\"poisson\", n_runs=10):\n    \"\"\"Benchmark GLM fitting.\"\"\"\n    y = np.random.poisson(5, n).astype(float)\n    X = np.column_stack([np.ones(n), np.random.randn(n, p)])\n\n    times = []\n    for _ in range(n_runs):\n        start = time.perf_counter()\n        result = rs.fit_glm(y, X, family=family)\n        times.append(time.perf_counter() - start)\n\n    print(f\"n={n}, p={p}, family={family}\")\n    print(f\"  Mean: {np.mean(times)*1000:.1f}ms\")\n    print(f\"  Std:  {np.std(times)*1000:.1f}ms\")\n    print(f\"  Iterations: {result.iterations}\")\n\n# Run benchmarks\nfor n in [10000, 100000, 500000]:\n    benchmark(n, 20)\n</code></pre>"},{"location":"maintenance/performance/#common-bottlenecks","title":"Common Bottlenecks","text":""},{"location":"maintenance/performance/#1-xwx-computation","title":"1. X'WX Computation","text":"<p>The Gram matrix computation is often the bottleneck:</p> <pre><code>// O(np\u00b2) - parallelized\nlet xtwx = compute_gram_matrix(x, w);\n</code></pre> <p>Optimization: Use flat Vec instead of Array2 for better cache performance.</p>"},{"location":"maintenance/performance/#2-matrix-solve","title":"2. Matrix Solve","text":"<p>Solving (X'WX)\u03b2 = X'Wz is O(p\u00b3):</p> <pre><code>let beta = cholesky_solve(&amp;xtwx, &amp;xtwz)?;\n</code></pre> <p>Optimization: For very large p, consider iterative solvers.</p>"},{"location":"maintenance/performance/#3-memory-allocation","title":"3. Memory Allocation","text":"<p>Allocations in hot loops hurt performance:</p> <pre><code>// Profile with: MALLOC_CONF=prof:true\n\n// Look for patterns like:\nfor iter in 0..max_iter {\n    let temp = compute();  // Allocation every iteration\n}\n</code></pre>"},{"location":"maintenance/performance/#4-cache-misses","title":"4. Cache Misses","text":"<p>Random memory access patterns cause cache misses:</p> <pre><code>// Bad: random access\nfor &amp;i in random_indices.iter() {\n    result += data[i];\n}\n\n// Good: sequential access\nfor &amp;val in data.iter() {\n    result += val;\n}\n</code></pre>"},{"location":"maintenance/performance/#optimization-checklist","title":"Optimization Checklist","text":"<ul> <li>[ ] Build with <code>--release</code></li> <li>[ ] Enable LTO in Cargo.toml</li> <li>[ ] Parallelize with Rayon where appropriate</li> <li>[ ] Pre-allocate buffers outside loops</li> <li>[ ] Use views instead of copies</li> <li>[ ] Prefer row-major access patterns</li> <li>[ ] Profile to find actual bottlenecks</li> <li>[ ] Benchmark before and after changes</li> </ul>"},{"location":"maintenance/performance/#comparison-with-other-libraries","title":"Comparison with Other Libraries","text":""},{"location":"maintenance/performance/#vs-statsmodels","title":"vs Statsmodels","text":"Operation RustyStats Statsmodels Poisson 100K \u00d7 20 ~100ms ~500ms Lasso GLM 100K \u00d7 50 ~500ms N/A"},{"location":"maintenance/performance/#vs-glmnet-r","title":"vs glmnet (R)","text":"Operation RustyStats glmnet Lasso path 100K \u00d7 100 ~2s ~1.5s <p>glmnet is highly optimized; RustyStats is competitive.</p>"},{"location":"maintenance/performance/#why-rustystats-is-fast","title":"Why RustyStats is Fast","text":"<ol> <li>Rust: No GC, predictable performance</li> <li>Rayon: Automatic parallelization</li> <li>SIMD: Auto-vectorization by LLVM</li> <li>Cache-friendly: Careful memory layout</li> <li>Zero-copy: NumPy interop where possible</li> </ol>"},{"location":"maintenance/rust-best-practices/","title":"Rust Best Practices","text":"<p>This guide covers Rust coding conventions and best practices for maintaining the RustyStats codebase.</p>"},{"location":"maintenance/rust-best-practices/#code-organization","title":"Code Organization","text":""},{"location":"maintenance/rust-best-practices/#module-structure","title":"Module Structure","text":"<p>Each major component follows this pattern:</p> <pre><code>component/\n\u251c\u2500\u2500 mod.rs      # Public interface, re-exports\n\u251c\u2500\u2500 types.rs    # Data structures (optional)\n\u251c\u2500\u2500 impl.rs     # Main implementation (optional)\n\u2514\u2500\u2500 tests.rs    # Tests (or inline with #[cfg(test)])\n</code></pre> <p>For simpler components, everything goes in <code>mod.rs</code>:</p> <pre><code>// component/mod.rs\n\n// Types\npub struct MyType { ... }\n\n// Implementation\nimpl MyType { ... }\n\n// Tests\n#[cfg(test)]\nmod tests { ... }\n</code></pre>"},{"location":"maintenance/rust-best-practices/#public-api","title":"Public API","text":"<p>Export public items at the crate root for convenient access:</p> <pre><code>// lib.rs\npub use families::Family;\npub use families::poisson::PoissonFamily;\n</code></pre> <p>Users can then write: <pre><code>use rustystats_core::PoissonFamily;\n</code></pre></p>"},{"location":"maintenance/rust-best-practices/#error-handling","title":"Error Handling","text":""},{"location":"maintenance/rust-best-practices/#use-thiserror-for-error-types","title":"Use thiserror for Error Types","text":"<pre><code>use thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum RustyStatsError {\n    #[error(\"Invalid input: {0}\")]\n    InvalidInput(String),\n\n    #[error(\"Convergence failed after {iterations} iterations\")]\n    ConvergenceFailure { iterations: usize, tolerance: f64 },\n\n    #[error(\"Dimension mismatch: expected {expected}, got {got}\")]\n    DimensionMismatch { expected: usize, got: usize },\n}\n\npub type Result&lt;T&gt; = std::result::Result&lt;T, RustyStatsError&gt;;\n</code></pre>"},{"location":"maintenance/rust-best-practices/#use-for-propagation","title":"Use ? for Propagation","text":"<pre><code>pub fn compute(x: &amp;Array1&lt;f64&gt;) -&gt; Result&lt;f64&gt; {\n    let validated = validate_input(x)?;  // Propagates error\n    let result = process(validated)?;\n    Ok(result)\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#provide-context","title":"Provide Context","text":"<pre><code>// Bad: loses context\nlet result = operation().map_err(|_| RustyStatsError::InvalidInput(\"failed\".into()))?;\n\n// Good: preserves context\nlet result = operation().map_err(|e| \n    RustyStatsError::InvalidInput(format!(\"operation failed: {}\", e))\n)?;\n</code></pre>"},{"location":"maintenance/rust-best-practices/#traits","title":"Traits","text":""},{"location":"maintenance/rust-best-practices/#use-trait-objects-for-polymorphism","title":"Use Trait Objects for Polymorphism","text":"<pre><code>pub trait Family: Send + Sync {\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n    // ...\n}\n\npub fn fit_glm(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,  // Trait object\n    link: &amp;dyn Link,\n) -&gt; Result&lt;IRLSResult&gt; {\n    // Works with any Family implementation\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#require-send-sync-for-thread-safety","title":"Require Send + Sync for Thread Safety","text":"<pre><code>// Required for parallel processing with Rayon\npub trait Family: Send + Sync { ... }\npub trait Link: Send + Sync { ... }\n</code></pre>"},{"location":"maintenance/rust-best-practices/#default-implementations-where-sensible","title":"Default Implementations Where Sensible","text":"<pre><code>pub trait Family {\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    // Default implementation using unit_deviance\n    fn deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;, \n                weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;) -&gt; f64 {\n        let unit_dev = self.unit_deviance(y, mu);\n        match weights {\n            Some(w) =&gt; (&amp;unit_dev * w).sum(),\n            None =&gt; unit_dev.sum(),\n        }\n    }\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#memory-and-performance","title":"Memory and Performance","text":""},{"location":"maintenance/rust-best-practices/#prefer-references-over-clones","title":"Prefer References Over Clones","text":"<pre><code>// Bad: unnecessary clone\nfn process(data: Array1&lt;f64&gt;) { ... }\n\n// Good: borrow when possible\nfn process(data: &amp;Array1&lt;f64&gt;) { ... }\n</code></pre>"},{"location":"maintenance/rust-best-practices/#use-views-for-read-only-access","title":"Use Views for Read-Only Access","text":"<pre><code>// ndarray view (no copy)\nlet column = matrix.column(j);\n\n// Iterate without owning\nfor row in matrix.rows() {\n    // row is a view, not a copy\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#clone-explicitly-when-needed","title":"Clone Explicitly When Needed","text":"<pre><code>// When you need ownership\nlet owned_data = data.to_owned();\n\n// When returning modified data\nfn double(x: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n    x * 2.0  // Creates new array\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#pre-allocate-containers","title":"Pre-allocate Containers","text":"<pre><code>// Bad: grows dynamically\nlet mut results = Vec::new();\nfor i in 0..n {\n    results.push(compute(i));\n}\n\n// Good: pre-allocate\nlet mut results = Vec::with_capacity(n);\nfor i in 0..n {\n    results.push(compute(i));\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#parallelism-with-rayon","title":"Parallelism with Rayon","text":""},{"location":"maintenance/rust-best-practices/#use-parallel-iterators","title":"Use Parallel Iterators","text":"<pre><code>use rayon::prelude::*;\n\n// Sequential\nlet sum: f64 = (0..n).map(|i| compute(i)).sum();\n\n// Parallel\nlet sum: f64 = (0..n).into_par_iter().map(|i| compute(i)).sum();\n</code></pre>"},{"location":"maintenance/rust-best-practices/#parallel-fold-reduce-pattern","title":"Parallel Fold-Reduce Pattern","text":"<p>For accumulating results:</p> <pre><code>let result = (0..n).into_par_iter()\n    .fold(\n        || initial_value(),  // Per-thread initializer\n        |acc, i| accumulate(acc, i),  // Fold within thread\n    )\n    .reduce(\n        || initial_value(),  // Identity for reduce\n        |a, b| combine(a, b),  // Combine thread results\n    );\n</code></pre>"},{"location":"maintenance/rust-best-practices/#example-parallel-matrix-computation","title":"Example: Parallel Matrix Computation","text":"<pre><code>let (xtwx, xtwz) = (0..n).into_par_iter()\n    .fold(\n        || (Array2::zeros((p, p)), Array1::zeros(p)),\n        |(mut gram, mut moment), i| {\n            let xi = x.row(i);\n            let wi = w[i];\n\n            // Accumulate\n            for j in 0..p {\n                moment[j] += xi[j] * wi * z[i];\n                for k in j..p {\n                    gram[[j, k]] += wi * xi[j] * xi[k];\n                }\n            }\n            (gram, moment)\n        }\n    )\n    .reduce(\n        || (Array2::zeros((p, p)), Array1::zeros(p)),\n        |(g1, m1), (g2, m2)| (g1 + g2, m1 + m2)\n    );\n</code></pre>"},{"location":"maintenance/rust-best-practices/#numerical-stability","title":"Numerical Stability","text":""},{"location":"maintenance/rust-best-practices/#avoid-division-by-zero","title":"Avoid Division by Zero","text":"<pre><code>// Clamp denominators\nlet safe_denom = denom.max(1e-10);\nlet result = numerator / safe_denom;\n</code></pre>"},{"location":"maintenance/rust-best-practices/#avoid-log-of-zero","title":"Avoid Log of Zero","text":"<pre><code>// Clamp before log\nlet safe_x = x.max(1e-10);\nlet log_x = safe_x.ln();\n</code></pre>"},{"location":"maintenance/rust-best-practices/#use-stable-algorithms","title":"Use Stable Algorithms","text":"<pre><code>// Naive log-sum-exp (can overflow)\nlet result = values.iter().map(|x| x.exp()).sum::&lt;f64&gt;().ln();\n\n// Stable log-sum-exp\nfn log_sum_exp(values: &amp;[f64]) -&gt; f64 {\n    let max_val = values.iter().cloned().fold(f64::NEG_INFINITY, f64::max);\n    if max_val.is_infinite() {\n        return max_val;\n    }\n    max_val + values.iter().map(|x| (x - max_val).exp()).sum::&lt;f64&gt;().ln()\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#check-for-naninf","title":"Check for NaN/Inf","text":"<pre><code>fn validate_result(x: f64) -&gt; Result&lt;f64&gt; {\n    if x.is_nan() {\n        return Err(RustyStatsError::NumericalError(\"Result is NaN\".into()));\n    }\n    if x.is_infinite() {\n        return Err(RustyStatsError::NumericalError(\"Result is infinite\".into()));\n    }\n    Ok(x)\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#testing","title":"Testing","text":""},{"location":"maintenance/rust-best-practices/#unit-tests-in-same-module","title":"Unit Tests in Same Module","text":"<pre><code>// In families/poisson.rs\npub struct PoissonFamily;\n\nimpl Family for PoissonFamily { ... }\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_variance() {\n        let family = PoissonFamily;\n        let mu = array![1.0, 2.0, 5.0];\n        let var = family.variance(&amp;mu);\n        assert_eq!(var, mu);\n    }\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#use-approx-for-float-comparisons","title":"Use approx for Float Comparisons","text":"<pre><code>use approx::assert_relative_eq;\n\n#[test]\nfn test_computation() {\n    let result = compute();\n    assert_relative_eq!(result, expected, epsilon = 1e-10);\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#test-edge-cases","title":"Test Edge Cases","text":"<pre><code>#[test]\nfn test_empty_input() {\n    let empty = Array1::&lt;f64&gt;::zeros(0);\n    let result = process(&amp;empty);\n    assert!(result.is_err());\n}\n\n#[test]\nfn test_single_observation() {\n    let single = array![1.0];\n    let result = process(&amp;single);\n    assert!(result.is_ok());\n}\n\n#[test]\nfn test_extreme_values() {\n    let extreme = array![1e-300, 1e300];\n    let result = process(&amp;extreme);\n    // Check numerical stability\n    assert!(!result.unwrap().iter().any(|x| x.is_nan()));\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#documentation","title":"Documentation","text":""},{"location":"maintenance/rust-best-practices/#document-public-items","title":"Document Public Items","text":"<pre><code>/// Compute the variance function V(\u03bc) for the Poisson family.\n/// \n/// For Poisson, V(\u03bc) = \u03bc (variance equals mean).\n/// \n/// # Arguments\n/// * `mu` - Array of mean values, must be positive\n/// \n/// # Returns\n/// Array of variance values\n/// \n/// # Example\n/// ```\n/// use rustystats_core::families::PoissonFamily;\n/// use ndarray::array;\n/// \n/// let family = PoissonFamily;\n/// let mu = array![1.0, 2.0, 3.0];\n/// let var = family.variance(&amp;mu);\n/// assert_eq!(var, mu);\n/// ```\npub fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n    mu.clone()\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#use-module-level-documentation","title":"Use Module-Level Documentation","text":"<pre><code>//! # Distribution Families\n//!\n//! This module implements distribution families for GLMs.\n//!\n//! ## Available Families\n//! - [`GaussianFamily`] - For continuous data\n//! - [`PoissonFamily`] - For count data\n//! - [`BinomialFamily`] - For binary data\n//!\n//! ## Example\n//! ```\n//! use rustystats_core::families::{Family, PoissonFamily};\n//! ```\n\npub struct PoissonFamily;\n</code></pre>"},{"location":"maintenance/rust-best-practices/#common-patterns","title":"Common Patterns","text":""},{"location":"maintenance/rust-best-practices/#builder-pattern-for-complex-types","title":"Builder Pattern for Complex Types","text":"<pre><code>pub struct IRLSConfig {\n    pub max_iterations: usize,\n    pub tolerance: f64,\n    pub verbose: bool,\n}\n\nimpl IRLSConfig {\n    pub fn new() -&gt; Self {\n        Self::default()\n    }\n\n    pub fn max_iterations(mut self, n: usize) -&gt; Self {\n        self.max_iterations = n;\n        self\n    }\n\n    pub fn tolerance(mut self, tol: f64) -&gt; Self {\n        self.tolerance = tol;\n        self\n    }\n}\n\n// Usage\nlet config = IRLSConfig::new()\n    .max_iterations(50)\n    .tolerance(1e-6);\n</code></pre>"},{"location":"maintenance/rust-best-practices/#newtype-pattern-for-type-safety","title":"Newtype Pattern for Type Safety","text":"<pre><code>pub struct Deviance(f64);\npub struct LogLikelihood(f64);\n\nimpl Deviance {\n    pub fn value(&amp;self) -&gt; f64 { self.0 }\n}\n\n// Can't accidentally confuse deviance with log-likelihood\nfn compute_aic(ll: LogLikelihood, p: usize) -&gt; f64 {\n    -2.0 * ll.0 + 2.0 * p as f64\n}\n</code></pre>"},{"location":"maintenance/rust-best-practices/#iterator-chains","title":"Iterator Chains","text":"<pre><code>// Process with iterator chain\nlet result: Vec&lt;_&gt; = data.iter()\n    .filter(|&amp;x| *x &gt; 0.0)\n    .map(|x| x.ln())\n    .collect();\n\n// ndarray version\nlet result = data.mapv(|x| if x &gt; 0.0 { x.ln() } else { 0.0 });\n</code></pre>"},{"location":"maintenance/testing/","title":"Testing Strategy","text":"<p>This guide covers the testing approach for RustyStats, including test organization, writing effective tests, and running the test suite.</p>"},{"location":"maintenance/testing/#test-organization","title":"Test Organization","text":"<pre><code>rustystats/\n\u251c\u2500\u2500 crates/\n\u2502   \u2514\u2500\u2500 rustystats-core/\n\u2502       \u2514\u2500\u2500 src/\n\u2502           \u2514\u2500\u2500 families/\n\u2502               \u2514\u2500\u2500 poisson.rs  # Contains #[cfg(test)] mod tests\n\u2502\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 python/\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 test_glm.py\n        \u251c\u2500\u2500 test_families.py\n        \u251c\u2500\u2500 test_links.py\n        \u251c\u2500\u2500 test_regularization.py\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"maintenance/testing/#rust-unit-tests","title":"Rust Unit Tests","text":"<p>Located inline with the code using <code>#[cfg(test)]</code>:</p> <pre><code>// In families/poisson.rs\npub struct PoissonFamily;\n\nimpl Family for PoissonFamily { ... }\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_variance() { ... }\n}\n</code></pre>"},{"location":"maintenance/testing/#python-integration-tests","title":"Python Integration Tests","text":"<p>Located in <code>tests/python/</code>:</p> <pre><code># tests/python/test_families.py\nimport rustystats as rs\n\ndef test_poisson_variance():\n    ...\n</code></pre>"},{"location":"maintenance/testing/#running-tests","title":"Running Tests","text":""},{"location":"maintenance/testing/#rust-tests","title":"Rust Tests","text":"<pre><code># All Rust tests\ncargo test\n\n# Specific crate\ncargo test -p rustystats-core\n\n# Specific test\ncargo test -p rustystats-core poisson\n\n# With output\ncargo test -p rustystats-core -- --nocapture\n\n# Release mode (faster, catches some bugs)\ncargo test --release\n</code></pre>"},{"location":"maintenance/testing/#python-tests","title":"Python Tests","text":"<pre><code># All Python tests\nuv run pytest tests/python/ -v\n\n# Specific file\nuv run pytest tests/python/test_glm.py -v\n\n# Specific test\nuv run pytest tests/python/test_glm.py::test_poisson_fit -v\n\n# With coverage\nuv run pytest tests/python/ --cov=rustystats\n\n# Stop on first failure\nuv run pytest tests/python/ -x\n</code></pre>"},{"location":"maintenance/testing/#full-test-suite","title":"Full Test Suite","text":"<pre><code># Both Rust and Python\ncargo test &amp;&amp; uv run pytest tests/python/ -v\n</code></pre>"},{"location":"maintenance/testing/#writing-rust-tests","title":"Writing Rust Tests","text":""},{"location":"maintenance/testing/#basic-test-structure","title":"Basic Test Structure","text":"<pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n    use ndarray::array;\n    use approx::assert_relative_eq;\n\n    #[test]\n    fn test_basic_functionality() {\n        // Arrange\n        let input = array![1.0, 2.0, 3.0];\n\n        // Act\n        let result = process(&amp;input);\n\n        // Assert\n        assert_eq!(result.len(), 3);\n    }\n}\n</code></pre>"},{"location":"maintenance/testing/#testing-floating-point","title":"Testing Floating Point","text":"<p>Use <code>approx</code> crate for float comparisons:</p> <pre><code>use approx::assert_relative_eq;\n\n#[test]\nfn test_computation() {\n    let result = compute();\n\n    // Relative tolerance\n    assert_relative_eq!(result, expected, epsilon = 1e-10);\n\n    // Or absolute tolerance\n    assert_relative_eq!(result, expected, max_relative = 1e-8);\n}\n</code></pre>"},{"location":"maintenance/testing/#testing-errors","title":"Testing Errors","text":"<pre><code>#[test]\nfn test_invalid_input_returns_error() {\n    let result = validate(&amp;invalid_input);\n\n    assert!(result.is_err());\n\n    // Check error type\n    match result {\n        Err(RustyStatsError::InvalidInput(_)) =&gt; (),\n        _ =&gt; panic!(\"Expected InvalidInput error\"),\n    }\n}\n</code></pre>"},{"location":"maintenance/testing/#property-based-testing","title":"Property-Based Testing","text":"<p>Test invariants:</p> <pre><code>#[test]\nfn test_deviance_nonnegative() {\n    let family = PoissonFamily;\n\n    // Random test cases\n    for _ in 0..100 {\n        let y = random_positive_array(10);\n        let mu = random_positive_array(10);\n\n        let dev = family.deviance(&amp;y, &amp;mu, None);\n        assert!(dev &gt;= 0.0, \"Deviance must be non-negative\");\n    }\n}\n\n#[test]\nfn test_perfect_fit_zero_deviance() {\n    let family = PoissonFamily;\n\n    for _ in 0..100 {\n        let y = random_positive_array(10);\n        let mu = y.clone();  // Perfect fit\n\n        let dev = family.deviance(&amp;y, &amp;mu, None);\n        assert!(dev &lt; 1e-10, \"Perfect fit should have zero deviance\");\n    }\n}\n</code></pre>"},{"location":"maintenance/testing/#ignoredlong-running-tests","title":"Ignored/Long-Running Tests","text":"<pre><code>#[test]\n#[ignore]  // Skip by default\nfn test_large_dataset() {\n    // Long-running test\n}\n\n// Run with: cargo test -- --ignored\n</code></pre>"},{"location":"maintenance/testing/#writing-python-tests","title":"Writing Python Tests","text":""},{"location":"maintenance/testing/#basic-test-structure_1","title":"Basic Test Structure","text":"<pre><code>import pytest\nimport numpy as np\nimport rustystats as rs\n\nclass TestPoissonFit:\n    \"\"\"Tests for Poisson GLM fitting.\"\"\"\n\n    def test_basic_fit(self):\n        \"\"\"Test basic Poisson fit converges.\"\"\"\n        y = np.array([1, 2, 3, 4, 5])\n        X = np.column_stack([np.ones(5), [1, 2, 3, 4, 5]])\n\n        result = rs.fit_glm(y, X, family=\"poisson\")\n\n        assert result.converged\n        assert len(result.params) == 2\n\n    def test_predictions_positive(self):\n        \"\"\"Test that Poisson predictions are positive.\"\"\"\n        y = np.random.poisson(5, 100)\n        X = np.column_stack([np.ones(100), np.random.randn(100)])\n\n        result = rs.fit_glm(y, X, family=\"poisson\")\n\n        assert np.all(result.fittedvalues &gt; 0)\n</code></pre>"},{"location":"maintenance/testing/#fixtures","title":"Fixtures","text":"<pre><code>import pytest\n\n@pytest.fixture\ndef sample_data():\n    \"\"\"Generate sample Poisson data.\"\"\"\n    np.random.seed(42)\n    n = 100\n    x = np.random.randn(n)\n    eta = 0.5 + 0.3 * x\n    y = np.random.poisson(np.exp(eta))\n    X = np.column_stack([np.ones(n), x])\n    return y, X\n\ndef test_with_fixture(sample_data):\n    y, X = sample_data\n    result = rs.fit_glm(y, X, family=\"poisson\")\n    assert result.converged\n</code></pre>"},{"location":"maintenance/testing/#parametrized-tests","title":"Parametrized Tests","text":"<pre><code>@pytest.mark.parametrize(\"family\", [\n    \"gaussian\", \"poisson\", \"binomial\", \"gamma\"\n])\ndef test_all_families_converge(family):\n    \"\"\"Test that all families can fit.\"\"\"\n    np.random.seed(42)\n    y = np.abs(np.random.randn(50)) + 0.1\n    if family == \"binomial\":\n        y = (y &gt; np.median(y)).astype(float)\n\n    X = np.column_stack([np.ones(50), np.random.randn(50)])\n    result = rs.fit_glm(y, X, family=family)\n\n    assert result.converged\n</code></pre>"},{"location":"maintenance/testing/#comparison-with-statsmodels","title":"Comparison with Statsmodels","text":"<pre><code>def test_vs_statsmodels():\n    \"\"\"Compare results to statsmodels.\"\"\"\n    import statsmodels.api as sm\n\n    np.random.seed(42)\n    y = np.random.poisson(5, 100).astype(float)\n    X = np.column_stack([np.ones(100), np.random.randn(100, 2)])\n\n    # RustyStats\n    rs_result = rs.fit_glm(y, X, family=\"poisson\")\n\n    # Statsmodels\n    sm_result = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n    # Compare\n    np.testing.assert_allclose(\n        rs_result.params, \n        sm_result.params, \n        rtol=1e-5\n    )\n    np.testing.assert_allclose(\n        rs_result.bse(), \n        sm_result.bse, \n        rtol=1e-4\n    )\n</code></pre>"},{"location":"maintenance/testing/#edge-case-tests","title":"Edge Case Tests","text":"<pre><code>class TestEdgeCases:\n\n    def test_single_observation(self):\n        \"\"\"Test behavior with single observation.\"\"\"\n        y = np.array([1.0])\n        X = np.array([[1.0]])\n\n        result = rs.fit_glm(y, X, family=\"gaussian\")\n        assert result.converged\n\n    def test_all_zeros_poisson(self):\n        \"\"\"Test Poisson with all zero response.\"\"\"\n        y = np.zeros(10)\n        X = np.column_stack([np.ones(10), np.random.randn(10)])\n\n        result = rs.fit_glm(y, X, family=\"poisson\")\n        assert result.converged\n        # Predictions should still be positive\n        assert np.all(result.fittedvalues &gt; 0)\n\n    def test_large_values(self):\n        \"\"\"Test numerical stability with large values.\"\"\"\n        y = np.array([1e6, 2e6, 3e6])\n        X = np.column_stack([np.ones(3), [1, 2, 3]])\n\n        result = rs.fit_glm(y, X, family=\"gaussian\")\n        assert result.converged\n        assert not np.any(np.isnan(result.params))\n</code></pre>"},{"location":"maintenance/testing/#test-categories","title":"Test Categories","text":""},{"location":"maintenance/testing/#1-unit-tests","title":"1. Unit Tests","text":"<p>Test individual functions in isolation:</p> <pre><code>#[test]\nfn test_soft_threshold() {\n    assert_eq!(soft_threshold(5.0, 2.0), 3.0);\n    assert_eq!(soft_threshold(-5.0, 2.0), -3.0);\n    assert_eq!(soft_threshold(1.0, 2.0), 0.0);\n}\n</code></pre>"},{"location":"maintenance/testing/#2-integration-tests","title":"2. Integration Tests","text":"<p>Test components working together:</p> <pre><code>def test_formula_to_fit():\n    \"\"\"Test full formula API workflow.\"\"\"\n    data = pl.DataFrame({\n        \"y\": [1, 2, 3, 4, 5],\n        \"x\": [1.0, 2.0, 3.0, 4.0, 5.0],\n        \"cat\": [\"A\", \"B\", \"A\", \"B\", \"A\"],\n    })\n\n    result = rs.glm(\"y ~ x + C(cat)\", data, family=\"gaussian\").fit()\n\n    assert result.converged\n    assert \"x\" in result.feature_names\n</code></pre>"},{"location":"maintenance/testing/#3-regression-tests","title":"3. Regression Tests","text":"<p>Ensure bugs don't recur:</p> <pre><code>def test_issue_123_overflow():\n    \"\"\"Regression test for issue #123 - overflow with large exposure.\"\"\"\n    y = np.array([1, 2, 3])\n    exposure = np.array([1e10, 1e10, 1e10])\n    X = np.column_stack([np.ones(3), [1, 2, 3]])\n\n    # Should not overflow\n    result = rs.fit_glm(y, X, family=\"poisson\", offset=np.log(exposure))\n    assert not np.any(np.isnan(result.params))\n</code></pre>"},{"location":"maintenance/testing/#4-performance-tests","title":"4. Performance Tests","text":"<p>Verify performance characteristics:</p> <pre><code>@pytest.mark.slow\ndef test_large_dataset_performance():\n    \"\"\"Test that large dataset fits in reasonable time.\"\"\"\n    import time\n\n    n = 100000\n    p = 50\n    y = np.random.poisson(5, n)\n    X = np.column_stack([np.ones(n), np.random.randn(n, p)])\n\n    start = time.time()\n    result = rs.fit_glm(y, X, family=\"poisson\")\n    elapsed = time.time() - start\n\n    assert result.converged\n    assert elapsed &lt; 5.0  # Should complete in &lt; 5 seconds\n</code></pre>"},{"location":"maintenance/testing/#test-coverage","title":"Test Coverage","text":""},{"location":"maintenance/testing/#rust-coverage","title":"Rust Coverage","text":"<pre><code># Install cargo-tarpaulin\ncargo install cargo-tarpaulin\n\n# Run with coverage\ncargo tarpaulin -p rustystats-core --out Html\n</code></pre>"},{"location":"maintenance/testing/#python-coverage","title":"Python Coverage","text":"<pre><code># Run with coverage\nuv run pytest tests/python/ --cov=rustystats --cov-report=html\n\n# View report\nopen htmlcov/index.html\n</code></pre>"},{"location":"maintenance/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"maintenance/testing/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Rust\n        uses: actions-rs/toolchain@v1\n        with:\n          toolchain: stable\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install uv\n          uv sync\n\n      - name: Run Rust tests\n        run: cargo test\n\n      - name: Build Python package\n        run: uv run maturin develop\n\n      - name: Run Python tests\n        run: uv run pytest tests/python/ -v\n</code></pre>"},{"location":"maintenance/testing/#best-practices","title":"Best Practices","text":"<ol> <li>Test behavior, not implementation - Focus on what the code does, not how</li> <li>Use descriptive test names - <code>test_poisson_variance_equals_mean</code></li> <li>One assertion per concept - Split complex tests</li> <li>Test edge cases - Empty inputs, single values, extremes</li> <li>Compare to reference - Use statsmodels, R, or scipy</li> <li>Keep tests fast - Mark slow tests with <code>@pytest.mark.slow</code></li> <li>Clean up fixtures - Don't leave state between tests</li> </ol>"},{"location":"reference/glossary/","title":"Glossary","text":"<p>Terms and concepts used throughout RustyStats documentation.</p>"},{"location":"reference/glossary/#a","title":"A","text":""},{"location":"reference/glossary/#aic-akaike-information-criterion","title":"AIC (Akaike Information Criterion)","text":"<p>A measure of model quality balancing fit and complexity: AIC = -2\u00d7loglik + 2p. Lower is better.</p>"},{"location":"reference/glossary/#alpha","title":"Alpha (\u03b1)","text":"<p>Regularization strength parameter. Higher \u03b1 means more penalty.</p>"},{"location":"reference/glossary/#b","title":"B","text":""},{"location":"reference/glossary/#bic-bayesian-information-criterion","title":"BIC (Bayesian Information Criterion)","text":"<p>Similar to AIC but with stronger penalty for complexity: BIC = -2\u00d7loglik + p\u00d7log(n).</p>"},{"location":"reference/glossary/#b-spline","title":"B-spline","text":"<p>Piecewise polynomial basis functions defined by knots. See Splines.</p>"},{"location":"reference/glossary/#binomial-family","title":"Binomial Family","text":"<p>Distribution for binary (0/1) or proportion data. Variance: V(\u03bc) = \u03bc(1-\u03bc).</p>"},{"location":"reference/glossary/#c","title":"C","text":""},{"location":"reference/glossary/#canonical-link","title":"Canonical Link","text":"<p>The \"natural\" link function for a family, derived from the exponential family form.</p>"},{"location":"reference/glossary/#coordinate-descent","title":"Coordinate Descent","text":"<p>Optimization algorithm that updates one variable at a time. Used for regularized GLMs.</p>"},{"location":"reference/glossary/#cross-validation-cv","title":"Cross-Validation (CV)","text":"<p>Technique for model selection by training on subsets and testing on holdout data.</p>"},{"location":"reference/glossary/#d","title":"D","text":""},{"location":"reference/glossary/#design-matrix","title":"Design Matrix","text":"<p>The matrix X containing predictor values. Each row is an observation, each column a feature.</p>"},{"location":"reference/glossary/#deviance","title":"Deviance","text":"<p>A measure of model fit: D = 2\u00d7[\u2113(saturated) - \u2113(fitted)]. Lower is better.</p>"},{"location":"reference/glossary/#dispersion","title":"Dispersion (\u03c6)","text":"<p>Scale parameter relating variance to the variance function: Var(Y) = \u03c6\u00d7V(\u03bc).</p>"},{"location":"reference/glossary/#dummy-coding","title":"Dummy Coding","text":"<p>Representing a categorical variable with k levels using k-1 binary columns.</p>"},{"location":"reference/glossary/#e","title":"E","text":""},{"location":"reference/glossary/#elastic-net","title":"Elastic Net","text":"<p>Regularization combining L1 and L2 penalties: \u03b1\u00d7[\u03c1\u00d7\u2016\u03b2\u2016\u2081 + (1-\u03c1)/2\u00d7\u2016\u03b2\u2016\u2082\u00b2].</p>"},{"location":"reference/glossary/#exponential-family","title":"Exponential Family","text":"<p>Class of probability distributions with specific form, including Gaussian, Poisson, Binomial.</p>"},{"location":"reference/glossary/#f","title":"F","text":""},{"location":"reference/glossary/#family","title":"Family","text":"<p>The distribution of the response variable in a GLM. See Families.</p>"},{"location":"reference/glossary/#fisher-scoring","title":"Fisher Scoring","text":"<p>Variant of Newton-Raphson using expected Hessian. Equivalent to IRLS for GLMs.</p>"},{"location":"reference/glossary/#fitted-values","title":"Fitted Values","text":"<p>Predicted means \u03bc\u0302 = g\u207b\u00b9(X\u03b2\u0302).</p>"},{"location":"reference/glossary/#g","title":"G","text":""},{"location":"reference/glossary/#gamma-family","title":"Gamma Family","text":"<p>Distribution for positive continuous data. Variance: V(\u03bc) = \u03bc\u00b2.</p>"},{"location":"reference/glossary/#gaussian-family","title":"Gaussian Family","text":"<p>Normal distribution. Variance: V(\u03bc) = 1 (constant).</p>"},{"location":"reference/glossary/#gini-coefficient","title":"Gini Coefficient","text":"<p>Measure of model discrimination: Gini = 2\u00d7AUC - 1. Range [0, 1].</p>"},{"location":"reference/glossary/#glm-generalized-linear-model","title":"GLM (Generalized Linear Model)","text":"<p>Extension of linear regression allowing different response distributions and link functions.</p>"},{"location":"reference/glossary/#h","title":"H","text":""},{"location":"reference/glossary/#hc0-hc1-hc2-hc3","title":"HC0, HC1, HC2, HC3","text":"<p>Heteroscedasticity-consistent standard error estimators. See Robust SE.</p>"},{"location":"reference/glossary/#hosmer-lemeshow-test","title":"Hosmer-Lemeshow Test","text":"<p>Statistical test for model calibration.</p>"},{"location":"reference/glossary/#i","title":"I","text":""},{"location":"reference/glossary/#identity-link","title":"Identity Link","text":"<p>Link function g(\u03bc) = \u03bc. Used with Gaussian family.</p>"},{"location":"reference/glossary/#intercept","title":"Intercept","text":"<p>The constant term \u03b2\u2080 in the linear predictor.</p>"},{"location":"reference/glossary/#irls-iteratively-reweighted-least-squares","title":"IRLS (Iteratively Reweighted Least Squares)","text":"<p>Algorithm for fitting GLMs. See IRLS.</p>"},{"location":"reference/glossary/#l","title":"L","text":""},{"location":"reference/glossary/#l1-ratio","title":"L1 Ratio","text":"<p>Parameter controlling Elastic Net mix: 1.0 = Lasso, 0.0 = Ridge.</p>"},{"location":"reference/glossary/#lasso","title":"Lasso","text":"<p>L1-penalized regression that performs variable selection: min D(\u03b2) + \u03b1\u2016\u03b2\u2016\u2081.</p>"},{"location":"reference/glossary/#linear-predictor","title":"Linear Predictor (\u03b7)","text":"<p>The linear combination \u03b7 = X\u03b2 + offset.</p>"},{"location":"reference/glossary/#link-function","title":"Link Function","text":"<p>Function g(\u03bc) connecting mean to linear predictor. See Links.</p>"},{"location":"reference/glossary/#log-link","title":"Log Link","text":"<p>Link function g(\u03bc) = log(\u03bc). Ensures positive predictions.</p>"},{"location":"reference/glossary/#logit-link","title":"Logit Link","text":"<p>Link function g(\u03bc) = log(\u03bc/(1-\u03bc)). Maps (0,1) to \u211d.</p>"},{"location":"reference/glossary/#log-likelihood","title":"Log-Likelihood","text":"<p>The log of the probability of observing the data given the model.</p>"},{"location":"reference/glossary/#m","title":"M","text":""},{"location":"reference/glossary/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<p>Finding parameters that maximize the likelihood of the data.</p>"},{"location":"reference/glossary/#n","title":"N","text":""},{"location":"reference/glossary/#natural-spline","title":"Natural Spline","text":"<p>B-spline with additional constraints for linear extrapolation at boundaries.</p>"},{"location":"reference/glossary/#negative-binomial","title":"Negative Binomial","text":"<p>Distribution for overdispersed counts. Variance: V(\u03bc) = \u03bc + \u03bc\u00b2/\u03b8.</p>"},{"location":"reference/glossary/#newton-raphson","title":"Newton-Raphson","text":"<p>Iterative optimization algorithm using gradient and Hessian.</p>"},{"location":"reference/glossary/#null-deviance","title":"Null Deviance","text":"<p>Deviance of the intercept-only model.</p>"},{"location":"reference/glossary/#o","title":"O","text":""},{"location":"reference/glossary/#odds-ratio","title":"Odds Ratio","text":"<p>exp(\u03b2) in logistic regression. Multiplicative effect on odds.</p>"},{"location":"reference/glossary/#offset","title":"Offset","text":"<p>Known component added to linear predictor but not estimated. Example: log(exposure).</p>"},{"location":"reference/glossary/#overdispersion","title":"Overdispersion","text":"<p>When observed variance exceeds model-predicted variance.</p>"},{"location":"reference/glossary/#p","title":"P","text":""},{"location":"reference/glossary/#p-value","title":"P-value","text":"<p>Probability of observing test statistic as extreme as computed, under null hypothesis.</p>"},{"location":"reference/glossary/#pearson-residual","title":"Pearson Residual","text":"<p>Standardized residual: (y - \u03bc) / \u221aV(\u03bc).</p>"},{"location":"reference/glossary/#poisson-family","title":"Poisson Family","text":"<p>Distribution for count data. Variance: V(\u03bc) = \u03bc.</p>"},{"location":"reference/glossary/#prior-weight","title":"Prior Weight","text":"<p>Known weight for each observation, used in likelihood.</p>"},{"location":"reference/glossary/#probit","title":"Probit","text":"<p>Link function using inverse normal CDF: g(\u03bc) = \u03a6\u207b\u00b9(\u03bc).</p>"},{"location":"reference/glossary/#pyo3","title":"PyO3","text":"<p>Rust library for Python bindings.</p>"},{"location":"reference/glossary/#q","title":"Q","text":""},{"location":"reference/glossary/#quasipoisson-quasibinomial","title":"QuasiPoisson / QuasiBinomial","text":"<p>Quasi-likelihood families that estimate dispersion from data.</p>"},{"location":"reference/glossary/#r","title":"R","text":""},{"location":"reference/glossary/#rayon","title":"Rayon","text":"<p>Rust library for data parallelism.</p>"},{"location":"reference/glossary/#reference-level","title":"Reference Level","text":"<p>The omitted category in dummy coding (absorbed into intercept).</p>"},{"location":"reference/glossary/#regularization","title":"Regularization","text":"<p>Adding penalty to prevent overfitting. See Regularization.</p>"},{"location":"reference/glossary/#relativity","title":"Relativity","text":"<p>exp(\u03b2) for log-link models. Multiplicative effect on mean.</p>"},{"location":"reference/glossary/#residual","title":"Residual","text":"<p>Difference between observed and predicted: various types exist.</p>"},{"location":"reference/glossary/#ridge","title":"Ridge","text":"<p>L2-penalized regression: min D(\u03b2) + \u03b1\u2016\u03b2\u2016\u2082\u00b2.</p>"},{"location":"reference/glossary/#robust-standard-errors","title":"Robust Standard Errors","text":"<p>Standard errors that account for model misspecification.</p>"},{"location":"reference/glossary/#s","title":"S","text":""},{"location":"reference/glossary/#sandwich-estimator","title":"Sandwich Estimator","text":"<p>Robust covariance estimator: (X'WX)\u207b\u00b9 \u00d7 Meat \u00d7 (X'WX)\u207b\u00b9.</p>"},{"location":"reference/glossary/#score-function","title":"Score Function","text":"<p>Derivative of log-likelihood with respect to parameters.</p>"},{"location":"reference/glossary/#soft-thresholding","title":"Soft Thresholding","text":"<p>Operation used in Lasso: S(z,\u03b3) = sign(z)\u00d7max(|z|-\u03b3, 0).</p>"},{"location":"reference/glossary/#spline","title":"Spline","text":"<p>Piecewise polynomial function. See Splines.</p>"},{"location":"reference/glossary/#standard-error-se","title":"Standard Error (SE)","text":"<p>Estimated standard deviation of a parameter estimate.</p>"},{"location":"reference/glossary/#t","title":"T","text":""},{"location":"reference/glossary/#target-encoding","title":"Target Encoding","text":"<p>Replacing categories with target-based statistics. See Target Encoding.</p>"},{"location":"reference/glossary/#theta","title":"Theta (\u03b8)","text":"<p>Negative Binomial dispersion parameter. Larger \u03b8 = less overdispersion.</p>"},{"location":"reference/glossary/#tweedie","title":"Tweedie","text":"<p>Family with variance V(\u03bc) = \u03bc\u1d56. Useful for insurance pure premiums.</p>"},{"location":"reference/glossary/#u","title":"U","text":""},{"location":"reference/glossary/#unit-deviance","title":"Unit Deviance","text":"<p>Per-observation contribution to total deviance.</p>"},{"location":"reference/glossary/#v","title":"V","text":""},{"location":"reference/glossary/#variance-function","title":"Variance Function","text":"<p>V(\u03bc) defining how variance relates to mean: Var(Y) = \u03c6\u00d7V(\u03bc).</p>"},{"location":"reference/glossary/#variance-power-p","title":"Variance Power (p)","text":"<p>Parameter in Tweedie family: V(\u03bc) = \u03bc\u1d56.</p>"},{"location":"reference/glossary/#w","title":"W","text":""},{"location":"reference/glossary/#wald-test","title":"Wald Test","text":"<p>Hypothesis test using (\u03b2\u0302/SE)\u00b2 as test statistic.</p>"},{"location":"reference/glossary/#weights","title":"Weights","text":"<p>Prior observation weights or IRLS working weights.</p>"},{"location":"reference/glossary/#working-response","title":"Working Response","text":"<p>Linearized response in IRLS: z = \u03b7 + (y - \u03bc)\u00d7g'(\u03bc).</p>"},{"location":"reference/glossary/#working-weights","title":"Working Weights","text":"<p>IRLS weights: W = 1/[V(\u03bc)\u00d7g'(\u03bc)\u00b2].</p>"},{"location":"reference/glossary/#z","title":"Z","text":""},{"location":"reference/glossary/#z-statistic","title":"z-statistic","text":"<p>Test statistic: z = \u03b2\u0302 / SE(\u03b2\u0302). Used for hypothesis testing.</p>"},{"location":"reference/notation/","title":"Mathematical Notation","text":"<p>This page defines the mathematical notation used throughout the documentation.</p>"},{"location":"reference/notation/#variables-and-data","title":"Variables and Data","text":"Symbol Description \\(n\\) Number of observations \\(p\\) Number of parameters (including intercept) \\(Y\\), \\(y\\) Response variable (random/observed) \\(y_i\\) Response for observation \\(i\\) \\(\\mathbf{y}\\) Response vector \\((y_1, \\ldots, y_n)^T\\) \\(X\\), \\(\\mathbf{X}\\) Design matrix (\\(n \\times p\\)) \\(\\mathbf{x}_i\\) Row vector of predictors for observation \\(i\\) \\(x_{ij}\\) Value of predictor \\(j\\) for observation \\(i\\)"},{"location":"reference/notation/#parameters","title":"Parameters","text":"Symbol Description \\(\\boldsymbol{\\beta}\\) Coefficient vector \\((\\beta_0, \\beta_1, \\ldots, \\beta_{p-1})^T\\) \\(\\beta_0\\) Intercept \\(\\beta_j\\) Coefficient for predictor \\(j\\) \\(\\hat{\\boldsymbol{\\beta}}\\) Estimated coefficients \\(\\phi\\) Dispersion parameter \\(\\hat{\\phi}\\) Estimated dispersion"},{"location":"reference/notation/#glm-components","title":"GLM Components","text":""},{"location":"reference/notation/#linear-predictor","title":"Linear Predictor","text":"Symbol Description \\(\\eta\\) Linear predictor \\(\\eta_i\\) Linear predictor for observation \\(i\\): \\(\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}\\) \\(\\boldsymbol{\\eta}\\) Vector of linear predictors"},{"location":"reference/notation/#mean-and-link","title":"Mean and Link","text":"Symbol Description \\(\\mu\\) Mean: \\(E(Y)\\) \\(\\mu_i\\) Mean for observation \\(i\\): \\(\\mu_i = E(Y_i)\\) \\(\\hat{\\mu}_i\\) Fitted mean \\(g(\\cdot)\\) Link function \\(g^{-1}(\\cdot)\\) Inverse link function (mean function) \\(g'(\\mu)\\) Derivative of link function <p>The relationship: [ \\eta = g(\\mu) \\quad \\Leftrightarrow \\quad \\mu = g^{-1}(\\eta) ]</p>"},{"location":"reference/notation/#variance-function","title":"Variance Function","text":"Symbol Description \\(V(\\mu)\\) Variance function \\(\\text{Var}(Y)\\) Variance of response: \\(\\text{Var}(Y) = \\phi \\cdot V(\\mu)\\)"},{"location":"reference/notation/#matrices-and-vectors","title":"Matrices and Vectors","text":"Symbol Description \\(\\mathbf{I}\\) Identity matrix \\(\\mathbf{W}\\) Diagonal weight matrix \\(W_{ii}\\) Weight for observation \\(i\\) \\(\\mathbf{z}\\) Working response vector \\((\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}\\) Unscaled covariance matrix"},{"location":"reference/notation/#likelihood-and-deviance","title":"Likelihood and Deviance","text":"Symbol Description \\(L\\) Likelihood \\(\\ell\\) Log-likelihood: \\(\\ell = \\log L\\) \\(D\\) Deviance \\(d_i\\) Unit deviance for observation \\(i\\) \\(D_{\\text{null}}\\) Null deviance (intercept-only model) <p>Deviance definition: [ D = 2[\\ell(\\text{saturated}) - \\ell(\\text{fitted})] = \\sum_{i=1}^n d_i ]</p>"},{"location":"reference/notation/#regularization","title":"Regularization","text":"Symbol Description \\(\\alpha\\) Overall penalty strength \\(\\rho\\) L1 ratio (1 = Lasso, 0 = Ridge) \\(\\lambda\\) Alternative notation for penalty strength \\(\\|\\boldsymbol{\\beta}\\|_1\\) L1 norm: (\\sum_j \\(\\|\\boldsymbol{\\beta}\\|_2^2\\) Squared L2 norm: \\(\\sum_j \\beta_j^2\\) <p>Elastic Net penalty: [ P(\\boldsymbol{\\beta}) = \\alpha \\left[ \\rho |\\boldsymbol{\\beta}|_1 + \\frac{1-\\rho}{2} |\\boldsymbol{\\beta}|_2^2 \\right] ]</p>"},{"location":"reference/notation/#inference","title":"Inference","text":"Symbol Description \\(\\text{SE}(\\hat{\\beta}_j)\\) Standard error of \\(\\hat{\\beta}_j\\) \\(z\\) z-statistic: \\(z = \\hat{\\beta} / \\text{SE}(\\hat{\\beta})\\) \\(\\alpha\\) Significance level (context-dependent) \\(\\text{CI}\\) Confidence interval <p>Standard error: [ \\text{SE}(\\hat{\\beta}j) = \\sqrt{\\hat{\\phi} \\cdot [(\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}] ]}</p>"},{"location":"reference/notation/#residuals","title":"Residuals","text":"Symbol Description \\(r_i^{\\text{response}}\\) Response residual: \\(y_i - \\hat{\\mu}_i\\) \\(r_i^{\\text{Pearson}}\\) Pearson residual: \\(\\frac{y_i - \\hat{\\mu}_i}{\\sqrt{V(\\hat{\\mu}_i)}}\\) \\(r_i^{\\text{deviance}}\\) Deviance residual: \\(\\text{sign}(y_i - \\hat{\\mu}_i)\\sqrt{d_i}\\) \\(r_i^{\\text{working}}\\) Working residual: \\((y_i - \\hat{\\mu}_i) \\cdot g'(\\hat{\\mu}_i)\\)"},{"location":"reference/notation/#information-criteria","title":"Information Criteria","text":"Symbol Formula AIC \\(-2\\ell + 2p\\) BIC \\(-2\\ell + p \\log(n)\\)"},{"location":"reference/notation/#common-link-functions","title":"Common Link Functions","text":"Name \\(g(\\mu)\\) \\(g^{-1}(\\eta)\\) \\(g'(\\mu)\\) Identity \\(\\mu\\) \\(\\eta\\) \\(1\\) Log \\(\\log(\\mu)\\) \\(e^\\eta\\) \\(1/\\mu\\) Logit \\(\\log\\frac{\\mu}{1-\\mu}\\) \\(\\frac{e^\\eta}{1+e^\\eta}\\) \\(\\frac{1}{\\mu(1-\\mu)}\\) Probit \\(\\Phi^{-1}(\\mu)\\) \\(\\Phi(\\eta)\\) \\(\\frac{1}{\\phi(\\Phi^{-1}(\\mu))}\\)"},{"location":"reference/notation/#common-variance-functions","title":"Common Variance Functions","text":"Family \\(V(\\mu)\\) Gaussian \\(1\\) Poisson \\(\\mu\\) Binomial \\(\\mu(1-\\mu)\\) Gamma \\(\\mu^2\\) Tweedie \\(\\mu^p\\) Negative Binomial \\(\\mu + \\mu^2/\\theta\\)"},{"location":"reference/notation/#irls-algorithm","title":"IRLS Algorithm","text":"<p>Working weights: [ W_{ii} = \\frac{1}{V(\\mu_i) \\cdot [g'(\\mu_i)]^2} ]</p> <p>Working response: [ z_i = \\eta_i + (y_i - \\mu_i) \\cdot g'(\\mu_i) ]</p> <p>Update step: [ \\boldsymbol{\\beta}^{(t+1)} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{z} ]</p>"},{"location":"reference/notation/#splines","title":"Splines","text":"Symbol Description \\(B_{i,k}(x)\\) B-spline basis function of degree \\(k\\) \\(t_0, t_1, \\ldots\\) Knot sequence df Degrees of freedom (number of basis functions) <p>Cox-de Boor recursion: [ B_{i,k}(x) = \\frac{x - t_i}{t_{i+k} - t_i} B_{i,k-1}(x) + \\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} B_{i+1,k-1}(x) ]</p>"},{"location":"reference/notation/#probability-distributions","title":"Probability Distributions","text":"Distribution Notation Parameters Normal \\(N(\\mu, \\sigma^2)\\) mean \\(\\mu\\), variance \\(\\sigma^2\\) Poisson \\(\\text{Pois}(\\lambda)\\) rate \\(\\lambda\\) Binomial \\(\\text{Bin}(n, p)\\) trials \\(n\\), probability \\(p\\) Gamma \\(\\text{Gamma}(\\alpha, \\beta)\\) shape \\(\\alpha\\), rate \\(\\beta\\) Negative Binomial \\(\\text{NB}(\\mu, \\theta)\\) mean \\(\\mu\\), dispersion \\(\\theta\\)"},{"location":"rust-guide/code-walkthrough/","title":"RustyStats Code Walkthrough","text":"<p>This guide walks through the actual RustyStats source code, explaining every Rust syntax element. Each section takes a real function from the codebase and annotates it line-by-line.</p> <p>Goal: After reading this, you should be able to read and modify any function in the codebase.</p>"},{"location":"rust-guide/code-walkthrough/#part-1-understanding-rust-syntax-basics","title":"Part 1: Understanding Rust Syntax Basics","text":"<p>Before diving into the code, here's a quick reference for Rust syntax you'll encounter:</p> Syntax Meaning Example <code>fn name() -&gt; T</code> Function returning type <code>T</code> <code>fn variance() -&gt; Array1&lt;f64&gt;</code> <code>&amp;</code> Immutable reference (borrow) <code>&amp;self</code>, <code>&amp;Array1&lt;f64&gt;</code> <code>&amp;mut</code> Mutable reference <code>&amp;mut self</code> <code>pub</code> Public (visible outside module) <code>pub fn fit_glm()</code> <code>let</code> Variable binding (immutable) <code>let x = 5;</code> <code>let mut</code> Mutable variable <code>let mut count = 0;</code> <code>::</code> Path separator <code>Array1::zeros(n)</code> <code>&lt;T&gt;</code> Generic type parameter <code>Vec&lt;f64&gt;</code> <code>impl</code> Implementation block <code>impl Family for PoissonFamily</code> <code>self</code> Current instance <code>self.name()</code> <code>?</code> Propagate error if Result is Err <code>result?</code> <code>\\|x\\| expr</code> Closure (anonymous function) <code>\\|x\\| x * 2</code> <code>.iter()</code> Create iterator <code>vec.iter()</code> <code>.map()</code> Transform each element <code>.map(\\|x\\| x + 1)</code> <code>.collect()</code> Gather iterator into collection <code>.collect::&lt;Vec&lt;_&gt;&gt;()</code>"},{"location":"rust-guide/code-walkthrough/#part-2-the-family-trait","title":"Part 2: The Family Trait","text":"<p>File: <code>crates/rustystats-core/src/families/mod.rs</code></p> <p>A trait defines shared behavior. Every distribution family implements this trait.</p> <pre><code>/// The Family trait defines the interface for all distribution families.\npub trait Family: Send + Sync {\n</code></pre> <p>Line-by-line breakdown:</p> Code Explanation <code>///</code> Documentation comment (appears in generated docs) <code>pub</code> This trait is public (can be used outside this module) <code>trait Family</code> Declares a trait named <code>Family</code> <code>: Send + Sync</code> Trait bounds: Types implementing <code>Family</code> must also implement <code>Send</code> and <code>Sync</code> (required for thread safety with Rayon) <code>{</code> Start of trait definition"},{"location":"rust-guide/code-walkthrough/#trait-methods","title":"Trait Methods","text":"<pre><code>    /// Returns the name of this family.\n    fn name(&amp;self) -&gt; &amp;str;\n</code></pre> Code Explanation <code>fn name</code> Method named <code>name</code> <code>(&amp;self)</code> Takes an immutable reference to self (the instance) <code>-&gt; &amp;str</code> Returns a string slice (borrowed string) <code>;</code> No body\u2014this is a required method that implementors must define <pre><code>    /// Compute the variance function V(\u03bc).\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n</code></pre> Code Explanation <code>mu: &amp;Array1&lt;f64&gt;</code> Parameter <code>mu</code> is a reference to a 1D array of <code>f64</code> (64-bit floats) <code>-&gt; Array1&lt;f64&gt;</code> Returns an owned array (not a reference) <pre><code>    /// Compute the total deviance with optional weights.\n    fn deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;, weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;) -&gt; f64 {\n        let unit_dev = self.unit_deviance(y, mu);\n        match weights {\n            Some(w) =&gt; (&amp;unit_dev * w).sum(),\n            None =&gt; unit_dev.sum(),\n        }\n    }\n</code></pre> Code Explanation <code>weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;</code> <code>Option</code> type: either <code>Some(value)</code> or <code>None</code>. This makes the parameter optional. <code>-&gt; f64</code> Returns a single float <code>{ ... }</code> Has a body\u2014this is a default implementation (can be overridden) <code>let unit_dev = ...</code> Bind result to immutable variable <code>self.unit_deviance(y, mu)</code> Call another method on self <code>match weights { ... }</code> Pattern matching on the Option <code>Some(w) =&gt;</code> If weights is Some, bind the inner value to <code>w</code> <code>(&amp;unit_dev * w).sum()</code> Element-wise multiply arrays, then sum <code>None =&gt;</code> If weights is None <code>unit_dev.sum()</code> Just sum without weighting"},{"location":"rust-guide/code-walkthrough/#part-3-implementing-a-family-poissonfamily","title":"Part 3: Implementing a Family (PoissonFamily)","text":"<p>File: <code>crates/rustystats-core/src/families/poisson.rs</code></p>"},{"location":"rust-guide/code-walkthrough/#the-struct-definition","title":"The Struct Definition","text":"<pre><code>use ndarray::Array1;\nuse crate::links::{Link, LogLink};\nuse super::Family;\n\n#[derive(Debug, Clone, Copy)]\npub struct PoissonFamily;\n</code></pre> Code Explanation <code>use ndarray::Array1</code> Import <code>Array1</code> from the <code>ndarray</code> crate <code>use crate::links::{Link, LogLink}</code> Import from our own crate's <code>links</code> module <code>use super::Family</code> Import <code>Family</code> from parent module (<code>super</code> = parent) <code>#[derive(...)]</code> Derive macro: auto-generate trait implementations <code>Debug</code> Enables <code>{:?}</code> formatting for debugging <code>Clone</code> Enables <code>.clone()</code> method <code>Copy</code> Type can be copied implicitly (no move semantics) <code>pub struct PoissonFamily;</code> Empty struct (no fields)\u2014a \"unit struct\""},{"location":"rust-guide/code-walkthrough/#the-implementation-block","title":"The Implementation Block","text":"<pre><code>impl Family for PoissonFamily {\n    fn name(&amp;self) -&gt; &amp;str {\n        \"Poisson\"\n    }\n</code></pre> Code Explanation <code>impl Family for PoissonFamily</code> \"Implement the Family trait for PoissonFamily\" <code>fn name(&amp;self) -&gt; &amp;str</code> Must match trait signature exactly <code>\"Poisson\"</code> String literal; last expression without <code>;</code> is the return value"},{"location":"rust-guide/code-walkthrough/#the-variance-function","title":"The Variance Function","text":"<pre><code>    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.clone()\n    }\n</code></pre> Code Explanation <code>mu.clone()</code> Create a copy of the array. For Poisson, V(\u03bc) = \u03bc, so we return a copy of mu."},{"location":"rust-guide/code-walkthrough/#the-unit-deviance-function","title":"The Unit Deviance Function","text":"<pre><code>    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        ndarray::Zip::from(y)\n            .and(mu)\n            .map_collect(|&amp;yi, &amp;mui| {\n                if yi == 0.0 {\n                    2.0 * mui\n                } else {\n                    2.0 * (yi * (yi / mui).ln() - (yi - mui))\n                }\n            })\n    }\n</code></pre> Code Explanation <code>ndarray::Zip::from(y)</code> Create a parallel zipper starting with array <code>y</code> <code>.and(mu)</code> Add array <code>mu</code> to the zip <code>.map_collect(\\|&amp;yi, &amp;mui\\| { ... })</code> Apply closure to each pair, collect into new array <code>\\|&amp;yi, &amp;mui\\|</code> Closure parameters. <code>&amp;</code> pattern-matches the reference to get the value. <code>if yi == 0.0 { ... } else { ... }</code> Conditional expression (returns a value) <code>(yi / mui).ln()</code> Divide, then natural log. Method chaining."},{"location":"rust-guide/code-walkthrough/#returning-a-boxed-trait-object","title":"Returning a Boxed Trait Object","text":"<pre><code>    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)\n    }\n</code></pre> Code Explanation <code>Box&lt;dyn Link&gt;</code> A heap-allocated trait object. <code>dyn Link</code> means \"some type implementing Link\" <code>Box::new(LogLink)</code> Allocate <code>LogLink</code> on the heap and return a pointer <p>Why Box? The trait method must return a consistent type, but different families return different link types. <code>Box&lt;dyn Link&gt;</code> erases the concrete type, allowing runtime polymorphism.</p>"},{"location":"rust-guide/code-walkthrough/#array-mapping","title":"Array Mapping","text":"<pre><code>    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.mapv(|yi| (yi + 0.1).max(0.1))\n    }\n</code></pre> Code Explanation <code>y.mapv(\\|yi\\| ...)</code> Map over values. <code>mapv</code> = map values (as opposed to <code>map</code> which maps references) <code>(yi + 0.1).max(0.1)</code> Add 0.1, then take max with 0.1. Ensures result \u2265 0.1"},{"location":"rust-guide/code-walkthrough/#iterator-with-all","title":"Iterator with All","text":"<pre><code>    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;x| x &gt; 0.0 &amp;&amp; x.is_finite())\n    }\n</code></pre> Code Explanation <code>mu.iter()</code> Create an iterator over the array <code>.all(\\|&amp;x\\| ...)</code> Returns <code>true</code> if predicate is true for ALL elements <code>x &gt; 0.0 &amp;&amp; x.is_finite()</code> Check positive AND finite (not NaN or infinity)"},{"location":"rust-guide/code-walkthrough/#part-4-the-link-trait-and-loglink","title":"Part 4: The Link Trait and LogLink","text":"<p>File: <code>crates/rustystats-core/src/links/log.rs</code></p> <pre><code>use ndarray::Array1;\nuse super::Link;\n\n#[derive(Debug, Clone, Copy)]\npub struct LogLink;\n\nimpl Link for LogLink {\n    fn name(&amp;self) -&gt; &amp;str {\n        \"log\"\n    }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|x| x.ln())\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        eta.mapv(|x| x.exp())\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|x| 1.0 / x)\n    }\n}\n</code></pre> Code Explanation <code>x.ln()</code> Natural logarithm (method on <code>f64</code>) <code>x.exp()</code> Exponential function e^x <code>1.0 / x</code> Division. <code>1.0</code> is a float literal."},{"location":"rust-guide/code-walkthrough/#part-5-error-handling","title":"Part 5: Error Handling","text":"<p>File: <code>crates/rustystats-core/src/error.rs</code></p>"},{"location":"rust-guide/code-walkthrough/#defining-error-types","title":"Defining Error Types","text":"<pre><code>use thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum RustyStatsError {\n    #[error(\"Dimension mismatch: {0}\")]\n    DimensionMismatch(String),\n\n    #[error(\"Invalid value: {0}\")]\n    InvalidValue(String),\n\n    #[error(\"Convergence failed: {0}\")]\n    ConvergenceFailure(String),\n}\n</code></pre> Code Explanation <code>use thiserror::Error</code> Import the <code>Error</code> derive macro from <code>thiserror</code> crate <code>#[derive(Error, Debug)]</code> Auto-implement <code>std::error::Error</code> and <code>Debug</code> traits <code>pub enum RustyStatsError</code> Define an enum (sum type) for all error variants <code>#[error(\"...\")]</code> Attribute macro: defines the error message <code>{0}</code> Placeholder for first field in the variant <code>DimensionMismatch(String)</code> Variant with one <code>String</code> field"},{"location":"rust-guide/code-walkthrough/#the-result-type-alias","title":"The Result Type Alias","text":"<pre><code>pub type Result&lt;T&gt; = std::result::Result&lt;T, RustyStatsError&gt;;\n</code></pre> Code Explanation <code>pub type Result&lt;T&gt;</code> Define a public type alias with generic parameter <code>T</code> <code>= std::result::Result&lt;T, RustyStatsError&gt;</code> Alias for <code>Result</code> with our error type <p>Now instead of writing <code>Result&lt;IRLSResult, RustyStatsError&gt;</code>, we write <code>Result&lt;IRLSResult&gt;</code>.</p>"},{"location":"rust-guide/code-walkthrough/#part-6-the-irls-solver","title":"Part 6: The IRLS Solver","text":"<p>File: <code>crates/rustystats-core/src/solvers/irls.rs</code></p>"},{"location":"rust-guide/code-walkthrough/#configuration-struct","title":"Configuration Struct","text":"<pre><code>#[derive(Debug, Clone)]\npub struct IRLSConfig {\n    pub max_iterations: usize,\n    pub tolerance: f64,\n    pub min_weight: f64,\n    pub verbose: bool,\n}\n</code></pre> Code Explanation <code>#[derive(Debug, Clone)]</code> Auto-implement Debug and Clone <code>pub struct IRLSConfig</code> Public struct <code>pub max_iterations: usize</code> Public field of type <code>usize</code> (unsigned integer, pointer-sized) <code>pub tolerance: f64</code> 64-bit floating point <code>pub verbose: bool</code> Boolean"},{"location":"rust-guide/code-walkthrough/#default-implementation","title":"Default Implementation","text":"<pre><code>impl Default for IRLSConfig {\n    fn default() -&gt; Self {\n        Self {\n            max_iterations: 25,\n            tolerance: 1e-8,\n            min_weight: 1e-10,\n            verbose: false,\n        }\n    }\n}\n</code></pre> Code Explanation <code>impl Default for IRLSConfig</code> Implement the <code>Default</code> trait <code>fn default() -&gt; Self</code> Returns an instance of Self (IRLSConfig) <code>Self { ... }</code> Struct literal. <code>Self</code> is an alias for the implementing type. <code>1e-8</code> Scientific notation: 1 \u00d7 10\u207b\u2078"},{"location":"rust-guide/code-walkthrough/#the-main-fitting-function","title":"The Main Fitting Function","text":"<pre><code>pub fn fit_glm(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,\n    link: &amp;dyn Link,\n    config: &amp;IRLSConfig,\n) -&gt; Result&lt;IRLSResult&gt; {\n    fit_glm_full(y, x, family, link, config, None, None)\n}\n</code></pre> Code Explanation <code>pub fn fit_glm(...)</code> Public function <code>y: &amp;Array1&lt;f64&gt;</code> Borrow a 1D array <code>x: &amp;Array2&lt;f64&gt;</code> Borrow a 2D array (matrix) <code>family: &amp;dyn Family</code> Borrow a trait object (any type implementing Family) <code>-&gt; Result&lt;IRLSResult&gt;</code> Returns <code>Ok(IRLSResult)</code> on success, <code>Err(...)</code> on failure <code>None, None</code> Pass <code>None</code> for optional parameters"},{"location":"rust-guide/code-walkthrough/#input-validation","title":"Input Validation","text":"<pre><code>pub fn fit_glm_full(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,\n    link: &amp;dyn Link,\n    config: &amp;IRLSConfig,\n    offset: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; Result&lt;IRLSResult&gt; {\n    let n = y.len();\n    let p = x.ncols();\n\n    if x.nrows() != n {\n        return Err(RustyStatsError::DimensionMismatch(format!(\n            \"X has {} rows but y has {} elements\",\n            x.nrows(),\n            n\n        )));\n    }\n</code></pre> Code Explanation <code>offset: Option&lt;&amp;Array1&lt;f64&gt;&gt;</code> Optional borrowed array <code>let n = y.len()</code> Get array length <code>let p = x.ncols()</code> Get number of columns <code>x.nrows()</code> Get number of rows <code>return Err(...)</code> Early return with error <code>format!(\"...\", x.nrows(), n)</code> String formatting macro (like Python f-strings)"},{"location":"rust-guide/code-walkthrough/#handling-optional-parameters","title":"Handling Optional Parameters","text":"<pre><code>    let offset_vec = match offset {\n        Some(o) =&gt; {\n            if o.len() != n {\n                return Err(RustyStatsError::DimensionMismatch(...));\n            }\n            o.clone()\n        }\n        None =&gt; Array1::zeros(n),\n    };\n</code></pre> Code Explanation <code>match offset { ... }</code> Pattern match on the Option <code>Some(o) =&gt; { ... }</code> If Some, bind inner value to <code>o</code>, execute block <code>o.clone()</code> Clone the borrowed array to get an owned copy <code>None =&gt; Array1::zeros(n)</code> If None, create zero array of length n"},{"location":"rust-guide/code-walkthrough/#the-irls-loop","title":"The IRLS Loop","text":"<pre><code>    let mut converged = false;\n    let mut iteration = 0;\n\n    while iteration &lt; config.max_iterations {\n        iteration += 1;\n\n        // Compute variance and link derivative\n        let variance = family.variance(&amp;mu);\n        let link_deriv = link.derivative(&amp;mu);\n</code></pre> Code Explanation <code>let mut converged = false</code> Mutable boolean, initially false <code>while condition { ... }</code> While loop <code>iteration += 1</code> Increment (shorthand for <code>iteration = iteration + 1</code>) <code>family.variance(&amp;mu)</code> Call trait method, passing borrow of mu"},{"location":"rust-guide/code-walkthrough/#parallel-iteration-with-rayon","title":"Parallel Iteration with Rayon","text":"<pre><code>        let results: Vec&lt;(f64, f64, f64)&gt; = (0..n)\n            .into_par_iter()\n            .map(|i| {\n                let v = variance[i];\n                let d = link_deriv[i];\n                let iw = (1.0 / (v * d * d)).max(min_weight).min(1e10);\n                let cw = prior_weights_vec[i] * iw;\n                let wr = (eta[i] - offset_vec[i]) + (y[i] - mu[i]) * d;\n                (iw, cw, wr)\n            })\n            .collect();\n</code></pre> Code Explanation <code>Vec&lt;(f64, f64, f64)&gt;</code> Vector of tuples, each containing 3 floats <code>(0..n)</code> Range from 0 to n-1 <code>.into_par_iter()</code> Convert to parallel iterator (Rayon) <code>.map(\\|i\\| { ... })</code> Transform each index in parallel <code>variance[i]</code> Index into array <code>(1.0 / (v * d * d))</code> Arithmetic expression <code>.max(min_weight)</code> Take maximum of value and min_weight <code>.min(1e10)</code> Take minimum (clamp to upper bound) <code>(iw, cw, wr)</code> Return a tuple <code>.collect()</code> Gather parallel results into Vec"},{"location":"rust-guide/code-walkthrough/#building-arrays-from-vectors","title":"Building Arrays from Vectors","text":"<pre><code>        let mut irls_weights_vec = Vec::with_capacity(n);\n        let mut combined_weights_vec = Vec::with_capacity(n);\n        let mut working_response_vec = Vec::with_capacity(n);\n\n        for (iw, cw, wr) in results {\n            irls_weights_vec.push(iw);\n            combined_weights_vec.push(cw);\n            working_response_vec.push(wr);\n        }\n\n        let irls_weights = Array1::from_vec(irls_weights_vec);\n</code></pre> Code Explanation <code>Vec::with_capacity(n)</code> Create Vec with pre-allocated capacity (performance) <code>for (iw, cw, wr) in results</code> Destructure tuple in for loop <code>.push(iw)</code> Append to vector <code>Array1::from_vec(...)</code> Convert Vec to ndarray Array1"},{"location":"rust-guide/code-walkthrough/#matrix-operations","title":"Matrix Operations","text":"<pre><code>        let eta_base = x.dot(&amp;new_coefficients);\n        eta = &amp;eta_base + &amp;offset_vec;\n        mu = link.inverse(&amp;eta);\n</code></pre> Code Explanation <code>x.dot(&amp;new_coefficients)</code> Matrix-vector multiplication <code>&amp;eta_base + &amp;offset_vec</code> Element-wise addition of array references <code>link.inverse(&amp;eta)</code> Apply inverse link function"},{"location":"rust-guide/code-walkthrough/#convergence-check","title":"Convergence Check","text":"<pre><code>        let rel_change = if deviance_old.abs() &gt; 1e-10 {\n            (deviance_old - deviance).abs() / deviance_old.abs()\n        } else {\n            (deviance_old - deviance).abs()\n        };\n\n        if rel_change &lt; config.tolerance {\n            converged = true;\n            break;\n        }\n</code></pre> Code Explanation <code>if ... { ... } else { ... }</code> If-else expression (returns value) <code>.abs()</code> Absolute value <code>break</code> Exit the loop"},{"location":"rust-guide/code-walkthrough/#returning-the-result","title":"Returning the Result","text":"<pre><code>    Ok(IRLSResult {\n        coefficients: final_coefficients,\n        fitted_values: mu,\n        linear_predictor: eta,\n        deviance,\n        iterations: iteration,\n        converged,\n        covariance_unscaled: cov_unscaled,\n        irls_weights: final_weights,\n        prior_weights: prior_weights_vec,\n        offset: offset_vec,\n        y: y.to_owned(),\n        family_name: family.name().to_string(),\n        penalty: Penalty::None,\n        design_matrix: None,\n    })\n}\n</code></pre> Code Explanation <code>Ok(IRLSResult { ... })</code> Return success with struct <code>coefficients: final_coefficients</code> Field name: value <code>deviance,</code> Shorthand when field name equals variable name <code>y.to_owned()</code> Convert borrowed slice to owned data <code>.to_string()</code> Convert &amp;str to owned String <code>Penalty::None</code> Enum variant <code>None</code> Option::None"},{"location":"rust-guide/code-walkthrough/#part-7-tests","title":"Part 7: Tests","text":"<p>File: <code>crates/rustystats-core/src/families/poisson.rs</code></p> <pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n    use ndarray::array;\n    use approx::assert_abs_diff_eq;\n\n    #[test]\n    fn test_poisson_variance() {\n        let family = PoissonFamily;\n        let mu = array![0.5, 1.0, 2.0, 10.0];\n\n        let var = family.variance(&amp;mu);\n        assert_abs_diff_eq!(var, mu, epsilon = 1e-10);\n    }\n</code></pre> Code Explanation <code>#[cfg(test)]</code> Conditional compilation: only compile when testing <code>mod tests</code> Nested module for tests <code>use super::*</code> Import everything from parent module <code>use ndarray::array</code> Import the <code>array!</code> macro <code>#[test]</code> Mark function as a test <code>fn test_poisson_variance()</code> Test function (no parameters, no return) <code>array![0.5, 1.0, 2.0, 10.0]</code> Macro to create array literal <code>assert_abs_diff_eq!(var, mu, epsilon = 1e-10)</code> Assert arrays are approximately equal"},{"location":"rust-guide/code-walkthrough/#part-8-coordinate-descent-specifics","title":"Part 8: Coordinate Descent Specifics","text":"<p>File: <code>crates/rustystats-core/src/solvers/coordinate_descent.rs</code></p>"},{"location":"rust-guide/code-walkthrough/#soft-thresholding","title":"Soft Thresholding","text":"<pre><code>pub fn soft_threshold(z: f64, gamma: f64) -&gt; f64 {\n    if z &gt; gamma {\n        z - gamma\n    } else if z &lt; -gamma {\n        z + gamma\n    } else {\n        0.0\n    }\n}\n</code></pre> Code Explanation <code>pub fn soft_threshold(z: f64, gamma: f64) -&gt; f64</code> Public function taking two floats, returning float <code>if ... else if ... else</code> Chained conditionals <code>0.0</code> Float literal (no semicolon = return value)"},{"location":"rust-guide/code-walkthrough/#parallel-fold-reduce-for-gram-matrix","title":"Parallel Fold-Reduce for Gram Matrix","text":"<pre><code>let xwx: Vec&lt;f64&gt; = (0..n)\n    .into_par_iter()\n    .fold(\n        || vec![0.0; p * p],\n        |mut acc, i| {\n            let w_i = combined_weights[i];\n            let x_i = x.row(i);\n            for j in 0..p {\n                let xij_w = x_i[j] * w_i;\n                for k in j..p {\n                    acc[j * p + k] += xij_w * x_i[k];\n                }\n            }\n            acc\n        },\n    )\n    .reduce(\n        || vec![0.0; p * p],\n        |mut a, b| {\n            for i in 0..a.len() {\n                a[i] += b[i];\n            }\n            a\n        },\n    );\n</code></pre> Code Explanation <code>.fold(init, f)</code> Parallel fold with initial value factory and accumulator function <code>\\|\\| vec![0.0; p * p]</code> Closure returning new zero vector (called per thread) <code>vec![0.0; p * p]</code> Vec macro: create vector of <code>p*p</code> zeros <code>\\|mut acc, i\\|</code> Closure taking mutable accumulator and index <code>x.row(i)</code> Get row i of matrix as array view <code>acc[j * p + k] += ...</code> Accumulate into flattened 2D index <code>.reduce(init, combine)</code> Combine thread-local results <code>\\|mut a, b\\|</code> Combine two accumulators"},{"location":"rust-guide/code-walkthrough/#part-9-common-patterns-summary","title":"Part 9: Common Patterns Summary","text":""},{"location":"rust-guide/code-walkthrough/#pattern-method-chaining","title":"Pattern: Method Chaining","text":"<pre><code>mu.iter().all(|&amp;x| x &gt; 0.0 &amp;&amp; x.is_finite())\n</code></pre> <p>Read left-to-right: take <code>mu</code>, create iterator, check if all elements satisfy condition.</p>"},{"location":"rust-guide/code-walkthrough/#pattern-error-propagation-with","title":"Pattern: Error Propagation with <code>?</code>","text":"<pre><code>let result = some_fallible_operation()?;\n</code></pre> <p>If <code>some_fallible_operation()</code> returns <code>Err</code>, immediately return that error. Otherwise, unwrap the <code>Ok</code> value.</p>"},{"location":"rust-guide/code-walkthrough/#pattern-builder-style-apis","title":"Pattern: Builder-style APIs","text":"<pre><code>let config = IRLSConfig {\n    max_iterations: 50,\n    ..Default::default()  // Fill remaining fields with defaults\n};\n</code></pre>"},{"location":"rust-guide/code-walkthrough/#pattern-iterating-and-collecting","title":"Pattern: Iterating and Collecting","text":"<pre><code>let squares: Vec&lt;f64&gt; = (0..10).map(|x| (x * x) as f64).collect();\n</code></pre>"},{"location":"rust-guide/code-walkthrough/#pattern-parallel-processing-with-rayon","title":"Pattern: Parallel Processing with Rayon","text":"<pre><code>use rayon::prelude::*;\n\nlet results: Vec&lt;_&gt; = data.par_iter().map(|x| expensive_operation(x)).collect();\n</code></pre> <p>Just change <code>.iter()</code> to <code>.par_iter()</code> for parallelism.</p>"},{"location":"rust-guide/code-walkthrough/#quick-reference-common-methods","title":"Quick Reference: Common Methods","text":"Method On Type Returns Description <code>.len()</code> Array/Vec <code>usize</code> Number of elements <code>.nrows()</code> Array2 <code>usize</code> Number of rows <code>.ncols()</code> Array2 <code>usize</code> Number of columns <code>.clone()</code> Any Clone Owned copy Deep copy <code>.iter()</code> Collection Iterator Iterate by reference <code>.into_iter()</code> Collection Iterator Iterate by value (consumes) <code>.map(f)</code> Iterator Iterator Transform elements <code>.filter(f)</code> Iterator Iterator Keep matching elements <code>.collect()</code> Iterator Collection Gather into collection <code>.sum()</code> Iterator Number Sum all elements <code>.all(f)</code> Iterator bool True if all match <code>.any(f)</code> Iterator bool True if any match <code>.zip(other)</code> Iterator Iterator Pair with another iterator <code>.mapv(f)</code> ndarray Array Map values <code>.dot(&amp;other)</code> Array Array/scalar Matrix multiplication <code>.abs()</code> Number Number Absolute value <code>.ln()</code> f64 f64 Natural log <code>.exp()</code> f64 f64 Exponential <code>.max(other)</code> f64 f64 Maximum <code>.min(other)</code> f64 f64 Minimum <code>.is_finite()</code> f64 bool Not NaN or infinity"},{"location":"rust-guide/code-walkthrough/#next-steps","title":"Next Steps","text":"<ul> <li>Rust Fundamentals \u2014 Ownership, borrowing, and core concepts</li> <li>Adding a New Family \u2014 Practice implementing a trait</li> <li>Adding a New Link \u2014 Another implementation exercise</li> </ul>"},{"location":"rust-guide/fundamentals/","title":"Rust Fundamentals for RustyStats","text":"<p>This guide teaches the Rust concepts you need to understand and maintain the RustyStats codebase. It's written for programmers who know another language (Python, C++, Java, etc.) but are new to Rust.</p> <p>What makes Rust different: Rust guarantees memory safety at compile time without garbage collection. This is achieved through its ownership system, which may feel unfamiliar at first but becomes natural with practice.</p>"},{"location":"rust-guide/fundamentals/#part-1-ownership-and-borrowing","title":"Part 1: Ownership and Borrowing","text":"<p>This is Rust's most distinctive feature. Understanding it is essential for reading and writing Rust code.</p>"},{"location":"rust-guide/fundamentals/#11-the-problem-rust-solves","title":"1.1 The Problem Rust Solves","text":"<p>In C/C++, memory bugs are common: - Use after free: accessing memory after it's been deallocated - Double free: deallocating the same memory twice - Dangling pointers: pointers to deallocated memory - Data races: multiple threads accessing memory unsafely</p> <p>Languages like Python and Java solve this with garbage collection (GC), but GC has overhead and unpredictable pauses.</p> <p>Rust solves it at compile time with zero runtime cost through ownership rules.</p>"},{"location":"rust-guide/fundamentals/#12-the-three-rules-of-ownership","title":"1.2 The Three Rules of Ownership","text":"<ol> <li>Each value has exactly one owner (a variable that \"owns\" it)</li> <li>When the owner goes out of scope, the value is dropped (memory is freed)</li> <li>Ownership can be transferred (moved) or temporarily lent (borrowed)</li> </ol>"},{"location":"rust-guide/fundamentals/#13-ownership-and-moves","title":"1.3 Ownership and Moves","text":"<pre><code>fn main() {\n    let s1 = String::from(\"hello\");  // s1 owns the string\n    let s2 = s1;                      // Ownership MOVES to s2\n\n    // println!(\"{}\", s1);  // ERROR! s1 no longer owns anything\n    println!(\"{}\", s2);     // OK: s2 owns the string\n}\n</code></pre> <p>When <code>s2 = s1</code>, the ownership moves. The variable <code>s1</code> becomes invalid\u2014Rust prevents you from using it.</p> <p>Why Move Instead of Copy?</p> <p><code>String</code> stores its data on the heap. If Rust copied the data, both <code>s1</code> and <code>s2</code> would point to the same memory. When both go out of scope, we'd have a double-free bug. Instead, Rust transfers ownership.</p> <p>Simple types are copied, not moved:</p> <pre><code>let x = 5;      // i32 is a simple type stored on the stack\nlet y = x;      // x is COPIED to y (no move)\nprintln!(\"{} {}\", x, y);  // Both are valid: \"5 5\"\n</code></pre> <p>Types like integers, floats, and booleans implement the <code>Copy</code> trait and are copied automatically.</p>"},{"location":"rust-guide/fundamentals/#14-borrowing-with-references","title":"1.4 Borrowing with References","text":"<p>What if you want to use a value without taking ownership? You borrow it with a reference:</p> <pre><code>fn main() {\n    let s = String::from(\"hello\");\n\n    let len = calculate_length(&amp;s);  // Pass a reference (borrow)\n\n    println!(\"Length of '{}' is {}\", s, len);  // s is still valid!\n}\n\nfn calculate_length(s: &amp;String) -&gt; usize {  // s is a reference\n    s.len()\n}  // s goes out of scope, but since it doesn't own the String, nothing is dropped\n</code></pre> <p>The <code>&amp;</code> creates a reference. The function borrows <code>s</code> temporarily but doesn't own it.</p>"},{"location":"rust-guide/fundamentals/#15-mutable-references","title":"1.5 Mutable References","text":"<p>By default, references are immutable. To modify borrowed data:</p> <pre><code>fn main() {\n    let mut s = String::from(\"hello\");  // s must be declared mut\n\n    change(&amp;mut s);  // Pass a mutable reference\n\n    println!(\"{}\", s);  // \"hello, world\"\n}\n\nfn change(s: &amp;mut String) {\n    s.push_str(\", world\");\n}\n</code></pre> <p>Critical rule: You can have either: - One mutable reference, OR - Any number of immutable references</p> <p>But not both at the same time! This prevents data races at compile time.</p> <pre><code>let mut s = String::from(\"hello\");\n\nlet r1 = &amp;s;      // OK: immutable borrow\nlet r2 = &amp;s;      // OK: another immutable borrow\n// let r3 = &amp;mut s;  // ERROR! Can't borrow mutably while immutably borrowed\n\nprintln!(\"{} {}\", r1, r2);\n\nlet r3 = &amp;mut s;  // OK: r1 and r2 are no longer used\n</code></pre>"},{"location":"rust-guide/fundamentals/#16-ownership-in-rustystats","title":"1.6 Ownership in RustyStats","text":"<p>In RustyStats, you'll see patterns like:</p> <pre><code>// Taking ownership (consumes the input)\npub fn process(data: Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n    // data is owned here, will be dropped when function returns\n    data * 2.0\n}\n\n// Borrowing (doesn't consume)\npub fn calculate_mean(data: &amp;Array1&lt;f64&gt;) -&gt; f64 {\n    data.sum() / data.len() as f64\n}\n\n// Mutable borrowing (modifies in place)\npub fn normalize(data: &amp;mut Array1&lt;f64&gt;) {\n    let mean = data.mean().unwrap();\n    *data -= mean;\n}\n</code></pre> <p>Guideline: Prefer borrowing (<code>&amp;</code> or <code>&amp;mut</code>) unless you need ownership.</p>"},{"location":"rust-guide/fundamentals/#part-2-structs-and-methods","title":"Part 2: Structs and Methods","text":""},{"location":"rust-guide/fundamentals/#21-defining-structs","title":"2.1 Defining Structs","text":"<p>Structs group related data:</p> <pre><code>pub struct IRLSConfig {\n    pub max_iterations: usize,\n    pub tolerance: f64,\n    pub min_weight: f64,\n    pub verbose: bool,\n}\n</code></pre> <ul> <li><code>pub</code> makes the struct and its fields accessible from other modules</li> <li>Each field has a name and type</li> </ul>"},{"location":"rust-guide/fundamentals/#22-creating-instances","title":"2.2 Creating Instances","text":"<pre><code>let config = IRLSConfig {\n    max_iterations: 25,\n    tolerance: 1e-8,\n    min_weight: 1e-10,\n    verbose: false,\n};\n\n// Field shorthand when variable name matches\nlet max_iterations = 50;\nlet config2 = IRLSConfig {\n    max_iterations,  // Same as max_iterations: max_iterations\n    ..config         // Copy remaining fields from config\n};\n</code></pre>"},{"location":"rust-guide/fundamentals/#23-methods-with-impl-blocks","title":"2.3 Methods with impl Blocks","text":"<p>Methods are defined in <code>impl</code> blocks:</p> <pre><code>impl IRLSConfig {\n    // Associated function (no self) - like a static method\n    pub fn new() -&gt; Self {\n        Self {\n            max_iterations: 25,\n            tolerance: 1e-8,\n            min_weight: 1e-10,\n            verbose: false,\n        }\n    }\n\n    // Method (takes self) - called on an instance\n    pub fn with_tolerance(mut self, tol: f64) -&gt; Self {\n        self.tolerance = tol;\n        self  // Return self for chaining\n    }\n\n    // Method that borrows self (doesn't consume)\n    pub fn is_converged(&amp;self, old_dev: f64, new_dev: f64) -&gt; bool {\n        (old_dev - new_dev).abs() / old_dev &lt; self.tolerance\n    }\n\n    // Method that mutably borrows self\n    pub fn set_verbose(&amp;mut self, verbose: bool) {\n        self.verbose = verbose;\n    }\n}\n\n// Usage\nlet config = IRLSConfig::new()\n    .with_tolerance(1e-6);  // Builder pattern\n</code></pre>"},{"location":"rust-guide/fundamentals/#24-the-self-parameter","title":"2.4 The self Parameter","text":"<ul> <li><code>self</code> \u2014 takes ownership, method consumes the instance</li> <li><code>&amp;self</code> \u2014 immutable borrow, method can read but not modify</li> <li><code>&amp;mut self</code> \u2014 mutable borrow, method can modify</li> </ul> <p>Most methods use <code>&amp;self</code> or <code>&amp;mut self</code>.</p>"},{"location":"rust-guide/fundamentals/#part-3-traits-interfaces","title":"Part 3: Traits (Interfaces)","text":"<p>Traits define shared behavior, similar to interfaces in other languages.</p>"},{"location":"rust-guide/fundamentals/#31-defining-traits","title":"3.1 Defining Traits","text":"<pre><code>pub trait Family {\n    /// Name of the distribution family\n    fn name(&amp;self) -&gt; &amp;str;\n\n    /// Variance function V(\u03bc)\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Unit deviance d(y, \u03bc)\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Default link function for this family\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt;;\n\n    /// Initialize \u03bc from y\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Check if \u03bc values are valid\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool;\n}\n</code></pre>"},{"location":"rust-guide/fundamentals/#32-implementing-traits","title":"3.2 Implementing Traits","text":"<pre><code>pub struct PoissonFamily;\n\nimpl Family for PoissonFamily {\n    fn name(&amp;self) -&gt; &amp;str {\n        \"Poisson\"\n    }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.clone()  // V(\u03bc) = \u03bc for Poisson\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        // d(y, \u03bc) = 2[y log(y/\u03bc) - (y - \u03bc)]\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            if yi &gt; 0.0 {\n                2.0 * (yi * (yi / mui).ln() - (yi - mui))\n            } else {\n                2.0 * mui\n            }\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.mapv(|yi| (yi + 0.1).max(0.1))\n    }\n\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;m| m &gt; 0.0)\n    }\n}\n</code></pre>"},{"location":"rust-guide/fundamentals/#33-trait-objects-and-dynamic-dispatch","title":"3.3 Trait Objects and Dynamic Dispatch","text":"<p>When the concrete type isn't known at compile time, use trait objects:</p> <pre><code>// This function works with ANY type implementing Family\npub fn fit_glm(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,  // Trait object: any Family implementation\n    link: &amp;dyn Link,\n) -&gt; Result&lt;IRLSResult&gt; {\n    let mu = family.initialize_mu(y);\n    let variance = family.variance(&amp;mu);\n    // ...\n}\n\n// Calling with different families\nlet result1 = fit_glm(&amp;y, &amp;x, &amp;PoissonFamily, &amp;LogLink)?;\nlet result2 = fit_glm(&amp;y, &amp;x, &amp;GaussianFamily, &amp;IdentityLink)?;\n</code></pre> <p>The <code>dyn</code> keyword indicates dynamic dispatch\u2014the method to call is determined at runtime.</p>"},{"location":"rust-guide/fundamentals/#34-trait-bounds","title":"3.4 Trait Bounds","text":"<p>Require types to implement traits:</p> <pre><code>// T must implement both Send and Sync (safe for parallel use)\npub trait Family: Send + Sync {\n    // ...\n}\n\n// Generic function with trait bound\nfn process&lt;T: Family&gt;(family: &amp;T) {\n    println!(\"Family: {}\", family.name());\n}\n\n// Alternative syntax with where clause\nfn process&lt;T&gt;(family: &amp;T) \nwhere \n    T: Family + Clone,\n{\n    // ...\n}\n</code></pre>"},{"location":"rust-guide/fundamentals/#35-common-standard-library-traits","title":"3.5 Common Standard Library Traits","text":"Trait Purpose Example <code>Clone</code> Deep copy <code>let b = a.clone()</code> <code>Copy</code> Implicit bitwise copy Integers, floats <code>Debug</code> Debug formatting <code>println!(\"{:?}\", x)</code> <code>Display</code> User-facing formatting <code>println!(\"{}\", x)</code> <code>Default</code> Default value <code>let x = T::default()</code> <code>Send</code> Safe to send between threads Required for Rayon <code>Sync</code> Safe to share references between threads Required for Rayon"},{"location":"rust-guide/fundamentals/#part-4-error-handling","title":"Part 4: Error Handling","text":"<p>Rust doesn't have exceptions. Instead, it uses the <code>Result</code> type for recoverable errors.</p>"},{"location":"rust-guide/fundamentals/#41-the-result-type","title":"4.1 The Result Type","text":"<pre><code>enum Result&lt;T, E&gt; {\n    Ok(T),   // Success, contains a value of type T\n    Err(E),  // Error, contains an error of type E\n}\n</code></pre> <p>Functions that can fail return <code>Result</code>:</p> <pre><code>fn divide(a: f64, b: f64) -&gt; Result&lt;f64, String&gt; {\n    if b == 0.0 {\n        Err(String::from(\"Division by zero\"))\n    } else {\n        Ok(a / b)\n    }\n}\n</code></pre>"},{"location":"rust-guide/fundamentals/#42-handling-results","title":"4.2 Handling Results","text":"<p>Option 1: Pattern matching</p> <pre><code>match divide(10.0, 2.0) {\n    Ok(result) =&gt; println!(\"Result: {}\", result),\n    Err(e) =&gt; println!(\"Error: {}\", e),\n}\n</code></pre> <p>Option 2: if let</p> <pre><code>if let Ok(result) = divide(10.0, 2.0) {\n    println!(\"Result: {}\", result);\n}\n</code></pre> <p>Option 3: The ? operator (propagate errors)</p> <pre><code>fn calculate() -&gt; Result&lt;f64, String&gt; {\n    let a = divide(10.0, 2.0)?;  // Returns early if Err\n    let b = divide(a, 3.0)?;\n    Ok(b)\n}\n</code></pre> <p>The <code>?</code> operator: 1. If <code>Result</code> is <code>Ok</code>, unwrap the value and continue 2. If <code>Result</code> is <code>Err</code>, return the error immediately</p>"},{"location":"rust-guide/fundamentals/#43-custom-error-types","title":"4.3 Custom Error Types","text":"<p>RustyStats defines its own error type:</p> <pre><code>use thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum RustyStatsError {\n    #[error(\"Invalid input: {0}\")]\n    InvalidInput(String),\n\n    #[error(\"Convergence failed after {iterations} iterations\")]\n    ConvergenceFailure { iterations: usize },\n\n    #[error(\"Dimension mismatch: expected {expected}, got {actual}\")]\n    DimensionMismatch { expected: usize, actual: usize },\n\n    #[error(\"Linear algebra error: {0}\")]\n    LinAlgError(String),\n}\n\n// Type alias for convenience\npub type Result&lt;T&gt; = std::result::Result&lt;T, RustyStatsError&gt;;\n</code></pre> <p>Usage:</p> <pre><code>pub fn fit_glm(y: &amp;Array1&lt;f64&gt;, x: &amp;Array2&lt;f64&gt;) -&gt; Result&lt;IRLSResult&gt; {\n    if y.len() != x.nrows() {\n        return Err(RustyStatsError::DimensionMismatch {\n            expected: x.nrows(),\n            actual: y.len(),\n        });\n    }\n    // ...\n}\n</code></pre>"},{"location":"rust-guide/fundamentals/#44-the-option-type","title":"4.4 The Option Type","text":"<p>For values that might be absent (not an error, just missing):</p> <pre><code>enum Option&lt;T&gt; {\n    Some(T),  // Value present\n    None,     // Value absent\n}\n\nfn find_max(data: &amp;[f64]) -&gt; Option&lt;f64&gt; {\n    if data.is_empty() {\n        None\n    } else {\n        Some(data.iter().cloned().fold(f64::NEG_INFINITY, f64::max))\n    }\n}\n\n// Usage\nmatch find_max(&amp;data) {\n    Some(max) =&gt; println!(\"Max: {}\", max),\n    None =&gt; println!(\"Empty data\"),\n}\n\n// Or with unwrap_or\nlet max = find_max(&amp;data).unwrap_or(0.0);\n</code></pre>"},{"location":"rust-guide/fundamentals/#part-5-iterators","title":"Part 5: Iterators","text":"<p>Iterators are Rust's powerful abstraction for processing sequences.</p>"},{"location":"rust-guide/fundamentals/#51-basic-iteration","title":"5.1 Basic Iteration","text":"<pre><code>let numbers = vec![1, 2, 3, 4, 5];\n\n// Iterate by reference (most common)\nfor n in &amp;numbers {\n    println!(\"{}\", n);\n}\n\n// Iterate by mutable reference\nlet mut numbers = vec![1, 2, 3, 4, 5];\nfor n in &amp;mut numbers {\n    *n *= 2;  // Double each element\n}\n\n// Iterate by value (consumes the vector)\nfor n in numbers {\n    println!(\"{}\", n);\n}\n// numbers is no longer usable here\n</code></pre>"},{"location":"rust-guide/fundamentals/#52-iterator-methods","title":"5.2 Iterator Methods","text":"<p>Iterators have powerful combinators:</p> <pre><code>let numbers = vec![1, 2, 3, 4, 5];\n\n// map: transform each element\nlet squared: Vec&lt;_&gt; = numbers.iter().map(|x| x * x).collect();\n// [1, 4, 9, 16, 25]\n\n// filter: keep elements matching a condition\nlet evens: Vec&lt;_&gt; = numbers.iter().filter(|&amp;x| x % 2 == 0).collect();\n// [2, 4]\n\n// fold: accumulate into a single value\nlet sum: i32 = numbers.iter().fold(0, |acc, x| acc + x);\n// 15\n\n// chain multiple operations\nlet result: Vec&lt;_&gt; = numbers.iter()\n    .filter(|&amp;x| x % 2 == 1)  // Keep odd numbers\n    .map(|x| x * 2)            // Double them\n    .collect();\n// [2, 6, 10]\n</code></pre>"},{"location":"rust-guide/fundamentals/#53-ndarray-iterators","title":"5.3 ndarray Iterators","text":"<p>RustyStats uses ndarray, which has its own iteration patterns:</p> <pre><code>use ndarray::{Array1, Array2, Zip};\n\nlet a = Array1::from_vec(vec![1.0, 2.0, 3.0]);\nlet b = Array1::from_vec(vec![4.0, 5.0, 6.0]);\n\n// Element-wise iteration with Zip\nlet c = Zip::from(&amp;a).and(&amp;b).map_collect(|&amp;ai, &amp;bi| ai + bi);\n// [5.0, 7.0, 9.0]\n\n// Iterate over rows of a 2D array\nlet matrix = Array2::from_shape_vec((3, 2), vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]).unwrap();\nfor row in matrix.rows() {\n    println!(\"{:?}\", row);\n}\n\n// mapv: map over values (creates new array)\nlet doubled = a.mapv(|x| x * 2.0);\n\n// mapv_inplace: modify in place\nlet mut a = a;\na.mapv_inplace(|x| x * 2.0);\n</code></pre>"},{"location":"rust-guide/fundamentals/#54-parallel-iterators-with-rayon","title":"5.4 Parallel Iterators with Rayon","text":"<p>Rayon makes parallelism easy:</p> <pre><code>use rayon::prelude::*;\n\nlet numbers: Vec&lt;i32&gt; = (0..1000000).collect();\n\n// Sequential\nlet sum: i32 = numbers.iter().sum();\n\n// Parallel (just change iter() to par_iter())\nlet sum: i32 = numbers.par_iter().sum();\n\n// Parallel map\nlet squared: Vec&lt;_&gt; = numbers.par_iter().map(|x| x * x).collect();\n\n// Parallel fold-reduce\nlet sum = (0..1000000).into_par_iter()\n    .fold(|| 0, |acc, x| acc + x)  // Fold per thread\n    .reduce(|| 0, |a, b| a + b);   // Combine threads\n</code></pre> <p>RustyStats uses parallel iterators for expensive computations like X'WX.</p>"},{"location":"rust-guide/fundamentals/#part-6-generics","title":"Part 6: Generics","text":"<p>Write code that works with multiple types.</p>"},{"location":"rust-guide/fundamentals/#61-generic-functions","title":"6.1 Generic Functions","text":"<pre><code>// Works with any type T that implements PartialOrd\nfn max&lt;T: PartialOrd&gt;(a: T, b: T) -&gt; T {\n    if a &gt; b { a } else { b }\n}\n\nlet x = max(5, 10);        // T = i32\nlet y = max(3.14, 2.71);   // T = f64\n</code></pre>"},{"location":"rust-guide/fundamentals/#62-generic-structs","title":"6.2 Generic Structs","text":"<pre><code>struct Point&lt;T&gt; {\n    x: T,\n    y: T,\n}\n\nimpl&lt;T&gt; Point&lt;T&gt; {\n    fn new(x: T, y: T) -&gt; Self {\n        Point { x, y }\n    }\n}\n\n// Can add methods only for specific types\nimpl Point&lt;f64&gt; {\n    fn distance_from_origin(&amp;self) -&gt; f64 {\n        (self.x.powi(2) + self.y.powi(2)).sqrt()\n    }\n}\n\nlet int_point = Point::new(5, 10);\nlet float_point = Point::new(1.0, 2.0);\nlet dist = float_point.distance_from_origin();\n</code></pre>"},{"location":"rust-guide/fundamentals/#63-monomorphization","title":"6.3 Monomorphization","text":"<p>Rust compiles generic code by generating specialized versions for each type used. This means: - No runtime overhead (unlike Java generics) - Larger binary size - Fast execution</p>"},{"location":"rust-guide/fundamentals/#part-7-modules-and-visibility","title":"Part 7: Modules and Visibility","text":""},{"location":"rust-guide/fundamentals/#71-module-structure","title":"7.1 Module Structure","text":"<pre><code>crate_name/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 lib.rs           // Crate root\n\u2502   \u251c\u2500\u2500 families/        // Module directory\n\u2502   \u2502   \u251c\u2500\u2500 mod.rs       // Module root\n\u2502   \u2502   \u251c\u2500\u2500 poisson.rs   // Submodule\n\u2502   \u2502   \u2514\u2500\u2500 gaussian.rs  // Submodule\n\u2502   \u2514\u2500\u2500 solvers/\n\u2502       \u251c\u2500\u2500 mod.rs\n\u2502       \u2514\u2500\u2500 irls.rs\n</code></pre> <p>In <code>lib.rs</code>: <pre><code>pub mod families;  // Declares the families module\npub mod solvers;\n</code></pre></p> <p>In <code>families/mod.rs</code>: <pre><code>mod poisson;   // Private submodule\nmod gaussian;\n\npub use poisson::PoissonFamily;   // Re-export publicly\npub use gaussian::GaussianFamily;\n\npub trait Family { ... }\n</code></pre></p>"},{"location":"rust-guide/fundamentals/#72-visibility","title":"7.2 Visibility","text":"<ul> <li><code>pub</code> \u2014 public, accessible from anywhere</li> <li><code>pub(crate)</code> \u2014 public within the crate only</li> <li><code>pub(super)</code> \u2014 public to parent module only</li> <li>(nothing) \u2014 private to current module</li> </ul> <pre><code>pub struct Config {\n    pub max_iter: usize,       // Public field\n    pub(crate) internal: bool, // Crate-only field\n    private: String,           // Private field\n}\n</code></pre>"},{"location":"rust-guide/fundamentals/#73-use-statements","title":"7.3 Use Statements","text":"<pre><code>// Import specific items\nuse crate::families::{Family, PoissonFamily};\n\n// Import everything from a module\nuse crate::families::*;\n\n// Rename on import\nuse std::collections::HashMap as Map;\n\n// Re-export\npub use crate::families::Family;\n</code></pre>"},{"location":"rust-guide/fundamentals/#part-8-common-patterns-in-rustystats","title":"Part 8: Common Patterns in RustyStats","text":""},{"location":"rust-guide/fundamentals/#81-builder-pattern","title":"8.1 Builder Pattern","text":"<pre><code>let config = IRLSConfig::new()\n    .max_iterations(50)\n    .tolerance(1e-6)\n    .verbose(true);\n</code></pre> <p>Implementation: <pre><code>impl IRLSConfig {\n    pub fn new() -&gt; Self { ... }\n\n    pub fn max_iterations(mut self, n: usize) -&gt; Self {\n        self.max_iterations = n;\n        self\n    }\n\n    pub fn tolerance(mut self, tol: f64) -&gt; Self {\n        self.tolerance = tol;\n        self\n    }\n}\n</code></pre></p>"},{"location":"rust-guide/fundamentals/#82-trait-objects-for-polymorphism","title":"8.2 Trait Objects for Polymorphism","text":"<pre><code>fn fit_glm(family: &amp;dyn Family, link: &amp;dyn Link) { ... }\n\n// Works with any Family implementation\nfit_glm(&amp;PoissonFamily, &amp;LogLink);\nfit_glm(&amp;GaussianFamily, &amp;IdentityLink);\n</code></pre>"},{"location":"rust-guide/fundamentals/#83-result-propagation","title":"8.3 Result Propagation","text":"<pre><code>pub fn fit() -&gt; Result&lt;IRLSResult&gt; {\n    let mu = initialize()?;      // ? propagates errors\n    let weights = compute_weights(&amp;mu)?;\n    let beta = solve(&amp;weights)?;\n    Ok(IRLSResult { ... })\n}\n</code></pre>"},{"location":"rust-guide/fundamentals/#84-parallel-computation","title":"8.4 Parallel Computation","text":"<pre><code>use rayon::prelude::*;\n\nlet result = (0..n).into_par_iter()\n    .fold(|| init_value, |acc, i| accumulate(acc, i))\n    .reduce(|| init_value, |a, b| combine(a, b));\n</code></pre>"},{"location":"rust-guide/fundamentals/#summary","title":"Summary","text":"<p>Key Rust concepts for RustyStats:</p> <ol> <li>Ownership: Values have one owner; use <code>&amp;</code> to borrow</li> <li>Structs: Group data; add methods with <code>impl</code></li> <li>Traits: Define interfaces; use <code>dyn Trait</code> for polymorphism</li> <li>Result/Option: Handle errors and missing values explicitly</li> <li>Iterators: Powerful, composable, parallelizable with Rayon</li> <li>Generics: Write type-safe reusable code</li> <li>Modules: Organize code; control visibility with <code>pub</code></li> </ol>"},{"location":"rust-guide/fundamentals/#further-reading","title":"Further Reading","text":"<ul> <li>The Rust Book \u2014 The official guide</li> <li>Rust By Example \u2014 Learn through examples</li> <li>ndarray documentation \u2014 Array library used by RustyStats</li> <li>Rayon documentation \u2014 Parallel iteration</li> </ul>"},{"location":"theory/coordinate-descent/","title":"Coordinate Descent for Penalized GLMs","text":"<p>This chapter provides a complete derivation of the coordinate descent algorithm used for fitting regularized GLMs. While IRLS handles unpenalized GLMs elegantly, adding L1 (Lasso) penalties requires a different approach because the L1 norm is non-differentiable at zero.</p> <p>Prerequisites: Understanding of GLMs, IRLS, and the regularization framework.</p>"},{"location":"theory/coordinate-descent/#part-1-why-not-just-use-irls","title":"Part 1: Why Not Just Use IRLS?","text":""},{"location":"theory/coordinate-descent/#11-the-problem-with-l1-penalties","title":"1.1 The Problem with L1 Penalties","text":"<p>Recall that IRLS solves the GLM by iteratively solving weighted least squares problems. At each iteration, we solve:</p> \\[ (\\mathbf{X}^T \\mathbf{W} \\mathbf{X}) \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{W} \\mathbf{z} \\] <p>For Ridge regression (L2 penalty), we can simply add a penalty to the diagonal:</p> \\[ (\\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{W} \\mathbf{z} \\] <p>This works because the L2 penalty \\(\\|\\boldsymbol{\\beta}\\|_2^2\\) is differentiable everywhere.</p> <p>However, the L1 penalty \\(\\|\\boldsymbol{\\beta}\\|_1 = \\sum_j |\\beta_j|\\) is not differentiable at zero:</p> \\[ \\frac{\\partial}{\\partial \\beta_j} |\\beta_j| = \\begin{cases} +1 &amp; \\text{if } \\beta_j &gt; 0 \\\\ -1 &amp; \\text{if } \\beta_j &lt; 0 \\\\ \\text{undefined} &amp; \\text{if } \\beta_j = 0 \\end{cases} \\] <p>This non-differentiability is precisely what enables Lasso to set coefficients exactly to zero\u2014but it means we can't use standard gradient-based methods.</p>"},{"location":"theory/coordinate-descent/#12-the-solution-coordinate-descent","title":"1.2 The Solution: Coordinate Descent","text":"<p>Coordinate descent solves this by updating one coefficient at a time while holding others fixed. For each single-variable subproblem, we can derive a closed-form solution using the soft-thresholding operator.</p> <p>The key insight: even though the overall objective is non-smooth, each single-variable optimization has an explicit solution.</p>"},{"location":"theory/coordinate-descent/#part-2-the-soft-thresholding-operator","title":"Part 2: The Soft-Thresholding Operator","text":""},{"location":"theory/coordinate-descent/#21-derivation-for-simple-lasso","title":"2.1 Derivation for Simple Lasso","text":"<p>Consider the simplest case: minimizing a univariate Lasso objective:</p> \\[ \\min_\\beta \\frac{1}{2}(\\beta - z)^2 + \\lambda |\\beta| \\] <p>where \\(z\\) is some target value.</p> <p>Case 1: \\(\\beta &gt; 0\\)</p> <p>The objective becomes \\(\\frac{1}{2}(\\beta - z)^2 + \\lambda \\beta\\).</p> <p>Taking the derivative and setting to zero: $$ \\beta - z + \\lambda = 0 \\implies \\beta = z - \\lambda $$</p> <p>This is valid only if \\(\\beta &gt; 0\\), which requires \\(z &gt; \\lambda\\).</p> <p>Case 2: \\(\\beta &lt; 0\\)</p> <p>The objective becomes \\(\\frac{1}{2}(\\beta - z)^2 - \\lambda \\beta\\).</p> <p>Taking the derivative: $$ \\beta - z - \\lambda = 0 \\implies \\beta = z + \\lambda $$</p> <p>This is valid only if \\(\\beta &lt; 0\\), which requires \\(z &lt; -\\lambda\\).</p> <p>Case 3: \\(\\beta = 0\\)</p> <p>If \\(|z| \\leq \\lambda\\), neither Case 1 nor Case 2 applies. We check that \\(\\beta = 0\\) is optimal by verifying the subgradient condition: at \\(\\beta = 0\\), the subgradient of \\(|\\beta|\\) is the interval \\([-1, 1]\\), so stationarity requires \\(-z + \\lambda \\cdot s = 0\\) for some \\(s \\in [-1, 1]\\), i.e., \\(|z| \\leq \\lambda\\).</p>"},{"location":"theory/coordinate-descent/#22-the-soft-thresholding-function","title":"2.2 The Soft-Thresholding Function","text":"<p>Combining all cases, the solution is the soft-thresholding operator:</p> \\[ S(z, \\lambda) = \\text{sign}(z) \\cdot \\max(|z| - \\lambda, 0) \\] <p>Equivalently:</p> \\[ S(z, \\lambda) = \\begin{cases} z - \\lambda &amp; \\text{if } z &gt; \\lambda \\\\ 0 &amp; \\text{if } |z| \\leq \\lambda \\\\ z + \\lambda &amp; \\text{if } z &lt; -\\lambda \\end{cases} \\] <p>Geometric Interpretation</p> <p>Soft-thresholding \"shrinks\" the value \\(z\\) toward zero by amount \\(\\lambda\\). If \\(|z| \\leq \\lambda\\), it shrinks all the way to zero. This is what produces exact zeros in Lasso solutions.</p>"},{"location":"theory/coordinate-descent/#23-implementation","title":"2.3 Implementation","text":"<pre><code>/// Soft-thresholding operator S(z, \u03b3) = sign(z) \u00d7 max(|z| - \u03b3, 0)\npub fn soft_threshold(z: f64, gamma: f64) -&gt; f64 {\n    if z &gt; gamma {\n        z - gamma\n    } else if z &lt; -gamma {\n        z + gamma\n    } else {\n        0.0\n    }\n}\n</code></pre>"},{"location":"theory/coordinate-descent/#part-3-coordinate-descent-for-weighted-lasso","title":"Part 3: Coordinate Descent for Weighted Lasso","text":""},{"location":"theory/coordinate-descent/#31-the-weighted-lasso-problem","title":"3.1 The Weighted Lasso Problem","text":"<p>In each IRLS iteration, we need to solve a weighted Lasso problem:</p> \\[ \\min_{\\boldsymbol{\\beta}} \\frac{1}{2} \\sum_{i=1}^n w_i (z_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^p |\\beta_j| \\] <p>where: - \\(z_i\\) is the working response for observation \\(i\\) - \\(w_i\\) is the combined weight (IRLS weight \u00d7 prior weight) - \\(\\lambda\\) is the L1 penalty strength</p>"},{"location":"theory/coordinate-descent/#32-single-coordinate-update","title":"3.2 Single-Coordinate Update","text":"<p>To update \\(\\beta_j\\) while holding all other coefficients fixed, define the partial residual:</p> \\[ r_i^{(j)} = z_i - \\sum_{k \\neq j} x_{ik} \\beta_k \\] <p>This is the \"residual\" if we remove the contribution of all predictors except \\(j\\).</p> <p>The objective for \\(\\beta_j\\) alone becomes:</p> \\[ \\min_{\\beta_j} \\frac{1}{2} \\sum_{i=1}^n w_i (r_i^{(j)} - x_{ij} \\beta_j)^2 + \\lambda |\\beta_j| \\] <p>Expanding the quadratic:</p> \\[ = \\frac{1}{2} \\sum_i w_i (r_i^{(j)})^2 - \\beta_j \\sum_i w_i x_{ij} r_i^{(j)} + \\frac{\\beta_j^2}{2} \\sum_i w_i x_{ij}^2 + \\lambda |\\beta_j| \\] <p>Let: - \\(\\rho_j = \\sum_i w_i x_{ij} r_i^{(j)}\\) (the weighted correlation of predictor \\(j\\) with partial residuals) - \\(\\nu_j = \\sum_i w_i x_{ij}^2\\) (the weighted sum of squares for predictor \\(j\\))</p> <p>The objective becomes:</p> \\[ \\min_{\\beta_j} \\frac{1}{2} \\nu_j \\beta_j^2 - \\rho_j \\beta_j + \\lambda |\\beta_j| + \\text{const} \\] <p>Dividing by \\(\\nu_j\\) (assuming \\(\\nu_j &gt; 0\\)):</p> \\[ \\min_{\\beta_j} \\frac{1}{2} \\beta_j^2 - \\frac{\\rho_j}{\\nu_j} \\beta_j + \\frac{\\lambda}{\\nu_j} |\\beta_j| \\] <p>This is equivalent to:</p> \\[ \\min_{\\beta_j} \\frac{1}{2} \\left(\\beta_j - \\frac{\\rho_j}{\\nu_j}\\right)^2 + \\frac{\\lambda}{\\nu_j} |\\beta_j| \\]"},{"location":"theory/coordinate-descent/#33-the-update-formula","title":"3.3 The Update Formula","text":"<p>Applying soft-thresholding:</p> \\[ \\boxed{\\beta_j^{\\text{new}} = \\frac{S(\\rho_j, \\lambda)}{\\nu_j} = \\frac{\\text{sign}(\\rho_j) \\max(|\\rho_j| - \\lambda, 0)}{\\nu_j}} \\] <p>where: - \\(\\rho_j = \\sum_i w_i x_{ij} r_i^{(j)} = \\sum_i w_i x_{ij} (z_i - \\sum_{k \\neq j} x_{ik} \\beta_k)\\) - \\(\\nu_j = \\sum_i w_i x_{ij}^2\\)</p>"},{"location":"theory/coordinate-descent/#34-elastic-net-extension","title":"3.4 Elastic Net Extension","text":"<p>For Elastic Net with L1 ratio \\(\\alpha\\) (called <code>l1_ratio</code> in RustyStats), the penalty is:</p> \\[ \\lambda \\left[ \\alpha \\sum_j |\\beta_j| + \\frac{1-\\alpha}{2} \\sum_j \\beta_j^2 \\right] \\] <p>The L2 part modifies the denominator:</p> \\[ \\boxed{\\beta_j^{\\text{new}} = \\frac{S(\\rho_j, \\lambda \\alpha)}{\\nu_j + \\lambda(1-\\alpha)}} \\] <p>When \\(\\alpha = 1\\) (pure Lasso), this reduces to the previous formula. When \\(\\alpha = 0\\) (pure Ridge), there's no soft-thresholding, just shrinkage.</p>"},{"location":"theory/coordinate-descent/#part-4-the-full-algorithm-ircd","title":"Part 4: The Full Algorithm (IRCD)","text":"<p>RustyStats uses Iteratively Reweighted Coordinate Descent (IRCD), which combines: 1. Outer loop: IRLS-style updates of working response and weights 2. Inner loop: Coordinate descent for the penalized weighted least squares</p>"},{"location":"theory/coordinate-descent/#41-algorithm-pseudocode","title":"4.1 Algorithm Pseudocode","text":"<pre><code>Input: y, X, family, link, \u03bb, \u03b1\nOutput: \u03b2\u0302\n\n1. Initialize:\n   \u03b2 \u2190 0  (or warm start)\n   \u03bc \u2190 family.initialize_mu(y)\n   \u03b7 \u2190 g(\u03bc)\n\n2. Outer loop (IRLS): repeat until deviance converges\n\n   a. Compute working response and weights:\n      For i = 1, ..., n:\n        V_i \u2190 family.variance(\u03bc_i)\n        d_i \u2190 link.derivative(\u03bc_i)\n        w_i \u2190 1 / (V_i \u00d7 d_i\u00b2)                    # IRLS weight\n        z_i \u2190 \u03b7_i + (y_i - \u03bc_i) \u00d7 d_i            # Working response\n\n   b. Inner loop (coordinate descent): repeat until \u03b2 converges\n\n      For j = 1, ..., p:\n        # Compute partial residual correlation\n        \u03c1_j \u2190 \u03a3\u1d62 w\u1d62 x\u1d62\u2c7c (z\u1d62 - \u03a3\u2096\u2260\u2c7c x\u1d62\u2096 \u03b2\u2096)\n\n        # Compute weighted sum of squares\n        \u03bd_j \u2190 \u03a3\u1d62 w\u1d62 x\u1d62\u2c7c\u00b2\n\n        # Update with soft-thresholding (skip intercept)\n        if j == 0 (intercept):\n          \u03b2_j \u2190 \u03c1_j / \u03bd_j\n        else:\n          \u03b2_j \u2190 S(\u03c1_j, \u03bb\u03b1) / (\u03bd_j + \u03bb(1-\u03b1))\n\n   c. Update predictions:\n      \u03b7 \u2190 X\u03b2 + offset\n      \u03bc \u2190 g\u207b\u00b9(\u03b7)\n\n   d. Check convergence:\n      D_new \u2190 family.deviance(y, \u03bc)\n      if |D_new - D_old| / D_old &lt; tolerance:\n        STOP\n      D_old \u2190 D_new\n\n3. Return \u03b2\n</code></pre>"},{"location":"theory/coordinate-descent/#42-the-intercept-is-never-penalized","title":"4.2 The Intercept is Never Penalized","text":"<p>Notice that the intercept (\\(j = 0\\)) uses a simple update without soft-thresholding:</p> \\[ \\beta_0 = \\frac{\\sum_i w_i (z_i - \\sum_{k &gt; 0} x_{ik} \\beta_k)}{\\sum_i w_i} \\] <p>This is critical because: 1. Penalizing the intercept would bias the model's overall level 2. The intercept controls the baseline prediction, which should be determined by the data</p>"},{"location":"theory/coordinate-descent/#part-5-computational-optimizations","title":"Part 5: Computational Optimizations","text":""},{"location":"theory/coordinate-descent/#51-covariance-updates-glmnet-style","title":"5.1 Covariance Updates (glmnet-style)","text":"<p>Computing \\(\\rho_j\\) naively requires \\(O(n)\\) operations per coefficient, giving \\(O(np)\\) per coordinate descent cycle.</p> <p>Key insight: We can use precomputed quantities to reduce this to \\(O(p)\\) per coefficient.</p> <p>Define: - \\(\\mathbf{G} = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}\\) (the weighted Gram matrix) - \\(\\mathbf{c} = \\mathbf{X}^T \\mathbf{W} \\mathbf{z}\\) (weighted correlation with working response)</p> <p>Then:</p> \\[ \\rho_j = c_j - \\sum_{k \\neq j} G_{jk} \\beta_k = c_j - \\sum_k G_{jk} \\beta_k + G_{jj} \\beta_j \\] <p>Since we track the gradient \\(\\nabla_j = c_j - \\sum_k G_{jk} \\beta_k\\):</p> \\[ \\rho_j = \\nabla_j + G_{jj} \\beta_j^{\\text{old}} \\] <p>After updating \\(\\beta_j\\), we update the gradient for all \\(k\\):</p> \\[ \\nabla_k \\leftarrow \\nabla_k - G_{kj} (\\beta_j^{\\text{new}} - \\beta_j^{\\text{old}}) \\] <p>This gives \\(O(p)\\) per coefficient update, and the Gram matrix \\(\\mathbf{G}\\) is computed once per outer iteration at cost \\(O(np^2)\\).</p>"},{"location":"theory/coordinate-descent/#52-rustystats-implementation","title":"5.2 RustyStats Implementation","text":"<pre><code>// Precompute X'Wz (gradient at \u03b2=0) - PARALLEL\nlet xwz: Vec&lt;f64&gt; = (0..p)\n    .into_par_iter()\n    .map(|j| {\n        x.column(j).iter()\n            .zip(weights.iter())\n            .zip(working_response.iter())\n            .map(|((&amp;xij, &amp;wi), &amp;zi)| wi * xij * zi)\n            .sum()\n    })\n    .collect();\n\n// Precompute X'WX (Gram matrix) - PARALLEL fold-reduce\nlet xwx: Vec&lt;f64&gt; = (0..n)\n    .into_par_iter()\n    .fold(\n        || vec![0.0; p * p],\n        |mut acc, i| {\n            let w_i = weights[i];\n            for j in 0..p {\n                for k in j..p {  // Upper triangle only\n                    acc[j * p + k] += w_i * x[[i, j]] * x[[i, k]];\n                }\n            }\n            acc\n        },\n    )\n    .reduce(|| vec![0.0; p * p], |a, b| /* element-wise sum */);\n</code></pre>"},{"location":"theory/coordinate-descent/#53-active-set-strategy","title":"5.3 Active Set Strategy","text":"<p>When \\(\\lambda\\) is large, many coefficients are zero. We can skip updating them:</p> <ol> <li>Strong rules: Predict which coefficients will be zero before fitting</li> <li>Active set cycling: Only iterate over non-zero coefficients after initial pass</li> <li>KKT checking: Verify zero coefficients satisfy optimality conditions</li> </ol> <p>RustyStats uses implicit active set management: coefficients that become zero stay zero unless \\(\\rho_j\\) exceeds the threshold.</p>"},{"location":"theory/coordinate-descent/#54-warm-starts","title":"5.4 Warm Starts","text":"<p>When computing a regularization path (fitting at many \\(\\lambda\\) values), we use warm starts:</p> <ol> <li>Start with large \\(\\lambda\\) where all coefficients (except intercept) are zero</li> <li>Use the solution at \\(\\lambda_k\\) as the starting point for \\(\\lambda_{k+1}\\)</li> </ol> <p>This dramatically speeds up path computation because solutions at adjacent \\(\\lambda\\) values are similar.</p>"},{"location":"theory/coordinate-descent/#part-6-convergence","title":"Part 6: Convergence","text":""},{"location":"theory/coordinate-descent/#61-inner-loop-convergence","title":"6.1 Inner Loop Convergence","text":"<p>The inner coordinate descent loop converges when the maximum coefficient change is small:</p> \\[ \\max_j |\\beta_j^{\\text{new}} - \\beta_j^{\\text{old}}| &lt; \\epsilon_{\\text{cd}} \\] <p>Default: \\(\\epsilon_{\\text{cd}} = 10^{-6}\\)</p>"},{"location":"theory/coordinate-descent/#62-outer-loop-convergence","title":"6.2 Outer Loop Convergence","text":"<p>The outer IRLS loop converges when the deviance stabilizes:</p> \\[ \\frac{|D^{(t)} - D^{(t-1)}|}{|D^{(t-1)}|} &lt; \\epsilon_{\\text{irls}} \\] <p>Default: \\(\\epsilon_{\\text{irls}} = 10^{-8}\\)</p>"},{"location":"theory/coordinate-descent/#63-convergence-theory","title":"6.3 Convergence Theory","text":"<p>Coordinate descent for Lasso is guaranteed to converge because: 1. The objective is convex (sum of convex loss and convex penalty) 2. Each coordinate update is the exact minimizer for that coordinate 3. The objective decreases monotonically</p> <p>For non-Gaussian families, the IRLS outer loop adds another layer, but convergence is typically fast because: - Working weights stabilize quickly - Each inner optimization starts from a good solution</p>"},{"location":"theory/coordinate-descent/#part-7-worked-example","title":"Part 7: Worked Example","text":""},{"location":"theory/coordinate-descent/#71-setup","title":"7.1 Setup","text":"<p>Consider a simple problem: - \\(n = 4\\) observations - \\(p = 3\\) predictors (including intercept) - Gaussian family (so \\(w_i = 1\\) for all \\(i\\)) - \\(\\lambda = 1.0\\), \\(\\alpha = 1.0\\) (pure Lasso)</p> <p>Data: $$ \\mathbf{X} = \\begin{pmatrix} 1 &amp; 2 &amp; 1 \\ 1 &amp; 4 &amp; 2 \\ 1 &amp; 6 &amp; 3 \\ 1 &amp; 8 &amp; 4 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 5 \\ 9 \\ 13 \\ 17 \\end{pmatrix} $$</p> <p>Note: \\(x_1\\) (column 2) and \\(x_2\\) (column 3) are perfectly correlated (\\(x_2 = x_1/2\\)). Lasso will select only one.</p>"},{"location":"theory/coordinate-descent/#72-iteration-1","title":"7.2 Iteration 1","text":"<p>Initialize: \\(\\boldsymbol{\\beta} = (0, 0, 0)^T\\)</p> <p>Compute \\(\\nu_j\\) (sum of squared predictor values): - \\(\\nu_0 = 1^2 + 1^2 + 1^2 + 1^2 = 4\\) - \\(\\nu_1 = 2^2 + 4^2 + 6^2 + 8^2 = 120\\) - \\(\\nu_2 = 1^2 + 2^2 + 3^2 + 4^2 = 30\\)</p> <p>Update \\(\\beta_0\\) (intercept, no penalty): $$ \\rho_0 = \\sum_i x_{i0}(y_i - 0) = 5 + 9 + 13 + 17 = 44 $$ $$ \\beta_0 = 44 / 4 = 11 $$</p> <p>Update \\(\\beta_1\\):</p> <p>Partial residual (removing intercept contribution): $$ r_i^{(1)} = y_i - 11 \\cdot 1 = y_i - 11 $$ So \\(r^{(1)} = (-6, -2, 2, 6)^T\\)</p> \\[ \\rho_1 = 2(-6) + 4(-2) + 6(2) + 8(6) = -12 - 8 + 12 + 48 = 40 \\] <p>Apply soft-thresholding: $$ \\beta_1 = S(40, 1.0) / 120 = 39 / 120 = 0.325 $$</p> <p>Update \\(\\beta_2\\):</p> <p>Now the residual accounts for both \\(\\beta_0\\) and \\(\\beta_1\\): $$ r_i^{(2)} = y_i - 11 - 0.325 \\cdot x_{i1} $$ $$ r^{(2)} = (5 - 11 - 0.65, 9 - 11 - 1.3, 13 - 11 - 1.95, 17 - 11 - 2.6) = (-6.65, -3.3, 0.05, 3.4) $$</p> \\[ \\rho_2 = 1(-6.65) + 2(-3.3) + 3(0.05) + 4(3.4) = -6.65 - 6.6 + 0.15 + 13.6 = 0.5 \\] <p>Apply soft-thresholding: $$ \\beta_2 = S(0.5, 1.0) / 30 = 0 / 30 = 0 $$</p> <p>Since \\(|\\rho_2| = 0.5 &lt; \\lambda = 1\\), the coefficient is set to exactly zero.</p>"},{"location":"theory/coordinate-descent/#73-subsequent-iterations","title":"7.3 Subsequent Iterations","text":"<p>The algorithm continues cycling through coefficients. After a few iterations: - \\(\\beta_0 \\approx 1.0\\) (intercept) - \\(\\beta_1 \\approx 2.0\\) (selected predictor) - \\(\\beta_2 = 0\\) (excluded due to collinearity)</p> <p>Lasso chose \\(x_1\\) over \\(x_2\\) because it had a larger initial correlation with \\(y\\).</p>"},{"location":"theory/coordinate-descent/#part-8-comparison-with-other-methods","title":"Part 8: Comparison with Other Methods","text":""},{"location":"theory/coordinate-descent/#81-coordinate-descent-vs-gradient-descent","title":"8.1 Coordinate Descent vs Gradient Descent","text":"Aspect Coordinate Descent Gradient Descent Update One variable at a time All variables simultaneously L1 handling Exact via soft-threshold Requires subgradients or proximal Convergence Often faster for sparse Can be slow near optimum Parallelism Sequential by nature Easily parallelized <p>RustyStats uses coordinate descent because it handles L1 penalties naturally and converges quickly for sparse solutions.</p>"},{"location":"theory/coordinate-descent/#82-ircd-vs-admm","title":"8.2 IRCD vs ADMM","text":"<p>ADMM (Alternating Direction Method of Multipliers) is another approach for L1 problems. Coordinate descent is typically faster for Lasso/Elastic Net, while ADMM is more general and handles complex constraints better.</p>"},{"location":"theory/coordinate-descent/#part-9-rustystats-implementation-details","title":"Part 9: RustyStats Implementation Details","text":""},{"location":"theory/coordinate-descent/#91-file-location","title":"9.1 File Location","text":"<pre><code>crates/rustystats-core/src/solvers/coordinate_descent.rs\n</code></pre>"},{"location":"theory/coordinate-descent/#92-key-functions","title":"9.2 Key Functions","text":"<pre><code>/// Main entry point for regularized GLM fitting\npub fn fit_glm_coordinate_descent(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,\n    link: &amp;dyn Link,\n    irls_config: &amp;IRLSConfig,\n    reg_config: &amp;RegularizationConfig,\n    offset: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n    weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;,\n) -&gt; Result&lt;IRLSResult&gt;\n</code></pre>"},{"location":"theory/coordinate-descent/#93-parallelization","title":"9.3 Parallelization","text":"<p>The Gram matrix computation uses Rayon's parallel fold-reduce:</p> <pre><code>let xwx: Vec&lt;f64&gt; = (0..n)\n    .into_par_iter()\n    .fold(\n        || vec![0.0; p * p],  // Per-thread accumulator\n        |mut acc, i| {\n            // Accumulate contribution from observation i\n            for j in 0..p {\n                for k in j..p {\n                    acc[j * p + k] += w_i * x[[i, j]] * x[[i, k]];\n                }\n            }\n            acc\n        },\n    )\n    .reduce(\n        || vec![0.0; p * p],  // Identity\n        |a, b| /* element-wise sum */\n    );\n</code></pre> <p>This parallelizes across observations, giving near-linear speedup on multi-core systems.</p>"},{"location":"theory/coordinate-descent/#94-numerical-stability","title":"9.4 Numerical Stability","text":"<ul> <li>Minimum weight floor: Prevents division by near-zero weights</li> <li>\u03bc clamping: Keeps predicted values in valid range for family</li> <li>Denominator check: Avoids division by zero in coefficient updates</li> </ul> <pre><code>let new_coef = if j &lt; pen_start {\n    rho / xwx_jj  // Intercept: no penalty\n} else {\n    let denom = xwx_jj + l2_penalty;\n    if denom.abs() &lt; 1e-10 {\n        0.0  // Avoid division by zero\n    } else {\n        soft_threshold(rho, l1_penalty) / denom\n    }\n};\n</code></pre>"},{"location":"theory/coordinate-descent/#summary","title":"Summary","text":"Concept Formula/Description Soft-thresholding \\(S(z, \\lambda) = \\text{sign}(z) \\max(\\|z\\| - \\lambda, 0)\\) Lasso update \\(\\beta_j = S(\\rho_j, \\lambda) / \\nu_j\\) Elastic Net update \\(\\beta_j = S(\\rho_j, \\lambda\\alpha) / (\\nu_j + \\lambda(1-\\alpha))\\) Partial residual \\(\\rho_j = \\sum_i w_i x_{ij} (z_i - \\sum_{k \\neq j} x_{ik} \\beta_k)\\) Intercept Never penalized: \\(\\beta_0 = \\rho_0 / \\nu_0\\) Convergence Inner: \\(\\max \\|\\Delta\\beta\\| &lt; \\epsilon\\); Outer: \\(\\|{\\Delta D}/{D}\\| &lt; \\epsilon\\) <p>Key advantages of coordinate descent: 1. Handles non-differentiable L1 penalty exactly 2. Produces exact zeros (true sparsity) 3. Efficient with covariance updates: \\(O(p)\\) per coefficient 4. Natural warm starts for regularization paths</p>"},{"location":"theory/coordinate-descent/#next-steps","title":"Next Steps","text":"<ul> <li>Regularization Theory \u2014 Ridge, Lasso, Elastic Net penalties</li> <li>Cross-Validation \u2014 Choosing optimal \u03bb</li> <li>IRLS Algorithm \u2014 The unpenalized solver</li> </ul>"},{"location":"theory/diagnostics/","title":"Model Diagnostics","text":"<p>After fitting a GLM, you need to assess whether the model fits well and identify areas for improvement. This chapter covers the diagnostic tools available in RustyStats.</p>"},{"location":"theory/diagnostics/#residuals","title":"Residuals","text":"<p>Residuals measure the discrepancy between observed and fitted values. GLMs have several types of residuals, each with different properties.</p>"},{"location":"theory/diagnostics/#response-residuals","title":"Response Residuals","text":"<p>The simplest residual - just the difference:</p> \\[ r_i^{\\text{response}} = y_i - \\hat{\\mu}_i \\] <pre><code>resid = result.resid_response()\n</code></pre> <p>Properties: - Easy to interpret - Not standardized (scale depends on \\(\\mu\\)) - Variance is not constant</p>"},{"location":"theory/diagnostics/#pearson-residuals","title":"Pearson Residuals","text":"<p>Standardized by the variance function:</p> \\[ r_i^{\\text{Pearson}} = \\frac{y_i - \\hat{\\mu}_i}{\\sqrt{V(\\hat{\\mu}_i)}} \\] <pre><code>resid = result.resid_pearson()\n</code></pre> <p>Properties: - Approximately standardized (unit variance under the model) - Useful for detecting outliers - Sum of squares = Pearson \\(\\chi^2\\) statistic</p>"},{"location":"theory/diagnostics/#deviance-residuals","title":"Deviance Residuals","text":"<p>Based on the contribution to deviance:</p> \\[ r_i^{\\text{deviance}} = \\text{sign}(y_i - \\hat{\\mu}_i) \\sqrt{d_i} \\] <p>where \\(d_i\\) is the unit deviance.</p> <pre><code>resid = result.resid_deviance()\n</code></pre> <p>Properties: - Sum of squares = Model deviance - More normally distributed than Pearson residuals - Preferred for most diagnostic plots</p>"},{"location":"theory/diagnostics/#working-residuals","title":"Working Residuals","text":"<p>Used internally by IRLS:</p> \\[ r_i^{\\text{working}} = (y_i - \\hat{\\mu}_i) \\cdot g'(\\hat{\\mu}_i) \\] <pre><code>resid = result.resid_working()\n</code></pre> <p>Properties: - On the linear predictor scale - Useful for partial residual plots</p>"},{"location":"theory/diagnostics/#goodness-of-fit-statistics","title":"Goodness-of-Fit Statistics","text":""},{"location":"theory/diagnostics/#deviance","title":"Deviance","text":"<p>The deviance measures overall model fit:</p> \\[ D = 2[\\ell(\\text{saturated}) - \\ell(\\text{fitted})] \\] <pre><code>deviance = result.deviance\n</code></pre> <p>Lower deviance = better fit. For comparing nested models:</p> \\[ D_{\\text{reduced}} - D_{\\text{full}} \\sim \\chi^2_{p_{\\text{full}} - p_{\\text{reduced}}} \\]"},{"location":"theory/diagnostics/#null-deviance","title":"Null Deviance","text":"<p>Deviance of the intercept-only model:</p> <pre><code>null_dev = result.null_deviance()\n</code></pre> <p>Pseudo-R\u00b2 (one of many definitions): [ R^2_{\\text{pseudo}} = 1 - \\frac{D}{D_{\\text{null}}} ]</p>"},{"location":"theory/diagnostics/#pearson-chi-squared","title":"Pearson Chi-Squared","text":"\\[ \\chi^2_{\\text{Pearson}} = \\sum_i \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)} \\] <pre><code>chi2 = result.pearson_chi2()\n</code></pre>"},{"location":"theory/diagnostics/#dispersion-estimation","title":"Dispersion Estimation","text":"<p>For families with unknown dispersion (Gaussian, Gamma, Quasi-families):</p> \\[ \\hat{\\phi}_{\\text{Pearson}} = \\frac{\\chi^2_{\\text{Pearson}}}{n - p} \\] \\[ \\hat{\\phi}_{\\text{deviance}} = \\frac{D}{n - p} \\] <pre><code>phi_deviance = result.scale()\nphi_pearson = result.scale_pearson()\n</code></pre>"},{"location":"theory/diagnostics/#checking-for-overdispersion","title":"Checking for Overdispersion","text":"<p>For Poisson/Binomial, dispersion should be \u2248 1:</p> <pre><code>dispersion_ratio = result.pearson_chi2() / result.df_resid\nprint(f\"Dispersion ratio: {dispersion_ratio:.2f}\")\n\n# &gt; 1.5 suggests overdispersion\n# &lt; 0.7 suggests underdispersion\n</code></pre>"},{"location":"theory/diagnostics/#information-criteria","title":"Information Criteria","text":""},{"location":"theory/diagnostics/#log-likelihood","title":"Log-Likelihood","text":"<pre><code>ll = result.llf()\n</code></pre>"},{"location":"theory/diagnostics/#aic-akaike-information-criterion","title":"AIC (Akaike Information Criterion)","text":"\\[ \\text{AIC} = -2\\ell + 2p \\] <p>Balances fit (likelihood) with complexity (number of parameters).</p> <pre><code>aic = result.aic()\n</code></pre> <p>Lower is better. AIC favors parsimony.</p>"},{"location":"theory/diagnostics/#bic-bayesian-information-criterion","title":"BIC (Bayesian Information Criterion)","text":"\\[ \\text{BIC} = -2\\ell + p \\log(n) \\] <pre><code>bic = result.bic()\n</code></pre> <p>More conservative than AIC for large samples (penalizes complexity more).</p>"},{"location":"theory/diagnostics/#when-to-use-each","title":"When to Use Each","text":"Criterion Penalty Best For AIC 2p Prediction, model averaging BIC p log(n) Model selection, large samples"},{"location":"theory/diagnostics/#calibration-diagnostics","title":"Calibration Diagnostics","text":"<p>Calibration measures how well predicted probabilities/means match observed values.</p>"},{"location":"theory/diagnostics/#actual-vs-expected-ratio","title":"Actual vs. Expected Ratio","text":"\\[ \\text{A/E} = \\frac{\\sum y_i}{\\sum \\hat{\\mu}_i} \\] <pre><code>diagnostics = result.diagnostics(data=data, categorical_factors=[\"Region\"])\nprint(f\"Overall A/E: {diagnostics.calibration['actual_expected_ratio']:.3f}\")\n</code></pre> <p>Interpretation: - A/E = 1.0: Perfect overall calibration - A/E &gt; 1.0: Model underpredicts - A/E &lt; 1.0: Model overpredicts</p>"},{"location":"theory/diagnostics/#calibration-by-decile","title":"Calibration by Decile","text":"<p>Split predictions into 10 groups and compare A/E in each:</p> <pre><code>for decile in diagnostics.calibration['by_decile']:\n    print(f\"Decile {decile['decile']}: A/E = {decile['ae_ratio']:.3f}\")\n</code></pre> <p>Good calibration: A/E \u2248 1 in all deciles.</p>"},{"location":"theory/diagnostics/#hosmer-lemeshow-test","title":"Hosmer-Lemeshow Test","text":"<p>Formal test for calibration (Binomial models):</p> \\[ H = \\sum_{g=1}^{G} \\frac{(O_g - E_g)^2}{E_g(1 - E_g/n_g)} \\] <p>Under \\(H_0\\) (good calibration): \\(H \\sim \\chi^2_{G-2}\\)</p>"},{"location":"theory/diagnostics/#discrimination-diagnostics","title":"Discrimination Diagnostics","text":"<p>Discrimination measures how well the model separates high and low values.</p>"},{"location":"theory/diagnostics/#gini-coefficient","title":"Gini Coefficient","text":"<p>Measures the area between the Lorenz curve and the line of equality:</p> \\[ \\text{Gini} = 2 \\times \\text{AUC} - 1 \\] <pre><code>gini = diagnostics.discrimination['gini_coefficient']\n</code></pre> <p>Interpretation: - Gini = 0: No discrimination (random predictions) - Gini = 1: Perfect discrimination - Typical values: 0.3-0.5 for insurance models</p>"},{"location":"theory/diagnostics/#lorenz-curve","title":"Lorenz Curve","text":"<p>Plots cumulative % of response vs. cumulative % of exposure, ordered by predicted risk:</p> <pre><code>lorenz = diagnostics.discrimination['lorenz_curve']\n# Plot: x = cumulative exposure %, y = cumulative claims %\n</code></pre>"},{"location":"theory/diagnostics/#lift","title":"Lift","text":"<p>How much better is the model than random in the top decile?</p> \\[ \\text{Lift} = \\frac{\\text{Response rate in top decile}}{\\text{Overall response rate}} \\]"},{"location":"theory/diagnostics/#factor-diagnostics","title":"Factor Diagnostics","text":"<p>Analyze model performance by individual factors.</p>"},{"location":"theory/diagnostics/#ae-by-factor-level","title":"A/E by Factor Level","text":"<p>For categorical factors:</p> <pre><code>for factor in diagnostics.factors:\n    if factor.factor_type == \"categorical\":\n        for level in factor.actual_vs_expected:\n            print(f\"{factor.name}={level['level']}: A/E={level['ae_ratio']:.3f}\")\n</code></pre>"},{"location":"theory/diagnostics/#residual-patterns","title":"Residual Patterns","text":"<p>Check if residuals are correlated with factors:</p> <pre><code>for factor in diagnostics.factors:\n    corr = factor.residual_pattern['correlation_with_residuals']\n    if abs(corr) &gt; 0.05:\n        print(f\"Warning: {factor.name} has residual correlation {corr:.3f}\")\n</code></pre> <p>High correlation suggests the factor effect is misspecified.</p>"},{"location":"theory/diagnostics/#interaction-detection","title":"Interaction Detection","text":"<p>RustyStats can automatically detect potential interactions:</p> <pre><code>diagnostics = result.diagnostics(\n    data=data,\n    categorical_factors=[\"Region\", \"VehBrand\"],\n    continuous_factors=[\"Age\"],\n)\n\nfor ic in diagnostics.interaction_candidates:\n    print(f\"Consider: {ic['factor1']} \u00d7 {ic['factor2']} (strength={ic['strength']:.3f})\")\n</code></pre> <p>The algorithm uses greedy residual-based detection to find factor pairs that explain residual variance.</p>"},{"location":"theory/diagnostics/#pre-fit-data-exploration","title":"Pre-Fit Data Exploration","text":"<p>Explore data before fitting any model:</p> <pre><code>exploration = rs.explore_data(\n    data=data,\n    response=\"ClaimNb\",\n    categorical_factors=[\"Region\", \"VehBrand\"],\n    continuous_factors=[\"Age\", \"VehPower\"],\n    exposure=\"Exposure\",\n    family=\"poisson\",\n    detect_interactions=True,\n)\n\nprint(exploration.response_stats)\nprint(exploration.to_json())\n</code></pre>"},{"location":"theory/diagnostics/#json-export-for-llm-integration","title":"JSON Export for LLM Integration","text":"<p>Export diagnostics in compact JSON format:</p> <pre><code>json_str = result.diagnostics_json(\n    data=data,\n    categorical_factors=[\"Region\"],\n    continuous_factors=[\"Age\"],\n)\n\n# Feed to LLM for analysis\nresponse = llm.analyze(f\"Analyze this model: {json_str}\")\n</code></pre>"},{"location":"theory/diagnostics/#diagnostic-plots-external","title":"Diagnostic Plots (External)","text":"<p>RustyStats focuses on computation. For visualization, use matplotlib/plotly:</p> <pre><code>import matplotlib.pyplot as plt\n\n# Residual vs. Fitted\nplt.scatter(result.fittedvalues, result.resid_deviance())\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('Fitted Values')\nplt.ylabel('Deviance Residuals')\n\n# Q-Q plot\nfrom scipy import stats\nstats.probplot(result.resid_deviance(), plot=plt)\n\n# Calibration plot\n# ... using diagnostics.calibration data\n</code></pre>"},{"location":"theory/diagnostics/#summary","title":"Summary","text":"<p>Key diagnostics workflow:</p> <ol> <li>Check convergence: <code>result.converged</code>, <code>result.iterations</code></li> <li>Assess overall fit: Deviance, AIC, BIC</li> <li>Check dispersion: Pearson \u03c7\u00b2/df \u2248 1</li> <li>Examine residuals: Patterns suggest model misspecification</li> <li>Verify calibration: A/E by decile</li> <li>Assess discrimination: Gini coefficient</li> <li>Factor analysis: A/E by level, residual correlations</li> <li>Detect interactions: Automated candidates</li> </ol>"},{"location":"theory/families/","title":"Distribution Families","text":"<p>This chapter provides a comprehensive treatment of the distribution families available in RustyStats. For each family, we derive the key quantities from first principles and show how they connect to the exponential family framework.</p> <p>Prerequisites: Familiarity with the exponential family and GLM framework.</p>"},{"location":"theory/families/#part-1-the-exponential-family-foundation","title":"Part 1: The Exponential Family Foundation","text":""},{"location":"theory/families/#11-recap-exponential-family-form","title":"1.1 Recap: Exponential Family Form","text":"<p>A distribution belongs to the exponential family if its density can be written as:</p> \\[ f(y; \\theta, \\phi) = \\exp\\left\\{\\frac{y\\theta - b(\\theta)}{\\phi} + c(y, \\phi)\\right\\} \\] <p>where: - \\(\\theta\\) = canonical parameter (related to the mean) - \\(\\phi\\) = dispersion parameter - \\(b(\\theta)\\) = cumulant function - \\(c(y, \\phi)\\) = normalization term</p>"},{"location":"theory/families/#12-key-derived-quantities","title":"1.2 Key Derived Quantities","text":"<p>From \\(b(\\theta)\\), we derive:</p> Quantity Formula Meaning Mean \\(\\mu = b'(\\theta)\\) Expected value of \\(Y\\) Variance \\(\\text{Var}(Y) = \\phi \\cdot b''(\\theta)\\) Spread of \\(Y\\) Variance function \\(V(\\mu) = b''(\\theta)\\) Variance as function of mean"},{"location":"theory/families/#13-the-family-trait-in-rustystats","title":"1.3 The Family Trait in RustyStats","text":"<p>Every family in RustyStats implements:</p> <pre><code>pub trait Family: Send + Sync {\n    /// Distribution name\n    fn name(&amp;self) -&gt; &amp;str;\n\n    /// Variance function V(\u03bc): how variance depends on mean\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Unit deviance d(y, \u03bc): contribution of each observation\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Total deviance (sum of weighted unit deviances)\n    fn deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;, \n                weights: Option&lt;&amp;Array1&lt;f64&gt;&gt;) -&gt; f64;\n\n    /// Canonical link function for this family\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt;;\n\n    /// Initialize \u03bc from y for IRLS\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;\n\n    /// Check if \u03bc values are in valid range\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool;\n}\n</code></pre>"},{"location":"theory/families/#part-2-the-gaussian-family","title":"Part 2: The Gaussian Family","text":""},{"location":"theory/families/#21-the-distribution","title":"2.1 The Distribution","text":"<p>The Gaussian (Normal) distribution has density:</p> \\[ f(y; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right\\} \\] <p>Support: \\(y \\in (-\\infty, +\\infty)\\)</p> <p>Parameters:  - \\(\\mu\\) = mean (location) - \\(\\sigma^2\\) = variance (spread)</p>"},{"location":"theory/families/#22-exponential-family-form","title":"2.2 Exponential Family Form","text":"<p>Rewrite the density:</p> \\[ f(y; \\mu, \\sigma^2) = \\exp\\left\\{-\\frac{(y-\\mu)^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)\\right\\} \\] <p>Expand the quadratic:</p> \\[ = \\exp\\left\\{-\\frac{y^2 - 2y\\mu + \\mu^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)\\right\\} \\] \\[ = \\exp\\left\\{\\frac{y\\mu - \\mu^2/2}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)\\right\\} \\] <p>Matching to exponential family form:</p> Component Value \\(\\theta\\) \\(\\mu\\) \\(\\phi\\) \\(\\sigma^2\\) \\(b(\\theta)\\) \\(\\theta^2/2 = \\mu^2/2\\) \\(c(y, \\phi)\\) \\(-\\frac{y^2}{2\\phi} - \\frac{1}{2}\\log(2\\pi\\phi)\\)"},{"location":"theory/families/#23-derived-quantities","title":"2.3 Derived Quantities","text":"<p>Mean: \\(\\mu = b'(\\theta) = \\theta\\) \u2713</p> <p>Variance function: \\(V(\\mu) = b''(\\theta) = 1\\)</p> <p>Variance: \\(\\text{Var}(Y) = \\phi \\cdot V(\\mu) = \\sigma^2 \\cdot 1 = \\sigma^2\\) \u2713</p>"},{"location":"theory/families/#24-unit-deviance","title":"2.4 Unit Deviance","text":"<p>The deviance compares the saturated model (one parameter per observation) to the fitted model. For Gaussian:</p> \\[ D = 2[\\ell_{\\text{saturated}} - \\ell_{\\text{fitted}}] \\] <p>The log-likelihood is: $$ \\ell = -\\frac{1}{2\\sigma^2}\\sum_i (y_i - \\mu_i)^2 - \\frac{n}{2}\\log(2\\pi\\sigma^2) $$</p> <p>For the saturated model, \\(\\hat{\\mu}_i = y_i\\), so \\(\\ell_{\\text{sat}} = -\\frac{n}{2}\\log(2\\pi\\sigma^2)\\).</p> <p>For the fitted model with predicted means \\(\\hat{\\mu}_i\\): $$ \\ell_{\\text{fit}} = -\\frac{1}{2\\sigma^2}\\sum_i (y_i - \\hat{\\mu}_i)^2 - \\frac{n}{2}\\log(2\\pi\\sigma^2) $$</p> <p>Thus: $$ D = 2[\\ell_{\\text{sat}} - \\ell_{\\text{fit}}] = \\frac{1}{\\sigma^2}\\sum_i (y_i - \\hat{\\mu}_i)^2 $$</p> <p>The scaled deviance is \\(D^* = D/\\phi = \\sum_i (y_i - \\hat{\\mu}_i)^2\\).</p> <p>The unit deviance is: $$ \\boxed{d(y, \\mu) = (y - \\mu)^2} $$</p> <p>This is just the squared residual!</p>"},{"location":"theory/families/#25-canonical-link","title":"2.5 Canonical Link","text":"<p>For Gaussian, \\(\\theta = \\mu\\), so the canonical link is the identity: $$ \\eta = g(\\mu) = \\mu $$</p>"},{"location":"theory/families/#26-irls-properties","title":"2.6 IRLS Properties","text":"<p>With identity link: - Weights: \\(w_i = 1/[V(\\mu_i) \\cdot g'(\\mu_i)^2] = 1/[1 \\cdot 1] = 1\\) - Working response: \\(z_i = \\eta_i + (y_i - \\mu_i) \\cdot 1 = y_i\\)</p> <p>IRLS reduces to ordinary least squares and converges in one iteration.</p>"},{"location":"theory/families/#27-implementation","title":"2.7 Implementation","text":"<pre><code>pub struct GaussianFamily;\n\nimpl Family for GaussianFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Gaussian\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Array1::ones(mu.len())  // V(\u03bc) = 1\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        (y - mu).mapv(|r| r * r)  // d(y,\u03bc) = (y-\u03bc)\u00b2\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(IdentityLink)\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.clone()  // Start with \u03bc = y\n    }\n\n    fn is_valid_mu(&amp;self, _mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        true  // Any real \u03bc is valid\n    }\n}\n</code></pre>"},{"location":"theory/families/#28-when-to-use","title":"2.8 When to Use","text":"<ul> <li>Continuous responses that can be positive or negative</li> <li>When variance doesn't depend on the mean</li> <li>Traditional linear regression problems</li> </ul>"},{"location":"theory/families/#part-3-the-poisson-family","title":"Part 3: The Poisson Family","text":""},{"location":"theory/families/#31-the-distribution","title":"3.1 The Distribution","text":"<p>The Poisson distribution models counts:</p> \\[ P(Y = y) = \\frac{\\mu^y e^{-\\mu}}{y!}, \\quad y = 0, 1, 2, \\ldots \\] <p>Support: \\(y \\in \\{0, 1, 2, \\ldots\\}\\)</p> <p>Parameter: \\(\\mu &gt; 0\\) = mean (and variance)</p>"},{"location":"theory/families/#32-exponential-family-form","title":"3.2 Exponential Family Form","text":"<p>Take the log: $$ \\log P(Y = y) = y \\log\\mu - \\mu - \\log(y!) $$</p> <p>Exponentiating: $$ P(Y = y) = \\exp{y \\log\\mu - \\mu - \\log(y!)} $$</p> <p>Matching to exponential family:</p> Component Value \\(\\theta\\) \\(\\log\\mu\\) \\(\\phi\\) \\(1\\) (fixed) \\(b(\\theta)\\) \\(e^\\theta = \\mu\\) \\(c(y, \\phi)\\) \\(-\\log(y!)\\)"},{"location":"theory/families/#33-derived-quantities","title":"3.3 Derived Quantities","text":"<p>Mean: \\(\\mu = b'(\\theta) = e^\\theta\\) \u2713 (since \\(\\theta = \\log\\mu\\))</p> <p>Variance function: \\(V(\\mu) = b''(\\theta) = e^\\theta = \\mu\\)</p> <p>Variance: \\(\\text{Var}(Y) = \\phi \\cdot V(\\mu) = 1 \\cdot \\mu = \\mu\\)</p> <p>Equidispersion</p> <p>For Poisson, mean = variance. This is called equidispersion and is a key assumption to check.</p>"},{"location":"theory/families/#34-unit-deviance-derivation","title":"3.4 Unit Deviance Derivation","text":"<p>The log-likelihood for one observation: $$ \\ell(y; \\mu) = y\\log\\mu - \\mu - \\log(y!) $$</p> <p>Saturated model (\\(\\hat{\\mu} = y\\)): $$ \\ell_{\\text{sat}} = y\\log y - y - \\log(y!) \\quad \\text{(for } y &gt; 0\\text{)} $$</p> <p>Fitted model: $$ \\ell_{\\text{fit}} = y\\log\\mu - \\mu - \\log(y!) $$</p> <p>Unit deviance: $$ d(y, \\mu) = 2[\\ell_{\\text{sat}} - \\ell_{\\text{fit}}] = 2[y\\log y - y - y\\log\\mu + \\mu] $$</p> \\[ \\boxed{d(y, \\mu) = 2\\left[y\\log\\frac{y}{\\mu} - (y - \\mu)\\right]} \\] <p>Special case: When \\(y = 0\\): $$ d(0, \\mu) = 2[0 - 0 - 0 + \\mu] = 2\\mu $$</p>"},{"location":"theory/families/#35-canonical-link","title":"3.5 Canonical Link","text":"<p>Since \\(\\theta = \\log\\mu\\), the canonical link is the log: $$ \\eta = g(\\mu) = \\log\\mu $$</p>"},{"location":"theory/families/#36-irls-properties","title":"3.6 IRLS Properties","text":"<p>With log link: - \\(g'(\\mu) = 1/\\mu\\) - Weights: \\(w_i = 1/[V(\\mu_i) \\cdot g'(\\mu_i)^2] = 1/[\\mu_i \\cdot (1/\\mu_i)^2] = \\mu_i\\) - Working response: \\(z_i = \\log\\mu_i + (y_i - \\mu_i)/\\mu_i\\)</p> <p>Observations with larger predicted means get higher weight.</p>"},{"location":"theory/families/#37-worked-example-claim-frequency","title":"3.7 Worked Example: Claim Frequency","text":"<p>Data: 5 policyholders with claim counts</p> Policyholder Exposure Claims Rate 1 1.0 0 0.0 2 0.5 1 2.0 3 1.0 2 2.0 4 0.75 1 1.33 5 1.0 3 3.0 <p>Model: \\(\\log(\\mu_i) = \\beta_0 + \\log(\\text{exposure}_i)\\)</p> <p>The offset \\(\\log(\\text{exposure})\\) makes this a rate model: $$ \\log(\\mu_i/\\text{exposure}_i) = \\beta_0 $$</p> <p>MLE for constant rate model: $$ \\hat{\\beta}_0 = \\log\\left(\\frac{\\sum y_i}{\\sum \\text{exposure}_i}\\right) = \\log\\left(\\frac{7}{4.25}\\right) = 0.499 $$</p> <p>Predicted rate: \\(e^{0.499} = 1.65\\) claims per exposure unit.</p> <p>Deviance: $$ D = 2\\sum_i \\left[y_i\\log\\frac{y_i}{\\hat{\\mu}_i} - (y_i - \\hat{\\mu}_i)\\right] $$</p> <p>where \\(\\hat{\\mu}_i = 1.65 \\times \\text{exposure}_i\\).</p>"},{"location":"theory/families/#38-checking-overdispersion","title":"3.8 Checking Overdispersion","text":"<p>If Var\\((Y) &gt; \\mu\\), the data is overdispersed. Test using:</p> \\[ \\text{Dispersion ratio} = \\frac{\\text{Pearson } \\chi^2}{n - p} = \\frac{\\sum (y_i - \\hat{\\mu}_i)^2/\\hat{\\mu}_i}{n - p} \\] <ul> <li>Ratio \\(\\approx 1\\): Poisson is appropriate</li> <li>Ratio \\(&gt; 1\\): Consider QuasiPoisson or Negative Binomial</li> </ul>"},{"location":"theory/families/#39-implementation","title":"3.9 Implementation","text":"<pre><code>pub struct PoissonFamily;\n\nimpl Family for PoissonFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Poisson\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.clone()  // V(\u03bc) = \u03bc\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            if yi &gt; 0.0 {\n                2.0 * (yi * (yi / mui).ln() - (yi - mui))\n            } else {\n                2.0 * mui  // Special case for y = 0\n            }\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.mapv(|yi| (yi + 0.1).max(0.1))  // Avoid log(0)\n    }\n\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;m| m &gt; 0.0)\n    }\n}\n</code></pre>"},{"location":"theory/families/#part-4-the-binomial-family","title":"Part 4: The Binomial Family","text":""},{"location":"theory/families/#41-the-distribution","title":"4.1 The Distribution","text":"<p>For \\(n\\) trials with success probability \\(p\\):</p> \\[ P(Y = y) = \\binom{n}{y} p^y (1-p)^{n-y}, \\quad y = 0, 1, \\ldots, n \\] <p>For binary data (\\(n = 1\\)): $$ P(Y = 1) = \\mu, \\quad P(Y = 0) = 1 - \\mu $$</p> <p>Support: \\(y \\in \\{0, 1\\}\\) (binary) or \\(y \\in [0, 1]\\) (proportion)</p> <p>Parameter: \\(\\mu = p \\in (0, 1)\\)</p>"},{"location":"theory/families/#42-exponential-family-form","title":"4.2 Exponential Family Form","text":"<p>For binary \\(Y \\in \\{0, 1\\}\\): $$ P(Y = y) = \\mu^y (1-\\mu)^{1-y} $$</p> <p>Take the log: $$ \\log P(Y = y) = y\\log\\mu + (1-y)\\log(1-\\mu) = y\\log\\frac{\\mu}{1-\\mu} + \\log(1-\\mu) $$</p> <p>Exponentiating: $$ P(Y = y) = \\exp\\left{y\\log\\frac{\\mu}{1-\\mu} + \\log(1-\\mu)\\right} $$</p> <p>Matching to exponential family:</p> Component Value \\(\\theta\\) \\(\\log\\frac{\\mu}{1-\\mu}\\) (log-odds) \\(\\phi\\) \\(1\\) (fixed) \\(b(\\theta)\\) \\(\\log(1 + e^\\theta)\\) \\(c(y, \\phi)\\) \\(0\\)"},{"location":"theory/families/#43-derived-quantities","title":"4.3 Derived Quantities","text":"<p>Mean: We have \\(\\mu = \\frac{e^\\theta}{1 + e^\\theta}\\). Check: $$ b'(\\theta) = \\frac{e^\\theta}{1 + e^\\theta} = \\mu \\quad \u2713 $$</p> <p>Variance function: $$ b''(\\theta) = \\frac{e^\\theta}{(1 + e^\\theta)^2} = \\mu(1-\\mu) $$</p> <p>So \\(V(\\mu) = \\mu(1-\\mu)\\).</p> <p>Variance: \\(\\text{Var}(Y) = \\phi \\cdot V(\\mu) = \\mu(1-\\mu)\\)</p> <p>The variance is maximized at \\(\\mu = 0.5\\) and goes to zero at \\(\\mu \\to 0\\) or \\(\\mu \\to 1\\).</p>"},{"location":"theory/families/#44-unit-deviance-derivation","title":"4.4 Unit Deviance Derivation","text":"<p>Log-likelihood for binary \\(y\\): $$ \\ell(y; \\mu) = y\\log\\mu + (1-y)\\log(1-\\mu) $$</p> <p>Saturated model: \\(\\hat{\\mu} = y\\), but this gives \\(\\log 0\\) when \\(y \\in \\{0, 1\\}\\). By convention, \\(\\ell_{\\text{sat}} = 0\\).</p> <p>Fitted model: $$ \\ell_{\\text{fit}} = y\\log\\mu + (1-y)\\log(1-\\mu) $$</p> <p>Unit deviance: $$ d(y, \\mu) = -2\\ell_{\\text{fit}} = -2[y\\log\\mu + (1-y)\\log(1-\\mu)] $$</p> <p>Rewriting: $$ \\boxed{d(y, \\mu) = 2\\left[y\\log\\frac{y}{\\mu} + (1-y)\\log\\frac{1-y}{1-\\mu}\\right]} $$</p> <p>Special cases: - \\(y = 1\\): \\(d(1, \\mu) = -2\\log\\mu\\) - \\(y = 0\\): \\(d(0, \\mu) = -2\\log(1-\\mu)\\)</p>"},{"location":"theory/families/#45-canonical-link-the-logit","title":"4.5 Canonical Link: The Logit","text":"<p>The canonical link is: $$ \\eta = g(\\mu) = \\log\\frac{\\mu}{1-\\mu} \\quad \\text{(log-odds or logit)} $$</p> <p>The inverse (logistic function): $$ \\mu = g^{-1}(\\eta) = \\frac{e^\\eta}{1 + e^\\eta} = \\frac{1}{1 + e^{-\\eta}} $$</p> <p>The derivative: $$ g'(\\mu) = \\frac{d}{d\\mu}\\log\\frac{\\mu}{1-\\mu} = \\frac{1}{\\mu} + \\frac{1}{1-\\mu} = \\frac{1}{\\mu(1-\\mu)} $$</p>"},{"location":"theory/families/#46-irls-properties","title":"4.6 IRLS Properties","text":"<p>With logit link: - Weights: \\(w_i = 1/[V(\\mu_i) \\cdot g'(\\mu_i)^2] = 1/[\\mu_i(1-\\mu_i) \\cdot 1/(\\mu_i(1-\\mu_i))^2] = \\mu_i(1-\\mu_i)\\) - Working response: \\(z_i = \\log\\frac{\\mu_i}{1-\\mu_i} + \\frac{y_i - \\mu_i}{\\mu_i(1-\\mu_i)}\\)</p> <p>Weights are largest when \\(\\mu \\approx 0.5\\) (most uncertainty) and smallest near 0 or 1.</p>"},{"location":"theory/families/#47-interpretation-odds-ratios","title":"4.7 Interpretation: Odds Ratios","text":"<p>The logit model: $$ \\log\\frac{\\mu}{1-\\mu} = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p $$</p> <p>For a one-unit increase in \\(x_j\\): $$ \\log\\frac{\\mu_{\\text{new}}}{1-\\mu_{\\text{new}}} - \\log\\frac{\\mu_{\\text{old}}}{1-\\mu_{\\text{old}}} = \\beta_j $$</p> <p>Thus: $$ \\frac{\\text{Odds}{\\text{new}}}{\\text{Odds} $$}}} = e^{\\beta_j</p> <p>\\(e^{\\beta_j}\\) is the odds ratio for a one-unit increase in \\(x_j\\).</p>"},{"location":"theory/families/#48-worked-example-claims-occurrence","title":"4.8 Worked Example: Claims Occurrence","text":"<p>Data: Whether policyholder had a claim</p> Age Has Claim 25 1 35 0 45 1 55 1 65 0 <p>Model: \\(\\text{logit}(\\mu_i) = \\beta_0 + \\beta_1 \\cdot \\text{age}_i\\)</p> <p>Let \\(x = (\\text{age} - 45)/10\\) for numerical stability.</p> <p>Iteration 1: - Initialize: \\(\\mu^{(0)} = (0.55, 0.45, 0.55, 0.55, 0.45)\\) (slightly toward observed) - Compute weights: \\(w_i = \\mu_i(1-\\mu_i)\\) - Compute working response - Solve WLS...</p> <p>After convergence (typically 5-8 iterations): - \\(\\hat{\\beta}_0 \\approx 0.4\\) (log-odds at age 45) - \\(\\hat{\\beta}_1 \\approx 0.05\\) (effect of 10-year age increase) - Odds ratio for 10-year increase: \\(e^{0.05} \\approx 1.05\\)</p>"},{"location":"theory/families/#49-complete-separation","title":"4.9 Complete Separation","text":"<p>A special problem in logistic regression: if a predictor perfectly separates 0s from 1s, the MLE doesn't exist.</p> <p>Example: All young people have claims, all old people don't.</p> <p>The algorithm tries \\(\\beta_1 \\to \\infty\\), which never converges.</p> <p>Solutions: 1. Remove the separating variable 2. Add regularization (Ridge/Lasso) 3. Use Firth's penalized likelihood</p>"},{"location":"theory/families/#410-implementation","title":"4.10 Implementation","text":"<pre><code>pub struct BinomialFamily;\n\nimpl Family for BinomialFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Binomial\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| m * (1.0 - m))  // V(\u03bc) = \u03bc(1-\u03bc)\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            let term1 = if yi &gt; 0.0 { yi * (yi / mui).ln() } else { 0.0 };\n            let term2 = if yi &lt; 1.0 { (1.0 - yi) * ((1.0 - yi) / (1.0 - mui)).ln() } else { 0.0 };\n            2.0 * (term1 + term2)\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogitLink)\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.mapv(|yi| (yi + 0.5) / 2.0)  // Shrink toward 0.5\n    }\n\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;m| m &gt; 0.0 &amp;&amp; m &lt; 1.0)\n    }\n}\n</code></pre>"},{"location":"theory/families/#part-5-the-gamma-family","title":"Part 5: The Gamma Family","text":""},{"location":"theory/families/#51-the-distribution","title":"5.1 The Distribution","text":"<p>The Gamma distribution for positive continuous data:</p> \\[ f(y; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} y^{\\alpha-1} e^{-\\beta y}, \\quad y &gt; 0 \\] <p>Reparameterized in terms of mean \\(\\mu\\) and shape \\(\\nu\\): - \\(\\mu = \\alpha/\\beta\\) (mean) - \\(\\nu = \\alpha\\) (shape)</p> \\[ E(Y) = \\mu, \\quad \\text{Var}(Y) = \\mu^2/\\nu \\]"},{"location":"theory/families/#52-exponential-family-form","title":"5.2 Exponential Family Form","text":"<p>With \\(\\alpha = \\nu\\) and \\(\\beta = \\nu/\\mu\\):</p> \\[ f(y; \\mu, \\nu) = \\frac{(\\nu/\\mu)^\\nu}{\\Gamma(\\nu)} y^{\\nu-1} \\exp\\{-\\nu y/\\mu\\} \\] <p>Rewriting: $$ \\log f = \\nu[\\log(\\nu/\\mu) + (\\nu-1)/\\nu \\cdot \\log y - y/\\mu] - \\log\\Gamma(\\nu) $$</p> <p>This is messy, but the key matching gives:</p> Component Value \\(\\theta\\) \\(-1/\\mu\\) \\(\\phi\\) \\(1/\\nu\\) \\(b(\\theta)\\) \\(-\\log(-\\theta)\\)"},{"location":"theory/families/#53-derived-quantities","title":"5.3 Derived Quantities","text":"<p>Mean: \\(\\mu = b'(\\theta) = -1/\\theta = \\mu\\) \u2713</p> <p>Variance function:  $$ b''(\\theta) = 1/\\theta^2 = \\mu^2 $$</p> <p>So \\(V(\\mu) = \\mu^2\\).</p> <p>Variance: \\(\\text{Var}(Y) = \\phi \\cdot V(\\mu) = \\mu^2/\\nu\\)</p>"},{"location":"theory/families/#54-key-property-constant-cv","title":"5.4 Key Property: Constant CV","text":"<p>The coefficient of variation (CV) is: $$ \\text{CV} = \\frac{\\text{SD}}{\\text{Mean}} = \\frac{\\sqrt{\\mu^2/\\nu}}{\\mu} = \\frac{1}{\\sqrt{\\nu}} $$</p> <p>This is constant regardless of the mean! This makes Gamma ideal for: - Claim severity (larger claims are more variable, proportionally) - Financial data with percentage-based volatility</p>"},{"location":"theory/families/#55-unit-deviance","title":"5.5 Unit Deviance","text":"\\[ \\boxed{d(y, \\mu) = 2\\left[-\\log\\frac{y}{\\mu} + \\frac{y - \\mu}{\\mu}\\right]} \\] <p>Or equivalently: $$ d(y, \\mu) = 2\\left[\\frac{y - \\mu}{\\mu} - \\log\\frac{y}{\\mu}\\right] $$</p>"},{"location":"theory/families/#56-links","title":"5.6 Links","text":"<p>Canonical link (inverse): \\(\\eta = -1/\\mu\\)</p> <p>This is rarely used because: - \\(\\mu &gt; 0\\) but \\(\\eta\\) can be any real number - The relationship is awkward to interpret</p> <p>Common link (log): \\(\\eta = \\log\\mu\\) - \\(\\mu = e^\\eta\\) is always positive - Coefficients have multiplicative interpretation - Recommended in most applications</p>"},{"location":"theory/families/#57-irls-with-log-link","title":"5.7 IRLS with Log Link","text":"<p>With log link: - \\(g'(\\mu) = 1/\\mu\\) - Weights: \\(w_i = 1/[\\mu_i^2 \\cdot (1/\\mu_i)^2] = 1\\)</p> <p>Constant weights! This is a happy coincidence when \\(V(\\mu) = \\mu^2\\) and \\(g'(\\mu) = 1/\\mu\\).</p>"},{"location":"theory/families/#58-worked-example-claim-severity","title":"5.8 Worked Example: Claim Severity","text":"<p>Data: Claim amounts for 4 claims</p> Claim Amount log(Amount) 1 500 6.21 2 1200 7.09 3 800 6.68 4 2500 7.82 <p>Model: \\(\\log(\\mu_i) = \\beta_0\\) (constant model)</p> <p>MLE: \\(\\hat{\\beta}_0 = \\frac{1}{4}\\sum \\log(y_i) = 6.95\\)</p> <p>(Note: For Gamma, the MLE of log-mean is the mean of logs, not log of means!)</p> <p>Predicted mean: \\(\\hat{\\mu} = e^{6.95} = 1047\\)</p> <p>Dispersion: Estimated from residuals, \\(\\hat{\\phi} \\approx 0.15\\), so \\(\\hat{\\nu} \\approx 6.7\\).</p>"},{"location":"theory/families/#59-implementation","title":"5.9 Implementation","text":"<pre><code>pub struct GammaFamily;\n\nimpl Family for GammaFamily {\n    fn name(&amp;self) -&gt; &amp;str { \"Gamma\" }\n\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| m * m)  // V(\u03bc) = \u03bc\u00b2\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            2.0 * ((yi - mui) / mui - (yi / mui).ln())\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)  // Log is more practical than canonical inverse\n    }\n\n    fn initialize_mu(&amp;self, y: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        y.mapv(|yi| yi.max(1e-10))  // Ensure positive\n    }\n\n    fn is_valid_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; bool {\n        mu.iter().all(|&amp;m| m &gt; 0.0)\n    }\n}\n</code></pre>"},{"location":"theory/families/#part-6-the-tweedie-family","title":"Part 6: The Tweedie Family","text":""},{"location":"theory/families/#61-the-power-variance-family","title":"6.1 The Power Variance Family","text":"<p>Tweedie distributions have variance function: $$ V(\\mu) = \\mu^p $$</p> <p>where \\(p\\) is the variance power.</p> Power \\(p\\) Distribution Support \\(p = 0\\) Gaussian \\((-\\infty, \\infty)\\) \\(p = 1\\) Poisson \\(\\{0, 1, 2, \\ldots\\}\\) \\(1 &lt; p &lt; 2\\) Compound Poisson-Gamma \\(\\{0\\} \\cup (0, \\infty)\\) \\(p = 2\\) Gamma \\((0, \\infty)\\) \\(p = 3\\) Inverse Gaussian \\((0, \\infty)\\)"},{"location":"theory/families/#62-the-sweet-spot-1-p-2","title":"6.2 The Sweet Spot: \\(1 &lt; p &lt; 2\\)","text":"<p>For \\(1 &lt; p &lt; 2\\), Tweedie has a remarkable property: it supports exact zeros AND positive continuous values.</p> <p>This is modeled as a compound Poisson process: $$ Y = \\sum_{j=1}^{N} Z_j $$</p> <p>where: - \\(N \\sim \\text{Poisson}(\\lambda)\\) \u2014 number of events - \\(Z_j \\sim \\text{Gamma}(\\alpha, \\beta)\\) \u2014 individual event sizes (i.i.d.)</p> <p>When \\(N = 0\\): \\(Y = 0\\) (exact zero) When \\(N &gt; 0\\): \\(Y &gt; 0\\) (sum of positive Gammas)</p>"},{"location":"theory/families/#63-connection-to-frequency-severity","title":"6.3 Connection to Frequency-Severity","text":"<p>In insurance, this naturally models pure premium: - \\(N\\) = number of claims (Poisson) - \\(Z_j\\) = individual claim amount (Gamma) - \\(Y\\) = total claims = pure premium</p> <p>The parameters connect: - \\(E(N) = \\lambda\\) - \\(E(Z) = \\alpha/\\beta\\) - \\(E(Y) = \\lambda \\cdot \\alpha/\\beta = \\mu\\)</p>"},{"location":"theory/families/#64-unit-deviance","title":"6.4 Unit Deviance","text":"<p>For \\(1 &lt; p &lt; 2\\):</p> \\[ \\boxed{d(y, \\mu) = 2\\left[\\frac{y^{2-p}}{(1-p)(2-p)} - \\frac{y\\mu^{1-p}}{1-p} + \\frac{\\mu^{2-p}}{2-p}\\right]} \\] <p>When \\(y = 0\\): $$ d(0, \\mu) = \\frac{2\\mu^{2-p}}{2-p} $$</p>"},{"location":"theory/families/#65-choosing-the-variance-power-p","title":"6.5 Choosing the Variance Power \\(p\\)","text":"<ul> <li>\\(p \\to 1\\): More Poisson-like (frequency dominates)</li> <li>\\(p = 1.5\\): Balanced (common default)</li> <li>\\(p \\to 2\\): More Gamma-like (severity dominates)</li> </ul> <p>Estimating \\(p\\): Profile the likelihood over a grid of \\(p\\) values.</p>"},{"location":"theory/families/#66-implementation","title":"6.6 Implementation","text":"<pre><code>pub struct TweedieFamily {\n    pub var_power: f64,  // p in V(\u03bc) = \u03bc^p\n}\n\nimpl Family for TweedieFamily {\n    fn variance(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| m.powf(self.var_power))\n    }\n\n    fn unit_deviance(&amp;self, y: &amp;Array1&lt;f64&gt;, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        let p = self.var_power;\n        Zip::from(y).and(mu).map_collect(|&amp;yi, &amp;mui| {\n            if yi == 0.0 {\n                2.0 * mui.powf(2.0 - p) / (2.0 - p)\n            } else {\n                let t1 = yi.powf(2.0 - p) / ((1.0 - p) * (2.0 - p));\n                let t2 = yi * mui.powf(1.0 - p) / (1.0 - p);\n                let t3 = mui.powf(2.0 - p) / (2.0 - p);\n                2.0 * (t1 - t2 + t3)\n            }\n        })\n    }\n\n    fn default_link(&amp;self) -&gt; Box&lt;dyn Link&gt; {\n        Box::new(LogLink)\n    }\n}\n</code></pre>"},{"location":"theory/families/#part-7-quasi-families","title":"Part 7: Quasi-Families","text":""},{"location":"theory/families/#71-the-quasi-likelihood-approach","title":"7.1 The Quasi-Likelihood Approach","text":"<p>Sometimes we don't have a full probability distribution\u2014just: - A mean model: \\(E(Y) = \\mu\\) - A variance model: \\(\\text{Var}(Y) = \\phi \\cdot V(\\mu)\\)</p> <p>Quasi-likelihood uses just these two relationships without specifying the full distribution.</p>"},{"location":"theory/families/#72-quasipoisson","title":"7.2 QuasiPoisson","text":"<p>Same variance function as Poisson, but \\(\\phi\\) is estimated:</p> \\[ V(\\mu) = \\mu, \\quad \\phi \\text{ estimated from data} \\] <p>Effect: - Point estimates: Identical to Poisson - Standard errors: Multiplied by \\(\\sqrt{\\hat{\\phi}}\\) - Confidence intervals: Wider if \\(\\hat{\\phi} &gt; 1\\)</p>"},{"location":"theory/families/#73-quasibinomial","title":"7.3 QuasiBinomial","text":"<p>Same idea for binomial:</p> \\[ V(\\mu) = \\mu(1-\\mu), \\quad \\phi \\text{ estimated} \\]"},{"location":"theory/families/#74-dispersion-estimation","title":"7.4 Dispersion Estimation","text":"\\[ \\hat{\\phi} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)} \\] <p>This is the Pearson chi-square divided by residual degrees of freedom.</p>"},{"location":"theory/families/#75-when-to-use-quasi-families","title":"7.5 When to Use Quasi-Families","text":"<ul> <li>Quick fix for overdispersion</li> <li>When you don't trust the full distributional assumption</li> <li>When AIC/BIC aren't the primary concern</li> </ul> <p>Caution: Quasi-likelihood inference is valid for large samples. AIC/BIC interpretation is murky.</p>"},{"location":"theory/families/#part-8-the-negative-binomial-family","title":"Part 8: The Negative Binomial Family","text":""},{"location":"theory/families/#81-the-distribution","title":"8.1 The Distribution","text":"<p>The Negative Binomial with mean \\(\\mu\\) and shape \\(\\theta\\):</p> \\[ P(Y = y) = \\frac{\\Gamma(y + \\theta)}{\\Gamma(\\theta) \\cdot y!} \\left(\\frac{\\theta}{\\theta + \\mu}\\right)^\\theta \\left(\\frac{\\mu}{\\theta + \\mu}\\right)^y \\] <p>Variance: $$ \\text{Var}(Y) = \\mu + \\frac{\\mu^2}{\\theta} $$</p>"},{"location":"theory/families/#82-variance-function","title":"8.2 Variance Function","text":"\\[ V(\\mu) = \\mu + \\frac{\\mu^2}{\\theta} \\] <p>This is quadratic in \\(\\mu\\), unlike QuasiPoisson's linear variance.</p>"},{"location":"theory/families/#83-the-theta-parameter","title":"8.3 The \\(\\theta\\) Parameter","text":"<ul> <li>Large \\(\\theta\\) (\\(\\to \\infty\\)): Approaches Poisson (\\(V \\to \\mu\\))</li> <li>Small \\(\\theta\\) (near 0): Strong overdispersion (\\(V \\to \\mu^2/\\theta\\))</li> </ul>"},{"location":"theory/families/#84-comparison-quasipoisson-vs-negative-binomial","title":"8.4 Comparison: QuasiPoisson vs. Negative Binomial","text":"Aspect QuasiPoisson Negative Binomial Variance \\(\\phi \\cdot \\mu\\) (linear) \\(\\mu + \\mu^2/\\theta\\) (quadratic) True distribution No Yes AIC/BIC valid No Yes Estimation \\(\\phi\\) from residuals \\(\\theta\\) from likelihood <p>Rule of thumb: Use NegBin when overdispersion increases with the mean.</p>"},{"location":"theory/families/#85-estimating-theta","title":"8.5 Estimating \\(\\theta\\)","text":"<p>RustyStats estimates \\(\\theta\\) by profiling: 1. For fixed \\(\\theta\\), fit GLM to get \\(\\hat{\\boldsymbol{\\beta}}(\\theta)\\) 2. Compute profile log-likelihood \\(\\ell_p(\\theta)\\) 3. Maximize over \\(\\theta\\)</p> <p>Or use an iterative algorithm alternating between \\(\\boldsymbol{\\beta}\\) and \\(\\theta\\).</p>"},{"location":"theory/families/#part-9-summary-and-selection-guide","title":"Part 9: Summary and Selection Guide","text":""},{"location":"theory/families/#91-decision-tree","title":"9.1 Decision Tree","text":"<pre><code>Response type?\n\u2502\n\u251c\u2500\u2500 Continuous\n\u2502   \u251c\u2500\u2500 Can be negative \u2192 Gaussian\n\u2502   \u2514\u2500\u2500 Always positive\n\u2502       \u251c\u2500\u2500 Has exact zeros \u2192 Tweedie (p \u2208 (1,2))\n\u2502       \u2514\u2500\u2500 No zeros \u2192 Gamma\n\u2502\n\u2514\u2500\u2500 Discrete\n    \u251c\u2500\u2500 Binary (0/1) \u2192 Binomial\n    \u2502   \u2514\u2500\u2500 Overdispersed? \u2192 QuasiBinomial\n    \u2502\n    \u2514\u2500\u2500 Counts (0, 1, 2, ...)\n        \u251c\u2500\u2500 Var \u2248 Mean \u2192 Poisson\n        \u2514\u2500\u2500 Var &gt; Mean (overdispersion)\n            \u251c\u2500\u2500 Quick fix \u2192 QuasiPoisson\n            \u2514\u2500\u2500 Better model \u2192 Negative Binomial\n</code></pre>"},{"location":"theory/families/#92-key-formulas-summary","title":"9.2 Key Formulas Summary","text":"Family \\(V(\\mu)\\) \\(\\phi\\) Canonical Link Support Gaussian \\(1\\) estimated Identity \\(\\mathbb{R}\\) Poisson \\(\\mu\\) \\(1\\) Log \\(\\mathbb{N}_0\\) Binomial \\(\\mu(1-\\mu)\\) \\(1\\) Logit \\(\\{0,1\\}\\) Gamma \\(\\mu^2\\) estimated Inverse (Log used) \\(\\mathbb{R}^+\\) Tweedie \\(\\mu^p\\) estimated Log \\(\\{0\\} \\cup \\mathbb{R}^+\\) NegBin \\(\\mu + \\mu^2/\\theta\\) \\(1\\) Log \\(\\mathbb{N}_0\\)"},{"location":"theory/families/#exercises","title":"Exercises","text":"<p>Exercise 1: Exponential Family</p> <p>Show that the Exponential distribution (\\(f(y) = \\lambda e^{-\\lambda y}\\)) is a special case of the Gamma family with \\(\\nu = 1\\).</p> <p>Exercise 2: Deviance Calculation</p> <p>For data \\(y = (3, 0, 2, 5)\\) with fitted means \\(\\mu = (2.5, 0.5, 2.5, 4.5)\\):</p> <p>a) Calculate the unit deviances for Poisson</p> <p>b) Calculate the total deviance</p> <p>c) What does a high deviance indicate?</p> <p>Exercise 3: Overdispersion</p> <p>You fit a Poisson model and get Pearson \\(\\chi^2 = 180\\) with 90 degrees of freedom.</p> <p>a) Calculate the dispersion ratio</p> <p>b) Is this evidence of overdispersion?</p> <p>c) How would QuasiPoisson adjust the standard errors?</p> <p>Exercise 4: Tweedie</p> <p>A Tweedie model with \\(p = 1.6\\) and \\(\\mu = 100\\) has \\(\\phi = 2\\).</p> <p>a) Calculate \\(V(\\mu)\\)</p> <p>b) Calculate \\(\\text{Var}(Y)\\)</p> <p>c) What's the CV?</p>"},{"location":"theory/families/#further-reading","title":"Further Reading","text":"<ul> <li>McCullagh, P. and Nelder, J.A. (1989). Generalized Linear Models, 2nd ed. Chapter 2.</li> <li>J\u00f8rgensen, B. (1997). The Theory of Dispersion Models. \u2014 Comprehensive treatment of Tweedie</li> <li>Hilbe, J.M. (2014). Modeling Count Data. \u2014 Poisson, NegBin, and overdispersion</li> </ul>"},{"location":"theory/glm-intro/","title":"Introduction to Generalized Linear Models","text":"<p>This chapter provides a comprehensive mathematical foundation for understanding Generalized Linear Models (GLMs). We build from first principles, starting with ordinary linear regression and systematically extending it to the full GLM framework. By the end of this chapter, you will understand not just what GLMs are, but why they work the way they do.</p> <p>Prerequisites: Basic calculus (derivatives, chain rule), linear algebra (matrix multiplication, inverses), and probability (expected value, variance). No prior statistics knowledge is assumed.</p>"},{"location":"theory/glm-intro/#part-1-from-linear-regression-to-glms","title":"Part 1: From Linear Regression to GLMs","text":""},{"location":"theory/glm-intro/#11-ordinary-linear-regression-a-review","title":"1.1 Ordinary Linear Regression: A Review","text":"<p>Before diving into GLMs, let's establish a solid foundation with ordinary linear regression (OLS). Even if you've seen this before, this section introduces the notation and concepts we'll generalize later.</p>"},{"location":"theory/glm-intro/#the-model","title":"The Model","text":"<p>In linear regression, we model a continuous response variable \\(Y\\) as a linear function of predictors plus random noise:</p> \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{p-1} x_{i,p-1} + \\varepsilon_i \\] <p>where:</p> <ul> <li>\\(Y_i\\) is the response for observation \\(i\\) (what we're trying to predict)</li> <li>\\(x_{i1}, x_{i2}, \\ldots\\) are the predictor values for observation \\(i\\)</li> <li>\\(\\beta_0, \\beta_1, \\ldots\\) are the coefficients (what we estimate)</li> <li>\\(\\varepsilon_i\\) is the random error term</li> </ul> <p>We can write this more compactly using matrix notation. Define:</p> \\[ \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \\quad \\mathbf{X} = \\begin{pmatrix}  1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1,p-1} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{n,p-1} \\end{pmatrix}, \\quad \\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\end{pmatrix} \\] <p>The column of 1s in \\(\\mathbf{X}\\) corresponds to the intercept \\(\\beta_0\\). Now we can write:</p> \\[ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\] <p>Matrix Multiplication Refresher</p> <p>The product \\(\\mathbf{X}\\boldsymbol{\\beta}\\) gives an \\(n \\times 1\\) vector where each element is:</p> \\[(\\mathbf{X}\\boldsymbol{\\beta})_i = \\sum_{j=0}^{p-1} X_{ij} \\beta_j = \\beta_0 + \\beta_1 x_{i1} + \\cdots\\] <p>This is exactly the linear combination we want.</p>"},{"location":"theory/glm-intro/#the-assumptions","title":"The Assumptions","text":"<p>Linear regression makes several key assumptions:</p> <ol> <li>Linearity: \\(E(Y_i | \\mathbf{x}_i) = \\mathbf{x}_i^T \\boldsymbol{\\beta}\\) (the mean is a linear function of predictors)</li> <li>Normality: \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\) (errors are normally distributed)</li> <li>Homoscedasticity: \\(\\text{Var}(\\varepsilon_i) = \\sigma^2\\) (constant variance)</li> <li>Independence: The \\(\\varepsilon_i\\) are independent of each other</li> </ol> <p>Under these assumptions, we can write:</p> \\[ Y_i \\sim N(\\mu_i, \\sigma^2) \\quad \\text{where} \\quad \\mu_i = \\mathbf{x}_i^T \\boldsymbol{\\beta} \\] <p>This says: \"The response \\(Y_i\\) follows a normal distribution with mean \\(\\mu_i\\) (which depends on the predictors) and constant variance \\(\\sigma^2\\).\"</p>"},{"location":"theory/glm-intro/#estimation-least-squares","title":"Estimation: Least Squares","text":"<p>The ordinary least squares (OLS) estimator minimizes the sum of squared residuals:</p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = \\arg\\min_{\\boldsymbol{\\beta}} \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2 \\] <p>Let's derive the solution step by step. Define the sum of squares:</p> \\[ S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\] <p>Expanding this (using \\((A-B)^T(A-B) = A^TA - 2A^TB + B^TB\\)):</p> \\[ S(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} \\] <p>Taking the derivative with respect to \\(\\boldsymbol{\\beta}\\):</p> \\[ \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} \\] <p>Matrix Calculus Rule</p> <p>For a vector \\(\\mathbf{x}\\) and matrix \\(\\mathbf{A}\\):</p> <ul> <li>\\(\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{a}^T\\mathbf{x}) = \\mathbf{a}\\)</li> <li>\\(\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\) (if \\(\\mathbf{A}\\) is symmetric)</li> </ul> <p>Setting the derivative to zero and solving:</p> \\[ -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0} \\] \\[ \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y} \\] \\[ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\] <p>This is the famous normal equations solution.</p> <p>Why 'Normal Equations'?</p> <p>The name comes from the fact that the residual vector \\(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) is orthogonal (perpendicular, or \"normal\" in geometric terms) to the column space of \\(\\mathbf{X}\\). Geometrically, we're projecting \\(\\mathbf{y}\\) onto the space spanned by the columns of \\(\\mathbf{X}\\).</p>"},{"location":"theory/glm-intro/#a-worked-example","title":"A Worked Example","text":"<p>Let's do a tiny example by hand. Suppose we have 3 observations:</p> \\(i\\) \\(y_i\\) \\(x_{i1}\\) 1 2 1 2 4 2 3 5 3 <p>Our model is \\(Y = \\beta_0 + \\beta_1 x_1 + \\varepsilon\\).</p> <p>The matrices are:</p> \\[ \\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 5 \\end{pmatrix}, \\quad \\mathbf{X} = \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{pmatrix}, \\quad \\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix} \\] <p>Computing \\(\\mathbf{X}^T\\mathbf{X}\\):</p> \\[ \\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{pmatrix} = \\begin{pmatrix} 3 &amp; 6 \\\\ 6 &amp; 14 \\end{pmatrix} \\] <p>Computing \\(\\mathbf{X}^T\\mathbf{y}\\):</p> \\[ \\mathbf{X}^T\\mathbf{y} = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 25 \\end{pmatrix} \\] <p>Computing \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\):</p> <p>For a 2\u00d72 matrix \\(\\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}\\), the inverse is \\(\\frac{1}{ad-bc}\\begin{pmatrix} d &amp; -b \\\\ -c &amp; a \\end{pmatrix}\\).</p> \\[ (\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{3 \\cdot 14 - 6 \\cdot 6}\\begin{pmatrix} 14 &amp; -6 \\\\ -6 &amp; 3 \\end{pmatrix} = \\frac{1}{6}\\begin{pmatrix} 14 &amp; -6 \\\\ -6 &amp; 3 \\end{pmatrix} \\] <p>Finally:</p> \\[ \\hat{\\boldsymbol{\\beta}} = \\frac{1}{6}\\begin{pmatrix} 14 &amp; -6 \\\\ -6 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 11 \\\\ 25 \\end{pmatrix} = \\frac{1}{6}\\begin{pmatrix} 154 - 150 \\\\ -66 + 75 \\end{pmatrix} = \\frac{1}{6}\\begin{pmatrix} 4 \\\\ 9 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 3/2 \\end{pmatrix} \\] <p>So \\(\\hat{\\beta}_0 = 0.667\\) and \\(\\hat{\\beta}_1 = 1.5\\). The fitted line is \\(\\hat{y} = 0.667 + 1.5x\\).</p>"},{"location":"theory/glm-intro/#12-the-limitations-of-linear-regression","title":"1.2 The Limitations of Linear Regression","text":"<p>Linear regression works beautifully for continuous data that can reasonably be assumed normal. But many real-world problems involve data that violate these assumptions:</p>"},{"location":"theory/glm-intro/#problem-1-count-data","title":"Problem 1: Count Data","text":"<p>Example: Number of insurance claims per policy, number of website visits, number of defects in manufacturing.</p> <p>Counts are:</p> <ul> <li>Non-negative integers (0, 1, 2, 3, ...)</li> <li>Often right-skewed (many zeros, few large values)</li> <li>Variance typically increases with the mean</li> </ul> <p>Linear regression can predict negative counts (nonsense!) and assumes constant variance (wrong!).</p> <p>Concrete example: If we model claim counts with linear regression and get \\(\\hat{y} = -0.5\\), what does that mean? Negative half a claim? This is meaningless.</p>"},{"location":"theory/glm-intro/#problem-2-binary-data","title":"Problem 2: Binary Data","text":"<p>Example: Did a customer churn? (yes/no), Did a patient survive? (yes/no), Is this email spam? (yes/no)</p> <p>Binary outcomes are:</p> <ul> <li>Either 0 or 1</li> <li>What we really want to model is \\(P(Y=1)\\), a probability between 0 and 1</li> </ul> <p>Linear regression can predict probabilities outside [0, 1] (impossible!) and the normality assumption makes no sense for binary data.</p> <p>Concrete example: If we model churn with linear regression and get \\(\\hat{y} = 1.3\\) for a customer, what's their churn probability? 130%? Impossible.</p>"},{"location":"theory/glm-intro/#problem-3-positive-continuous-data","title":"Problem 3: Positive Continuous Data","text":"<p>Example: Insurance claim amounts, time until failure, income.</p> <p>Positive continuous data is:</p> <ul> <li>Strictly positive</li> <li>Often right-skewed</li> <li>Variance often proportional to mean (or mean squared)</li> </ul> <p>Linear regression can predict negative values and assumes the wrong variance structure.</p> <p>Concrete example: Claim amounts can't be negative, but a linear model might predict \\(\\hat{y} = -\\$500\\). Also, larger claims are more variable than smaller ones\u2014the variance isn't constant.</p>"},{"location":"theory/glm-intro/#13-the-glm-solution-three-components","title":"1.3 The GLM Solution: Three Components","text":"<p>GLMs solve these problems elegantly by generalizing linear regression in three ways. A GLM has three components:</p>"},{"location":"theory/glm-intro/#component-1-random-component-distribution-family","title":"Component 1: Random Component (Distribution Family)","text":"<p>Instead of assuming \\(Y \\sim N(\\mu, \\sigma^2)\\), we allow \\(Y\\) to follow any distribution from the exponential family, which includes:</p> Distribution Typical Use Case Support Normal (Gaussian) Continuous data \\((-\\infty, +\\infty)\\) Poisson Count data \\(\\{0, 1, 2, \\ldots\\}\\) Binomial Binary/proportion data \\(\\{0, 1\\}\\) or \\([0, 1]\\) Gamma Positive continuous data \\((0, +\\infty)\\) Inverse Gaussian Positive continuous data \\((0, +\\infty)\\) Negative Binomial Overdispersed counts \\(\\{0, 1, 2, \\ldots\\}\\) <p>Each distribution has a variance function \\(V(\\mu)\\) that describes how variance relates to the mean:</p> Distribution Variance Function \\(V(\\mu)\\) Meaning Normal \\(1\\) (constant) Variance doesn't depend on mean Poisson \\(\\mu\\) Higher mean \u2192 higher variance Binomial \\(\\mu(1-\\mu)\\) Max variance at \\(\\mu=0.5\\) Gamma \\(\\mu^2\\) Variance grows quadratically"},{"location":"theory/glm-intro/#component-2-systematic-component-linear-predictor","title":"Component 2: Systematic Component (Linear Predictor)","text":"<p>We keep the linear structure, defining the linear predictor:</p> \\[ \\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots = \\mathbf{x}_i^T \\boldsymbol{\\beta} \\] <p>This is exactly like linear regression\u2014a linear combination of predictors. The \\(\\eta_i\\) can be any real number.</p>"},{"location":"theory/glm-intro/#component-3-link-function","title":"Component 3: Link Function","text":"<p>The innovation of GLMs is the link function \\(g(\\cdot)\\) that connects the mean \\(\\mu\\) to the linear predictor \\(\\eta\\):</p> \\[ g(\\mu_i) = \\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta} \\] <p>Or equivalently, the inverse link (also called the mean function):</p> \\[ \\mu_i = g^{-1}(\\eta_i) = g^{-1}(\\mathbf{x}_i^T \\boldsymbol{\\beta}) \\] <p>The link function serves two critical purposes:</p> <p>Purpose 1: Maps the mean to the correct range</p> <p>For Poisson, \\(\\mu &gt; 0\\) (means must be positive). Using \\(g(\\mu) = \\log(\\mu)\\) (log link):</p> <ul> <li>The linear predictor \\(\\eta = \\mathbf{x}^T\\boldsymbol{\\beta}\\) can be any real number</li> <li>But \\(\\mu = e^\\eta &gt; 0\\) is always positive</li> </ul> <p>For Binomial, \\(\\mu \\in (0, 1)\\) (probabilities). Using \\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\) (logit link):</p> <ul> <li>The linear predictor \\(\\eta\\) can be any real number</li> <li>But \\(\\mu = \\frac{e^\\eta}{1+e^\\eta} \\in (0, 1)\\) is always a valid probability</li> </ul> <p>Purpose 2: Enables meaningful interpretation</p> <p>With log link, \\(\\eta = \\log(\\mu)\\), so:</p> \\[ \\log(\\mu) = \\beta_0 + \\beta_1 x_1 + \\cdots \\] <p>A one-unit increase in \\(x_1\\) increases \\(\\log(\\mu)\\) by \\(\\beta_1\\), which means it multiplies \\(\\mu\\) by \\(e^{\\beta_1}\\).</p> <p>Example: If \\(\\beta_1 = 0.2\\), then each unit increase in \\(x_1\\) multiplies the expected count by \\(e^{0.2} \\approx 1.22\\), a 22% increase.</p>"},{"location":"theory/glm-intro/#14-putting-it-together-the-full-glm","title":"1.4 Putting It Together: The Full GLM","text":"<p>A GLM specifies:</p> <ol> <li>Distribution: \\(Y_i \\sim \\text{SomeDistribution}(\\mu_i)\\) with variance \\(\\text{Var}(Y_i) = \\phi \\cdot V(\\mu_i)\\)</li> <li>Linear predictor: \\(\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}\\)</li> <li>Link: \\(g(\\mu_i) = \\eta_i\\)</li> </ol> <p>The parameter \\(\\phi\\) is called the dispersion parameter. For some families (Poisson, Binomial) it equals 1 by definition. For others (Gaussian, Gamma) it must be estimated.</p> <p>Example: Poisson Regression for Count Data</p> <p>For modeling counts (like insurance claims):</p> <ul> <li>Distribution: \\(Y_i \\sim \\text{Poisson}(\\mu_i)\\)</li> <li>Variance function: \\(V(\\mu) = \\mu\\), and \\(\\phi = 1\\)</li> <li>Link function: \\(g(\\mu) = \\log(\\mu)\\) (the \"log link\")</li> </ul> <p>This means:</p> \\[ \\log(\\mu_i) = \\beta_0 + \\beta_1 x_{i1} + \\cdots \\] \\[ \\mu_i = \\exp(\\beta_0 + \\beta_1 x_{i1} + \\cdots) \\] <p>The mean is always positive (good for counts!), and the variance equals the mean (a property of the Poisson distribution).</p> <p>Example: Logistic Regression for Binary Data</p> <p>For modeling binary outcomes (like customer churn):</p> <ul> <li>Distribution: \\(Y_i \\sim \\text{Bernoulli}(\\mu_i)\\) where \\(\\mu_i = P(Y_i = 1)\\)</li> <li>Variance function: \\(V(\\mu) = \\mu(1-\\mu)\\), and \\(\\phi = 1\\)</li> <li>Link function: \\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\) (the \"logit link\")</li> </ul> <p>This means:</p> \\[ \\log\\frac{\\mu_i}{1-\\mu_i} = \\beta_0 + \\beta_1 x_{i1} + \\cdots \\] <p>The quantity \\(\\frac{\\mu}{1-\\mu}\\) is called the odds. If \\(\\mu = 0.75\\), the odds are \\(\\frac{0.75}{0.25} = 3\\) (\"3 to 1 in favor\"). </p> <p>The \\(\\log\\frac{\\mu}{1-\\mu}\\) is the log-odds. Solving for \\(\\mu\\):</p> \\[ \\mu_i = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\frac{1}{1 + e^{-\\eta_i}} \\] <p>This is the famous logistic function (also called sigmoid), which maps any real number to the interval (0, 1)\u2014perfect for probabilities!</p> <pre><code>\u03bc\n1 |                 ___________\n  |              __/\n  |           __/\n  |        __/\n  |     __/\n0 |____/\n  +---------------------------- \u03b7\n      -4  -2   0   2   4\n</code></pre> <p>The logistic function looks like an S-curve: very small \\(\\eta\\) gives \\(\\mu \\approx 0\\), very large \\(\\eta\\) gives \\(\\mu \\approx 1\\), and \\(\\eta = 0\\) gives \\(\\mu = 0.5\\).</p>"},{"location":"theory/glm-intro/#part-2-the-exponential-family-foundation","title":"Part 2: The Exponential Family Foundation","text":"<p>Understanding why certain distributions work well in GLMs requires understanding the exponential family. This section develops the mathematical theory that underlies everything. It's more technical, but understanding it will give you deep insight into how GLMs work.</p>"},{"location":"theory/glm-intro/#21-what-is-the-exponential-family","title":"2.1 What is the Exponential Family?","text":"<p>A probability distribution belongs to the exponential family if its probability density (or mass) function can be written as:</p> \\[ f(y; \\theta, \\phi) = \\exp\\left(\\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi)\\right) \\] <p>where:</p> <ul> <li>\\(\\theta\\) is the canonical parameter (related to the mean)</li> <li>\\(\\phi\\) is the dispersion parameter (scale parameter)</li> <li>\\(a(\\phi)\\) is typically just \\(\\phi\\) (we'll use this simplification)</li> <li>\\(b(\\theta)\\) is the cumulant function (determines the distribution)</li> <li>\\(c(y, \\phi)\\) is the normalizing term (ensures probabilities sum/integrate to 1)</li> </ul> <p>Why This Form Matters</p> <p>This might look like arbitrary notation, but this specific form has powerful mathematical properties:</p> <ol> <li>Derivatives of \\(b(\\theta)\\) give us moments of the distribution</li> <li>The likelihood equations have a nice form</li> <li>The canonical link simplifies everything further</li> </ol>"},{"location":"theory/glm-intro/#22-key-properties-of-the-exponential-family","title":"2.2 Key Properties of the Exponential Family","text":"<p>Here are the key results (we'll prove them):</p> <p>Property 1: Mean equals derivative of b</p> \\[ E(Y) = \\mu = b'(\\theta) = \\frac{db(\\theta)}{d\\theta} \\] <p>Property 2: Variance from second derivative</p> \\[ \\text{Var}(Y) = a(\\phi) \\cdot b''(\\theta) = \\phi \\cdot V(\\mu) \\] <p>where \\(V(\\mu) = b''(\\theta)\\) is the variance function, expressed in terms of \\(\\mu\\).</p> <p>Property 3: Canonical link</p> <p>The canonical link is defined by \\(g(\\mu) = \\theta\\). This directly links the mean to the canonical parameter.</p> <p>Let's prove Property 1 and 2.</p>"},{"location":"theory/glm-intro/#proof-of-properties-1-and-2","title":"Proof of Properties 1 and 2","text":"<p>The density must integrate to 1:</p> \\[ \\int f(y; \\theta, \\phi) dy = 1 \\] <p>Substituting the exponential family form:</p> \\[ \\int \\exp\\left(\\frac{y\\theta - b(\\theta)}{\\phi} + c(y, \\phi)\\right) dy = 1 \\] <p>This can be rewritten as:</p> \\[ \\exp\\left(-\\frac{b(\\theta)}{\\phi}\\right) \\int \\exp\\left(\\frac{y\\theta}{\\phi} + c(y, \\phi)\\right) dy = 1 \\] <p>Taking the derivative with respect to \\(\\theta\\) of both sides:</p> \\[ \\frac{d}{d\\theta}\\left[\\int f(y; \\theta, \\phi) dy\\right] = \\frac{d}{d\\theta}[1] = 0 \\] <p>By Leibniz's rule (swapping derivative and integral):</p> \\[ \\int \\frac{\\partial f(y; \\theta, \\phi)}{\\partial \\theta} dy = 0 \\] <p>Now, \\(\\frac{\\partial}{\\partial \\theta}\\left[\\frac{y\\theta - b(\\theta)}{\\phi}\\right] = \\frac{y - b'(\\theta)}{\\phi}\\), so:</p> \\[ \\frac{\\partial f}{\\partial \\theta} = f(y; \\theta, \\phi) \\cdot \\frac{y - b'(\\theta)}{\\phi} \\] <p>Therefore:</p> \\[ \\int f(y) \\cdot \\frac{y - b'(\\theta)}{\\phi} dy = 0 \\] \\[ \\frac{1}{\\phi}\\left[\\int y \\cdot f(y) dy - b'(\\theta) \\int f(y) dy\\right] = 0 \\] \\[ E(Y) - b'(\\theta) \\cdot 1 = 0 \\] \\[ E(Y) = b'(\\theta) = \\mu \\quad \\checkmark \\] <p>For the variance, differentiate again with respect to \\(\\theta\\). After similar calculations:</p> \\[ \\text{Var}(Y) = \\phi \\cdot b''(\\theta) \\quad \\checkmark \\]"},{"location":"theory/glm-intro/#23-examples-deriving-variance-functions","title":"2.3 Examples: Deriving Variance Functions","text":"<p>Let's verify these properties for specific distributions by putting them in exponential family form.</p>"},{"location":"theory/glm-intro/#poisson-distribution","title":"Poisson Distribution","text":"<p>The Poisson probability mass function is:</p> \\[ P(Y = y) = \\frac{\\mu^y e^{-\\mu}}{y!} \\quad \\text{for } y = 0, 1, 2, \\ldots \\] <p>Step 1: Rewrite using exponential</p> \\[ P(Y = y) = \\exp\\left(\\log\\left(\\frac{\\mu^y e^{-\\mu}}{y!}\\right)\\right) = \\exp\\left(y\\log\\mu - \\mu - \\log(y!)\\right) \\] <p>Step 2: Match to exponential family form</p> <p>Comparing with \\(\\exp\\left(\\frac{y\\theta - b(\\theta)}{\\phi} + c(y, \\phi)\\right)\\):</p> <ul> <li>\\(\\theta = \\log\\mu\\) (canonical parameter)</li> <li>\\(b(\\theta) = e^\\theta = \\mu\\) (since \\(\\theta = \\log\\mu\\) means \\(\\mu = e^\\theta\\))</li> <li>\\(\\phi = 1\\) (no separate dispersion for Poisson)</li> <li>\\(c(y, \\phi) = -\\log(y!)\\)</li> </ul> <p>Step 3: Verify properties</p> <ul> <li>\\(b'(\\theta) = \\frac{d}{d\\theta}e^\\theta = e^\\theta = \\mu\\) \u2713 (confirms \\(E(Y) = \\mu\\))</li> <li>\\(b''(\\theta) = \\frac{d^2}{d\\theta^2}e^\\theta = e^\\theta = \\mu\\), so \\(V(\\mu) = \\mu\\) \u2713 (variance equals mean)</li> <li>Canonical link: \\(g(\\mu) = \\theta = \\log\\mu\\) (the log link) \u2713</li> </ul>"},{"location":"theory/glm-intro/#bernoulli-binomial-with-n1-distribution","title":"Bernoulli (Binomial with n=1) Distribution","text":"<p>The Bernoulli PMF is:</p> \\[ P(Y = y) = \\mu^y (1-\\mu)^{1-y} \\quad \\text{for } y \\in \\{0, 1\\} \\] <p>Step 1: Rewrite</p> \\[ P(Y = y) = \\exp\\left(y\\log\\mu + (1-y)\\log(1-\\mu)\\right) \\] \\[ = \\exp\\left(y\\log\\mu - y\\log(1-\\mu) + \\log(1-\\mu)\\right) \\] \\[ = \\exp\\left(y\\log\\frac{\\mu}{1-\\mu} + \\log(1-\\mu)\\right) \\] <p>Step 2: Match to exponential family form</p> <ul> <li>\\(\\theta = \\log\\frac{\\mu}{1-\\mu}\\) (log-odds)</li> <li>\\(b(\\theta) = \\log(1 + e^\\theta)\\) (since \\(\\log(1-\\mu) = -\\log(1+e^\\theta)\\) when \\(\\theta = \\log\\frac{\\mu}{1-\\mu}\\))</li> <li>\\(\\phi = 1\\)</li> <li>\\(c(y, \\phi) = 0\\)</li> </ul> <p>Step 3: Verify properties</p> <p>To check \\(b'(\\theta) = \\mu\\):</p> \\[ b'(\\theta) = \\frac{e^\\theta}{1+e^\\theta} \\] <p>And indeed, if \\(\\theta = \\log\\frac{\\mu}{1-\\mu}\\), then \\(e^\\theta = \\frac{\\mu}{1-\\mu}\\), so:</p> \\[ b'(\\theta) = \\frac{\\mu/(1-\\mu)}{1 + \\mu/(1-\\mu)} = \\frac{\\mu/(1-\\mu)}{(1-\\mu+\\mu)/(1-\\mu)} = \\frac{\\mu/(1-\\mu)}{1/(1-\\mu)} = \\mu \\quad \\checkmark \\] <p>For the variance:</p> \\[ b''(\\theta) = \\frac{e^\\theta}{(1+e^\\theta)^2} = \\frac{\\mu/(1-\\mu)}{(1/(1-\\mu))^2} = \\mu(1-\\mu) \\quad \\checkmark \\] <p>So \\(V(\\mu) = \\mu(1-\\mu)\\), which is the variance of a Bernoulli distribution.</p> <p>Canonical link: \\(g(\\mu) = \\theta = \\log\\frac{\\mu}{1-\\mu}\\) (logit) \u2713</p>"},{"location":"theory/glm-intro/#normal-gaussian-distribution","title":"Normal (Gaussian) Distribution","text":"<p>The Normal PDF is:</p> \\[ f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) \\] <p>Step 1: Expand the square</p> \\[ -\\frac{(y-\\mu)^2}{2\\sigma^2} = -\\frac{y^2 - 2y\\mu + \\mu^2}{2\\sigma^2} = \\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2} - \\frac{y^2}{2\\sigma^2} \\] <p>So:</p> \\[ f(y) = \\exp\\left(\\frac{y\\mu - \\mu^2/2}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)\\right) \\] <p>Step 2: Match to exponential family form</p> <ul> <li>\\(\\theta = \\mu\\) (canonical parameter equals mean!)</li> <li>\\(b(\\theta) = \\theta^2/2 = \\mu^2/2\\)</li> <li>\\(\\phi = \\sigma^2\\) (dispersion is variance)</li> <li>\\(c(y, \\phi) = -\\frac{y^2}{2\\phi} - \\frac{1}{2}\\log(2\\pi\\phi)\\)</li> </ul> <p>Step 3: Verify</p> <ul> <li>\\(b'(\\theta) = \\theta = \\mu\\) \u2713</li> <li>\\(b''(\\theta) = 1\\), so \\(V(\\mu) = 1\\) \u2713 (constant variance)</li> <li>Canonical link: \\(g(\\mu) = \\theta = \\mu\\) (identity link) \u2713</li> </ul>"},{"location":"theory/glm-intro/#part-3-maximum-likelihood-estimation","title":"Part 3: Maximum Likelihood Estimation","text":"<p>Now that we understand the GLM structure, how do we estimate the parameters \\(\\boldsymbol{\\beta}\\)? The answer is maximum likelihood estimation (MLE).</p>"},{"location":"theory/glm-intro/#31-the-likelihood-principle","title":"3.1 The Likelihood Principle","text":"<p>The likelihood function answers: \"Given the data we observed, how likely is it that the true parameters are \\(\\boldsymbol{\\beta}\\)?\"</p> <p>Mathematically, if we observe data \\(y_1, \\ldots, y_n\\) (assumed independent), the likelihood is:</p> \\[ L(\\boldsymbol{\\beta}) = \\prod_{i=1}^n f(y_i; \\mu_i(\\boldsymbol{\\beta}), \\phi) \\] <p>where \\(\\mu_i(\\boldsymbol{\\beta}) = g^{-1}(\\mathbf{x}_i^T\\boldsymbol{\\beta})\\) is the predicted mean for observation \\(i\\).</p> <p>The maximum likelihood estimate \\(\\hat{\\boldsymbol{\\beta}}\\) is the value that maximizes \\(L(\\boldsymbol{\\beta})\\).</p>"},{"location":"theory/glm-intro/#32-the-log-likelihood","title":"3.2 The Log-Likelihood","text":"<p>Working with products is cumbersome, so we take the logarithm:</p> \\[ \\ell(\\boldsymbol{\\beta}) = \\log L(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\log f(y_i; \\mu_i, \\phi) \\] <p>Since \\(\\log\\) is monotonically increasing, maximizing \\(\\ell\\) is equivalent to maximizing \\(L\\).</p> <p>For exponential family distributions:</p> \\[ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[\\frac{y_i\\theta_i - b(\\theta_i)}{\\phi} + c(y_i, \\phi)\\right] \\] <p>The \\(c(y_i, \\phi)\\) term doesn't depend on \\(\\boldsymbol{\\beta}\\), so for optimization we can ignore it:</p> \\[ \\ell(\\boldsymbol{\\beta}) \\propto \\sum_{i=1}^n \\frac{y_i\\theta_i - b(\\theta_i)}{\\phi} \\]"},{"location":"theory/glm-intro/#33-the-score-function","title":"3.3 The Score Function","text":"<p>To find the maximum, we take the derivative of \\(\\ell\\) with respect to \\(\\boldsymbol{\\beta}\\) and set it to zero.</p> <p>The score function is:</p> \\[ \\mathbf{U}(\\boldsymbol{\\beta}) = \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}} = \\begin{pmatrix} \\frac{\\partial \\ell}{\\partial \\beta_0} \\\\ \\frac{\\partial \\ell}{\\partial \\beta_1} \\\\ \\vdots \\end{pmatrix} \\] <p>Let's compute \\(\\frac{\\partial \\ell}{\\partial \\beta_j}\\) using the chain rule:</p> \\[ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial \\ell_i}{\\partial \\theta_i} \\cdot \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\cdot \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\cdot \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\] <p>Let's compute each piece:</p> <p>Piece 1: \\(\\frac{\\partial \\ell_i}{\\partial \\theta_i}\\)</p> \\[ \\ell_i = \\frac{y_i\\theta_i - b(\\theta_i)}{\\phi} \\] \\[ \\frac{\\partial \\ell_i}{\\partial \\theta_i} = \\frac{y_i - b'(\\theta_i)}{\\phi} = \\frac{y_i - \\mu_i}{\\phi} \\] <p>Piece 2: \\(\\frac{\\partial \\theta_i}{\\partial \\mu_i}\\)</p> <p>Since \\(\\mu = b'(\\theta)\\), we have \\(\\frac{d\\mu}{d\\theta} = b''(\\theta) = V(\\mu)\\), so:</p> \\[ \\frac{d\\theta}{d\\mu} = \\frac{1}{V(\\mu)} \\] <p>Piece 3: \\(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\)</p> <p>Since \\(\\mu = g^{-1}(\\eta)\\), we have:</p> \\[ \\frac{d\\mu}{d\\eta} = \\frac{d g^{-1}(\\eta)}{d\\eta} = \\frac{1}{g'(\\mu)} \\] <p>(This uses the inverse function derivative rule: if \\(\\eta = g(\\mu)\\), then \\(\\frac{d\\mu}{d\\eta} = 1/\\frac{d\\eta}{d\\mu} = 1/g'(\\mu)\\).)</p> <p>Piece 4: \\(\\frac{\\partial \\eta_i}{\\partial \\beta_j}\\)</p> <p>Since \\(\\eta_i = \\mathbf{x}_i^T\\boldsymbol{\\beta} = \\sum_k x_{ik}\\beta_k\\):</p> \\[ \\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij} \\] <p>Putting it all together:</p> \\[ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{y_i - \\mu_i}{\\phi} \\cdot \\frac{1}{V(\\mu_i)} \\cdot \\frac{1}{g'(\\mu_i)} \\cdot x_{ij} \\] \\[ = \\sum_{i=1}^n \\frac{(y_i - \\mu_i) x_{ij}}{\\phi \\cdot V(\\mu_i) \\cdot g'(\\mu_i)} \\] <p>Setting this to zero for each \\(j\\) gives the score equations:</p> \\[ \\sum_{i=1}^n \\frac{(y_i - \\mu_i) x_{ij}}{V(\\mu_i) \\cdot g'(\\mu_i)} = 0 \\quad \\text{for } j = 0, 1, \\ldots, p-1 \\]"},{"location":"theory/glm-intro/#34-why-we-need-iteration","title":"3.4 Why We Need Iteration","text":"<p>Unlike linear regression, these equations are nonlinear in \\(\\boldsymbol{\\beta}\\).</p> <p>For linear regression: - \\(\\mu_i = \\eta_i = \\mathbf{x}_i^T\\boldsymbol{\\beta}\\) (linear in \\(\\boldsymbol{\\beta}\\)) - Score equations are linear \u2192 closed-form solution</p> <p>For GLMs: - \\(\\mu_i = g^{-1}(\\mathbf{x}_i^T\\boldsymbol{\\beta})\\) (nonlinear in \\(\\boldsymbol{\\beta}\\) through \\(g^{-1}\\)) - \\(V(\\mu_i)\\) and \\(g'(\\mu_i)\\) also depend on \\(\\boldsymbol{\\beta}\\) - Score equations are nonlinear \u2192 need iterative solution</p> <p>Example for Poisson: $$ \\mu_i = e^{\\mathbf{x}_i^T\\boldsymbol{\\beta}} $$</p> <p>The score equation involves \\(e^{\\mathbf{x}_i^T\\boldsymbol{\\beta}}\\) terms\u2014clearly nonlinear!</p>"},{"location":"theory/glm-intro/#part-4-deviance-and-model-assessment","title":"Part 4: Deviance and Model Assessment","text":"<p>Before moving to the estimation algorithm, let's understand how we measure model fit.</p>"},{"location":"theory/glm-intro/#41-the-concept-of-deviance","title":"4.1 The Concept of Deviance","text":"<p>The deviance measures how much our model deviates from a perfect fit. It's defined as:</p> \\[ D = 2[\\ell(\\text{saturated}) - \\ell(\\text{fitted})] \\] <p>where: - Saturated model: A model with one parameter per observation, achieving \\(\\hat{\\mu}_i = y_i\\) (perfect fit to data) - Fitted model: Our actual model with \\(p\\) parameters</p> <p>The deviance is always non-negative (the saturated model has the highest possible likelihood).</p>"},{"location":"theory/glm-intro/#42-unit-deviance","title":"4.2 Unit Deviance","text":"<p>Each observation contributes to the total deviance:</p> \\[ D = \\sum_{i=1}^n d(y_i, \\mu_i) \\] <p>where \\(d(y, \\mu)\\) is the unit deviance.</p> Family Unit Deviance \\(d(y, \\mu)\\) Gaussian \\((y - \\mu)^2\\) Poisson \\(2[y\\log(y/\\mu) - (y - \\mu)]\\) Binomial \\(2[y\\log(y/\\mu) + (1-y)\\log((1-y)/(1-\\mu))]\\) Gamma \\(2[-\\log(y/\\mu) + (y - \\mu)/\\mu]\\) <p>Deviance for Gaussian</p> <p>For Gaussian, the deviance is \\(D = \\sum_i (y_i - \\mu_i)^2\\), the residual sum of squares! </p> <p>This is why linear regression minimizes sum of squares\u2014it's equivalent to maximizing likelihood for normal data.</p>"},{"location":"theory/glm-intro/#43-using-deviance","title":"4.3 Using Deviance","text":"<p>Model comparison: For nested models (Model 1 is a special case of Model 2):</p> \\[ D_1 - D_2 \\sim \\chi^2_{p_2 - p_1} \\quad \\text{(approximately)} \\] <p>This is the likelihood ratio test.</p> <p>Assessing fit: The residual deviance should be roughly equal to its degrees of freedom (\\(n - p\\)). If deviance &gt;&gt; \\(n - p\\), the model fits poorly.</p>"},{"location":"theory/glm-intro/#part-5-summary-and-next-steps","title":"Part 5: Summary and Next Steps","text":"<p>We've built up GLMs from first principles:</p> <ol> <li>Linear regression works for normal data but fails for counts, binary data, etc.</li> <li>GLMs generalize by allowing different distributions (families) and using link functions</li> <li>The exponential family provides the mathematical foundation with nice properties</li> <li>Maximum likelihood gives us the estimation framework</li> <li>Deviance measures model fit</li> </ol> <p>What's next: The IRLS Algorithm chapter shows how we actually solve the nonlinear score equations efficiently by converting them to iterated weighted least squares problems.</p>"},{"location":"theory/glm-intro/#exercises","title":"Exercises","text":"<p>Exercise 1: Link Function Practice</p> <p>For each scenario, which link function is most appropriate and why?</p> <p>a) Modeling the probability a customer will click an ad b) Modeling the number of cars passing a sensor per hour c) Modeling a student's test score (0-100 continuous) d) Modeling insurance claim severity (positive dollar amounts)</p> <p>Exercise 2: Exponential Family - Gamma Distribution</p> <p>The Gamma PDF is:</p> \\[f(y; \\mu, \\nu) = \\frac{1}{\\Gamma(\\nu)}\\left(\\frac{\\nu}{\\mu}\\right)^\\nu y^{\\nu-1} e^{-\\nu y/\\mu}\\] <p>where \\(\\nu\\) is the shape parameter.</p> <p>a) Put this in exponential family form. What is \\(\\theta\\)? b) Find \\(b(\\theta)\\) and compute \\(b'(\\theta)\\) to verify \\(E(Y) = \\mu\\) c) Find \\(V(\\mu)\\). What is the canonical link?</p> <p>Exercise 3: Score Equations for Poisson</p> <p>For Poisson regression with log link:</p> <p>a) Write out the log-likelihood explicitly b) Compute \\(\\frac{\\partial \\ell}{\\partial \\beta_j}\\) directly (not using the chain rule result) c) Verify it matches the general formula with \\(V(\\mu) = \\mu\\) and \\(g'(\\mu) = 1/\\mu\\)</p> <p>Exercise 4: Hand Calculation</p> <p>Given the tiny dataset:</p> \\(y\\) \\(x\\) 1 0 3 1 7 2 <p>For a Poisson model \\(\\log(\\mu) = \\beta_0 + \\beta_1 x\\):</p> <p>a) If \\(\\beta_0 = 0.5\\) and \\(\\beta_1 = 0.8\\), what are the fitted means \\(\\mu_i\\)? b) Compute the deviance c) Compute the Pearson residuals \\((y_i - \\mu_i)/\\sqrt{\\mu_i}\\)</p>"},{"location":"theory/glm-intro/#solutions","title":"Solutions","text":"Solution to Exercise 1 <p>a) Logit link - probability must be in (0,1) b) Log link - counts must be positive c) Identity link - continuous on full range (or bounded, could consider) d) Log link - amounts must be positive</p> Solution to Exercise 4 <p>a) \\(\\mu_1 = e^{0.5} \\approx 1.65\\), \\(\\mu_2 = e^{0.5+0.8} \\approx 3.67\\), \\(\\mu_3 = e^{0.5+1.6} \\approx 8.17\\)</p> <p>b) Deviance = \\(2\\sum[y_i\\log(y_i/\\mu_i) - (y_i - \\mu_i)]\\)</p> <p>\\(= 2[(1)\\log(1/1.65) - (1-1.65) + (3)\\log(3/3.67) - (3-3.67) + (7)\\log(7/8.17) - (7-8.17)]\\)</p> <p>\\(= 2[(-0.50 + 0.65) + (-0.60 + 0.67) + (-1.10 + 1.17)]\\)</p> <p>\\(= 2[0.15 + 0.07 + 0.07] = 0.58\\)</p> <p>c) Pearson residuals: \\((1-1.65)/\\sqrt{1.65} \\approx -0.51\\), \\((3-3.67)/\\sqrt{3.67} \\approx -0.35\\), \\((7-8.17)/\\sqrt{8.17} \\approx -0.41\\)</p>"},{"location":"theory/glm-intro/#further-reading","title":"Further Reading","text":"<ul> <li>McCullagh, P. and Nelder, J.A. (1989). Generalized Linear Models, 2nd ed. Chapman &amp; Hall. \u2014 The classic reference.</li> <li>Dobson, A.J. and Barnett, A.G. (2018). An Introduction to Generalized Linear Models, 4th ed. CRC Press. \u2014 More accessible introduction.</li> <li>Wood, S.N. (2017). Generalized Additive Models: An Introduction with R, 2nd ed. CRC Press. \u2014 Extends to nonlinear effects.</li> </ul>"},{"location":"theory/irls/","title":"The IRLS Algorithm","text":"<p>Iteratively Reweighted Least Squares (IRLS) is the workhorse algorithm for fitting GLMs. This chapter provides a complete derivation from first principles, showing exactly why IRLS works and how it's implemented in RustyStats.</p> <p>Prerequisites: You should understand the GLM framework from the previous chapter, including the score equations and Fisher information.</p>"},{"location":"theory/irls/#part-1-the-problem-were-solving","title":"Part 1: The Problem We're Solving","text":""},{"location":"theory/irls/#11-recap-the-score-equations","title":"1.1 Recap: The Score Equations","text":"<p>From maximum likelihood theory, we want to find \\(\\boldsymbol{\\beta}\\) that satisfies the score equations:</p> \\[ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{(y_i - \\mu_i) x_{ij}}{\\phi \\cdot V(\\mu_i) \\cdot g'(\\mu_i)} = 0 \\quad \\text{for } j = 0, 1, \\ldots, p-1 \\] <p>In matrix notation, define the diagonal matrices: - \\(\\mathbf{V} = \\text{diag}(V(\\mu_1), \\ldots, V(\\mu_n))\\) \u2014 variance function values - \\(\\mathbf{G} = \\text{diag}(g'(\\mu_1), \\ldots, g'(\\mu_n))\\) \u2014 link derivatives</p> <p>Then the score equations become:</p> \\[ \\mathbf{U} = \\frac{1}{\\phi} \\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{G}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu}) = \\mathbf{0} \\]"},{"location":"theory/irls/#12-why-we-cant-solve-directly","title":"1.2 Why We Can't Solve Directly","text":"<p>For linear regression (Gaussian family, identity link), \\(\\mu_i = \\mathbf{x}_i^T\\boldsymbol{\\beta}\\) is linear in \\(\\boldsymbol{\\beta}\\), and we get the normal equations with a closed-form solution.</p> <p>For general GLMs, \\(\\mu_i = g^{-1}(\\mathbf{x}_i^T\\boldsymbol{\\beta})\\) is nonlinear in \\(\\boldsymbol{\\beta}\\). For example:</p> <ul> <li>Poisson with log link: \\(\\mu_i = e^{\\mathbf{x}_i^T\\boldsymbol{\\beta}}\\)</li> <li>Binomial with logit link: \\(\\mu_i = \\frac{e^{\\mathbf{x}_i^T\\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^T\\boldsymbol{\\beta}}}\\)</li> </ul> <p>The score equations involve these nonlinear functions, so there's no closed-form solution. We need an iterative method.</p>"},{"location":"theory/irls/#part-2-newton-raphson-and-fisher-scoring","title":"Part 2: Newton-Raphson and Fisher Scoring","text":""},{"location":"theory/irls/#21-newton-raphson-method","title":"2.1 Newton-Raphson Method","text":"<p>Newton-Raphson is a general method for solving equations \\(f(\\mathbf{x}) = \\mathbf{0}\\). The idea is to linearize \\(f\\) around the current estimate and solve the linear approximation.</p> <p>One-dimensional case: To solve \\(f(x) = 0\\):</p> <ol> <li>Start with initial guess \\(x^{(0)}\\)</li> <li>Taylor expand: \\(f(x) \\approx f(x^{(t)}) + f'(x^{(t)})(x - x^{(t)})\\)</li> <li>Set approximation to zero and solve: \\(x^{(t+1)} = x^{(t)} - \\frac{f(x^{(t)})}{f'(x^{(t)})}\\)</li> </ol> <p>Multi-dimensional case: To solve \\(\\mathbf{f}(\\mathbf{x}) = \\mathbf{0}\\):</p> \\[ \\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - \\mathbf{J}^{-1}(\\mathbf{x}^{(t)}) \\mathbf{f}(\\mathbf{x}^{(t)}) \\] <p>where \\(\\mathbf{J}\\) is the Jacobian matrix with \\(J_{jk} = \\frac{\\partial f_j}{\\partial x_k}\\).</p>"},{"location":"theory/irls/#22-newton-raphson-for-maximum-likelihood","title":"2.2 Newton-Raphson for Maximum Likelihood","text":"<p>For maximum likelihood, we want to solve \\(\\mathbf{U}(\\boldsymbol{\\beta}) = \\mathbf{0}\\) where \\(\\mathbf{U} = \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}}\\) is the score.</p> <p>The Jacobian of \\(\\mathbf{U}\\) is the Hessian of \\(\\ell\\):</p> \\[ \\mathbf{H} = \\frac{\\partial \\mathbf{U}}{\\partial \\boldsymbol{\\beta}^T} = \\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} \\] <p>So Newton-Raphson gives:</p> \\[ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\mathbf{H}^{-1}(\\boldsymbol{\\beta}^{(t)}) \\mathbf{U}(\\boldsymbol{\\beta}^{(t)}) \\]"},{"location":"theory/irls/#23-the-problem-with-the-hessian","title":"2.3 The Problem with the Hessian","text":"<p>Computing the Hessian requires second derivatives, which can be: - Complicated to derive - Computationally expensive - Potentially non-positive-definite (causing instability)</p> <p>Fisher's insight: Replace the Hessian with its expected value.</p>"},{"location":"theory/irls/#24-fisher-information","title":"2.4 Fisher Information","text":"<p>The Fisher information matrix is defined as:</p> \\[ \\mathcal{I}(\\boldsymbol{\\beta}) = E\\left[-\\mathbf{H}\\right] = E\\left[-\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T}\\right] \\] <p>Equivalently (under regularity conditions):</p> \\[ \\mathcal{I}(\\boldsymbol{\\beta}) = E\\left[\\mathbf{U}\\mathbf{U}^T\\right] \\] <p>The Fisher information has a beautiful property: it tells us the maximum precision we can achieve when estimating \\(\\boldsymbol{\\beta}\\).</p>"},{"location":"theory/irls/#25-deriving-the-fisher-information-for-glms","title":"2.5 Deriving the Fisher Information for GLMs","text":"<p>Let's compute \\(\\mathcal{I}\\) for a GLM. We need the second derivative of the log-likelihood.</p> <p>Starting from: $$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\frac{1}{\\phi} \\sum_i \\frac{(y_i - \\mu_i) x_{ij}}{V(\\mu_i) g'(\\mu_i)} $$</p> <p>Taking the derivative with respect to \\(\\beta_k\\):</p> \\[ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = \\frac{1}{\\phi} \\sum_i x_{ij} \\frac{\\partial}{\\partial \\beta_k}\\left[\\frac{y_i - \\mu_i}{V(\\mu_i) g'(\\mu_i)}\\right] \\] <p>This involves derivatives of \\(\\mu_i\\), \\(V(\\mu_i)\\), and \\(g'(\\mu_i)\\) with respect to \\(\\beta_k\\). After careful calculation (which is tedious but straightforward), we get:</p> \\[ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = -\\frac{1}{\\phi} \\sum_i \\frac{x_{ij} x_{ik}}{V(\\mu_i)[g'(\\mu_i)]^2} + \\text{(terms involving } y_i - \\mu_i \\text{)} \\] <p>Taking expectations (using \\(E[y_i - \\mu_i] = 0\\)):</p> \\[ \\mathcal{I}_{jk} = E\\left[-\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}\\right] = \\frac{1}{\\phi} \\sum_i \\frac{x_{ij} x_{ik}}{V(\\mu_i)[g'(\\mu_i)]^2} \\] <p>In matrix form:</p> \\[ \\mathcal{I} = \\frac{1}{\\phi} \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\] <p>where \\(\\mathbf{W}\\) is a diagonal matrix with:</p> \\[ W_{ii} = \\frac{1}{V(\\mu_i)[g'(\\mu_i)]^2} \\] <p>This is the weight matrix for IRLS!</p>"},{"location":"theory/irls/#26-fisher-scoring-algorithm","title":"2.6 Fisher Scoring Algorithm","text":"<p>Fisher scoring replaces Newton-Raphson's Hessian with the Fisher information:</p> \\[ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\mathcal{I}^{-1}(\\boldsymbol{\\beta}^{(t)}) \\mathbf{U}(\\boldsymbol{\\beta}^{(t)}) \\] <p>Substituting our expressions:</p> \\[ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\phi(\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\cdot \\frac{1}{\\phi} \\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{G}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu}^{(t)}) \\] \\[ = \\boldsymbol{\\beta}^{(t)} + (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{G}^{(t)} (\\mathbf{y} - \\boldsymbol{\\mu}^{(t)}) \\] <p>(The second line uses \\(\\mathbf{V}^{-1}\\mathbf{G}^{-1} = \\mathbf{W}\\mathbf{G}\\) since \\(W_{ii} = 1/(V_i G_i^2)\\).)</p>"},{"location":"theory/irls/#part-3-transforming-to-weighted-least-squares","title":"Part 3: Transforming to Weighted Least Squares","text":""},{"location":"theory/irls/#31-the-working-response","title":"3.1 The Working Response","text":"<p>The key insight is to define the working response (or adjusted dependent variable):</p> \\[ z_i = \\eta_i + (y_i - \\mu_i) g'(\\mu_i) \\] <p>where \\(\\eta_i = g(\\mu_i) = \\mathbf{x}_i^T\\boldsymbol{\\beta}^{(t)}\\) is the current linear predictor.</p> <p>In matrix form: $$ \\mathbf{z} = \\boldsymbol{\\eta} + \\mathbf{G}(\\mathbf{y} - \\boldsymbol{\\mu}) $$</p>"},{"location":"theory/irls/#32-the-magic-irls-update-equals-weighted-least-squares","title":"3.2 The Magic: IRLS Update Equals Weighted Least Squares","text":"<p>Let's show that the Fisher scoring update is equivalent to weighted least squares of \\(\\mathbf{z}\\) on \\(\\mathbf{X}\\) with weights \\(\\mathbf{W}\\).</p> <p>Weighted least squares solution:</p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{z} \\] <p>Substituting \\(\\mathbf{z}\\):</p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} [\\boldsymbol{\\eta} + \\mathbf{G}(\\mathbf{y} - \\boldsymbol{\\mu})] \\] \\[ = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\boldsymbol{\\eta} + (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{G}(\\mathbf{y} - \\boldsymbol{\\mu}) \\] <p>Now, \\(\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta}^{(t)}\\), so:</p> \\[ (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\boldsymbol{\\beta}^{(t)} = \\boldsymbol{\\beta}^{(t)} \\] <p>Therefore:</p> \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} = \\boldsymbol{\\beta}^{(t)} + (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{G}(\\mathbf{y} - \\boldsymbol{\\mu}) \\] <p>This is exactly the Fisher scoring update! \\(\\square\\)</p>"},{"location":"theory/irls/#33-why-this-matters","title":"3.3 Why This Matters","text":"<p>This equivalence is profound:</p> <ol> <li>Conceptual: GLM fitting is \"just\" repeated weighted linear regression</li> <li>Computational: We can use highly optimized linear algebra routines</li> <li>Software: The same code structure handles all GLM families</li> </ol>"},{"location":"theory/irls/#part-4-understanding-the-components","title":"Part 4: Understanding the Components","text":""},{"location":"theory/irls/#41-the-working-response-in-detail","title":"4.1 The Working Response in Detail","text":"\\[ z_i = \\eta_i + (y_i - \\mu_i) g'(\\mu_i) \\] <p>Intuition: The working response is a first-order Taylor approximation of \\(g(y_i)\\) around \\(\\mu_i\\):</p> \\[ g(y_i) \\approx g(\\mu_i) + g'(\\mu_i)(y_i - \\mu_i) = \\eta_i + g'(\\mu_i)(y_i - \\mu_i) \\] <p>So \\(z_i\\) is approximately what the linear predictor \"should be\" to match observation \\(y_i\\).</p> <p>Examples:</p> Family + Link \\(g'(\\mu)\\) Working Response \\(z_i\\) Gaussian + Identity \\(1\\) \\(\\eta_i + (y_i - \\mu_i) = y_i\\) Poisson + Log \\(1/\\mu\\) \\(\\eta_i + (y_i - \\mu_i)/\\mu_i\\) Binomial + Logit \\(1/[\\mu(1-\\mu)]\\) \\(\\eta_i + (y_i - \\mu_i)/[\\mu_i(1-\\mu_i)]\\) <p>For Gaussian, the working response is just \\(y_i\\)\u2014no adjustment needed!</p>"},{"location":"theory/irls/#42-the-working-weights-in-detail","title":"4.2 The Working Weights in Detail","text":"\\[ w_i = \\frac{1}{V(\\mu_i)[g'(\\mu_i)]^2} \\] <p>Interpretation: - Variance factor: \\(1/V(\\mu_i)\\) downweights observations with high variance (more noise \u2192 less information) - Link factor: \\(1/[g'(\\mu_i)]^2\\) adjusts for the curvature of the link function</p> <p>Examples:</p> Family + Link \\(V(\\mu)\\) \\(g'(\\mu)\\) Weight \\(w_i\\) Gaussian + Identity \\(1\\) \\(1\\) \\(1\\) Poisson + Log \\(\\mu\\) \\(1/\\mu\\) \\(\\mu\\) Binomial + Logit \\(\\mu(1-\\mu)\\) \\(1/[\\mu(1-\\mu)]\\) \\(\\mu(1-\\mu)\\) Gamma + Log \\(\\mu^2\\) \\(1/\\mu\\) \\(1\\) <p>Constant Weights</p> <p>For Gamma + Log, the weights are constant! This is because \\(V(\\mu) = \\mu^2\\) and \\(g'(\\mu) = 1/\\mu\\), so: $\\(w = \\frac{1}{\\mu^2 \\cdot (1/\\mu)^2} = 1\\)$</p>"},{"location":"theory/irls/#43-why-reweighted","title":"4.3 Why \"Reweighted\"?","text":"<p>The weights \\(w_i\\) depend on \\(\\mu_i\\), which depends on the current \\(\\boldsymbol{\\beta}^{(t)}\\). Each iteration:</p> <ol> <li>Updates \\(\\boldsymbol{\\beta}\\)</li> <li>Updates \\(\\boldsymbol{\\mu} = g^{-1}(\\mathbf{X}\\boldsymbol{\\beta})\\)</li> <li>Updates weights \\(\\mathbf{W}\\) based on new \\(\\boldsymbol{\\mu}\\)</li> </ol> <p>Hence \"iteratively reweighted\" least squares.</p>"},{"location":"theory/irls/#part-5-a-complete-worked-example","title":"Part 5: A Complete Worked Example","text":"<p>Let's trace through IRLS by hand for a tiny Poisson regression problem.</p>"},{"location":"theory/irls/#51-setup","title":"5.1 Setup","text":"<p>Data: | \\(i\\) | \\(y_i\\) | \\(x_i\\) | |-----|-------|-------| | 1 | 1 | 0 | | 2 | 4 | 1 | | 3 | 7 | 2 |</p> <p>Model: \\(\\log(\\mu_i) = \\beta_0 + \\beta_1 x_i\\)</p> <p>Design matrix:  $\\(\\mathbf{X} = \\begin{pmatrix} 1 &amp; 0 \\\\ 1 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix}\\)$</p> <p>Link function: \\(g(\\mu) = \\log(\\mu)\\), so \\(g'(\\mu) = 1/\\mu\\) and \\(g^{-1}(\\eta) = e^\\eta\\)</p> <p>Variance function: \\(V(\\mu) = \\mu\\)</p>"},{"location":"theory/irls/#52-iteration-0-initialization","title":"5.2 Iteration 0: Initialization","text":"<p>Initialize \\(\\mu_i\\) from the data (common choice: \\(\\mu_i^{(0)} = y_i + 0.1\\) to avoid \\(\\log(0)\\)):</p> \\[\\boldsymbol{\\mu}^{(0)} = \\begin{pmatrix} 1.1 \\\\ 4.1 \\\\ 7.1 \\end{pmatrix}\\] <p>Compute initial linear predictor: $\\(\\boldsymbol{\\eta}^{(0)} = \\log(\\boldsymbol{\\mu}^{(0)}) = \\begin{pmatrix} 0.095 \\\\ 1.411 \\\\ 1.960 \\end{pmatrix}\\)$</p> <p>We could solve for initial \\(\\boldsymbol{\\beta}^{(0)}\\) by WLS, but let's just proceed to iteration 1.</p>"},{"location":"theory/irls/#53-iteration-1","title":"5.3 Iteration 1","text":"<p>Step 1: Compute weights</p> \\[w_i = \\frac{1}{V(\\mu_i)[g'(\\mu_i)]^2} = \\frac{1}{\\mu_i \\cdot (1/\\mu_i)^2} = \\mu_i\\] \\[\\mathbf{W}^{(0)} = \\text{diag}(1.1, 4.1, 7.1)\\] <p>Step 2: Compute working response</p> \\[z_i = \\eta_i + (y_i - \\mu_i) g'(\\mu_i) = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i}\\] \\[z_1 = 0.095 + \\frac{1 - 1.1}{1.1} = 0.095 - 0.091 = 0.004$$ $$z_2 = 1.411 + \\frac{4 - 4.1}{4.1} = 1.411 - 0.024 = 1.387$$ $$z_3 = 1.960 + \\frac{7 - 7.1}{7.1} = 1.960 - 0.014 = 1.946\\] \\[\\mathbf{z}^{(0)} = \\begin{pmatrix} 0.004 \\\\ 1.387 \\\\ 1.946 \\end{pmatrix}\\] <p>Step 3: Compute \\(\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\\)</p> \\[\\mathbf{X}^T\\mathbf{W}\\mathbf{X} = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 2 \\end{pmatrix} \\begin{pmatrix} 1.1 &amp; 0 &amp; 0 \\\\ 0 &amp; 4.1 &amp; 0 \\\\ 0 &amp; 0 &amp; 7.1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 1 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix}\\] \\[= \\begin{pmatrix} 1.1 &amp; 4.1 &amp; 7.1 \\\\ 0 &amp; 4.1 &amp; 14.2 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 1 &amp; 1 \\\\ 1 &amp; 2 \\end{pmatrix} = \\begin{pmatrix} 12.3 &amp; 18.3 \\\\ 18.3 &amp; 32.5 \\end{pmatrix}\\] <p>Step 4: Compute \\(\\mathbf{X}^T\\mathbf{W}\\mathbf{z}\\)</p> \\[\\mathbf{X}^T\\mathbf{W}\\mathbf{z} = \\begin{pmatrix} 1.1 &amp; 4.1 &amp; 7.1 \\\\ 0 &amp; 4.1 &amp; 14.2 \\end{pmatrix} \\begin{pmatrix} 0.004 \\\\ 1.387 \\\\ 1.946 \\end{pmatrix} = \\begin{pmatrix} 19.51 \\\\ 33.32 \\end{pmatrix}\\] <p>Step 5: Solve for \\(\\boldsymbol{\\beta}^{(1)}\\)</p> \\[\\boldsymbol{\\beta}^{(1)} = (\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{W}\\mathbf{z}\\] <p>First, compute the inverse: $\\((\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1} = \\frac{1}{12.3 \\times 32.5 - 18.3^2} \\begin{pmatrix} 32.5 &amp; -18.3 \\\\ -18.3 &amp; 12.3 \\end{pmatrix}\\)$</p> \\[= \\frac{1}{64.86} \\begin{pmatrix} 32.5 &amp; -18.3 \\\\ -18.3 &amp; 12.3 \\end{pmatrix} = \\begin{pmatrix} 0.501 &amp; -0.282 \\\\ -0.282 &amp; 0.190 \\end{pmatrix}\\] <p>Then: $\\(\\boldsymbol{\\beta}^{(1)} = \\begin{pmatrix} 0.501 &amp; -0.282 \\\\ -0.282 &amp; 0.190 \\end{pmatrix} \\begin{pmatrix} 19.51 \\\\ 33.32 \\end{pmatrix} = \\begin{pmatrix} 0.38 \\\\ 0.83 \\end{pmatrix}\\)$</p> <p>Step 6: Update linear predictor and means</p> \\[\\boldsymbol{\\eta}^{(1)} = \\mathbf{X}\\boldsymbol{\\beta}^{(1)} = \\begin{pmatrix} 0.38 \\\\ 1.21 \\\\ 2.04 \\end{pmatrix}\\] \\[\\boldsymbol{\\mu}^{(1)} = e^{\\boldsymbol{\\eta}^{(1)}} = \\begin{pmatrix} 1.46 \\\\ 3.35 \\\\ 7.69 \\end{pmatrix}\\] <p>Step 7: Compute deviance</p> \\[D = 2\\sum_i [y_i \\log(y_i/\\mu_i) - (y_i - \\mu_i)]\\] \\[= 2[(1)\\log(1/1.46) - (1-1.46) + (4)\\log(4/3.35) - (4-3.35) + (7)\\log(7/7.69) - (7-7.69)]\\] \\[= 2[(-0.378 + 0.46) + (0.716 - 0.65) + (-0.661 + 0.69)]\\] \\[= 2[0.082 + 0.066 + 0.029] = 0.354\\]"},{"location":"theory/irls/#54-iteration-2-summary","title":"5.4 Iteration 2 (Summary)","text":"<p>Repeating with \\(\\boldsymbol{\\mu}^{(1)}\\):</p> <ul> <li>New weights: \\(\\mathbf{W}^{(1)} = \\text{diag}(1.46, 3.35, 7.69)\\)</li> <li>New working response computed</li> <li>Solve WLS again</li> <li>Get \\(\\boldsymbol{\\beta}^{(2)} \\approx (0.38, 0.83)\\) (nearly unchanged!)</li> <li>Deviance \\(\\approx 0.354\\) (unchanged)</li> </ul> <p>Converged! The change in deviance is below tolerance.</p>"},{"location":"theory/irls/#55-final-results","title":"5.5 Final Results","text":"\\[\\hat{\\beta}_0 = 0.38, \\quad \\hat{\\beta}_1 = 0.83\\] <p>Interpretation: Each unit increase in \\(x\\) multiplies the expected count by \\(e^{0.83} \\approx 2.29\\).</p>"},{"location":"theory/irls/#part-6-the-complete-irls-algorithm","title":"Part 6: The Complete IRLS Algorithm","text":"<pre><code>Algorithm: IRLS for GLM Fitting\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nInput:\n  - y: response vector (n \u00d7 1)\n  - X: design matrix (n \u00d7 p)\n  - family: distribution family (defines V(\u03bc))\n  - link: link function (defines g, g\u207b\u00b9, g')\n  - config: {max_iter, tolerance, min_weight}\n\nOutput:\n  - \u03b2: coefficient estimates (p \u00d7 1)\n  - \u03bc: fitted means (n \u00d7 1)\n  - D: final deviance\n  - \u03a3: covariance matrix (p \u00d7 p)\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n1. INITIALIZE\n   \u03bc\u207d\u2070\u207e \u2190 family.initialize(y)     // e.g., \u03bc = y + 0.1 for Poisson\n   \u03b7\u207d\u2070\u207e \u2190 g(\u03bc\u207d\u2070\u207e)                   // initial linear predictor\n   D\u207d\u2070\u207e \u2190 compute_deviance(y, \u03bc\u207d\u2070\u207e)\n\n2. ITERATE for t = 1, 2, ..., max_iter:\n\n   // 2a. Compute working weights\n   For i = 1 to n:\n       V_i \u2190 V(\u03bc\u1d62\u207d\u1d57\u207b\u00b9\u207e)\n       g'_i \u2190 g'(\u03bc\u1d62\u207d\u1d57\u207b\u00b9\u207e)\n       w_i \u2190 1 / (V_i \u00d7 g'_i\u00b2)\n       w_i \u2190 max(w_i, min_weight)    // numerical stability\n\n   W \u2190 diag(w\u2081, ..., w\u2099)\n\n   // 2b. Compute working response\n   For i = 1 to n:\n       z_i \u2190 \u03b7\u1d62\u207d\u1d57\u207b\u00b9\u207e + (y\u1d62 - \u03bc\u1d62\u207d\u1d57\u207b\u00b9\u207e) \u00d7 g'(\u03bc\u1d62\u207d\u1d57\u207b\u00b9\u207e)\n\n   // 2c. Solve weighted least squares\n   \u03b2\u207d\u1d57\u207e \u2190 solve(X'WX, X'Wz)         // Using Cholesky decomposition\n\n   // 2d. Update linear predictor and means\n   \u03b7\u207d\u1d57\u207e \u2190 X\u03b2\u207d\u1d57\u207e + offset             // offset is 0 if not specified\n   \u03bc\u207d\u1d57\u207e \u2190 g\u207b\u00b9(\u03b7\u207d\u1d57\u207e)\n\n   // Clamp means to valid range\n   \u03bc\u207d\u1d57\u207e \u2190 family.clamp(\u03bc\u207d\u1d57\u207e)        // e.g., \u03bc &gt; 0 for Poisson\n\n   // 2e. Compute deviance\n   D\u207d\u1d57\u207e \u2190 compute_deviance(y, \u03bc\u207d\u1d57\u207e)\n\n   // 2f. Check convergence\n   If |D\u207d\u1d57\u207e - D\u207d\u1d57\u207b\u00b9\u207e| / |D\u207d\u1d57\u207b\u00b9\u207e| &lt; tolerance:\n       converged \u2190 true\n       BREAK\n\n3. COMPUTE COVARIANCE\n   \u03a3_unscaled \u2190 (X'WX)\u207b\u00b9\n   \u03c6\u0302 \u2190 estimate_dispersion(y, \u03bc, family)\n   \u03a3 \u2190 \u03c6\u0302 \u00d7 \u03a3_unscaled\n\n4. RETURN \u03b2, \u03bc, D, \u03a3, converged\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n</code></pre>"},{"location":"theory/irls/#part-7-convergence-properties","title":"Part 7: Convergence Properties","text":""},{"location":"theory/irls/#71-when-does-irls-converge","title":"7.1 When Does IRLS Converge?","text":"<p>IRLS inherits the convergence properties of Fisher scoring:</p> <ul> <li>Local convergence: If started close enough to the MLE, IRLS converges</li> <li>Quadratic rate: Near the solution, the error roughly squares each iteration</li> <li>Not guaranteed globally: Can fail with poor initialization or problematic data</li> </ul>"},{"location":"theory/irls/#72-typical-convergence-behavior","title":"7.2 Typical Convergence Behavior","text":"Family Link Typical Iterations Gaussian Identity 1 (exact in one step!) Poisson Log 4-8 Binomial Logit 4-10 Gamma Log 4-8 Tweedie Log 5-15"},{"location":"theory/irls/#73-why-gaussian-converges-in-one-iteration","title":"7.3 Why Gaussian Converges in One Iteration","text":"<p>For Gaussian with identity link: - \\(V(\\mu) = 1\\), \\(g'(\\mu) = 1\\) - Weights \\(w_i = 1\\) (constant) - Working response \\(z_i = \\mu_i + (y_i - \\mu_i) = y_i\\)</p> <p>So IRLS becomes ordinary least squares of \\(\\mathbf{y}\\) on \\(\\mathbf{X}\\)\u2014the exact solution in one step!</p>"},{"location":"theory/irls/#74-when-convergence-fails","title":"7.4 When Convergence Fails","text":"<p>Problem 1: Complete Separation (Logistic Regression)</p> <p>If one predictor perfectly separates the 0s from the 1s, the MLE doesn't exist\u2014coefficients should be \\(\\pm\\infty\\).</p> <p>Symptom: Coefficients grow larger each iteration; deviance decreases slowly.</p> <p>Solution: Add regularization (ridge/lasso), remove the separating variable, or use Firth's bias-reduced logistic regression.</p> <p>Problem 2: Near-Zero Counts (Poisson)</p> <p>If many \\(y_i = 0\\), the means \\(\\mu_i\\) can become very small, causing numerical issues.</p> <p>Symptom: Warnings about small weights or invalid means.</p> <p>Solution: Check data quality; consider zero-inflated models.</p> <p>Problem 3: Multicollinearity</p> <p>If columns of \\(\\mathbf{X}\\) are nearly linearly dependent, \\(\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\\) is nearly singular.</p> <p>Symptom: Large standard errors, coefficients jump between iterations.</p> <p>Solution: Remove correlated predictors, add regularization.</p>"},{"location":"theory/irls/#part-8-implementation-in-rustystats","title":"Part 8: Implementation in RustyStats","text":""},{"location":"theory/irls/#81-code-structure","title":"8.1 Code Structure","text":"<pre><code>crates/rustystats-core/src/solvers/\n\u251c\u2500\u2500 mod.rs              // Module exports\n\u251c\u2500\u2500 irls.rs             // Main IRLS implementation\n\u2514\u2500\u2500 coordinate_descent.rs  // For regularized GLMs\n</code></pre>"},{"location":"theory/irls/#82-key-data-structures","title":"8.2 Key Data Structures","text":"<pre><code>/// Configuration for IRLS\npub struct IRLSConfig {\n    /// Maximum number of iterations (default: 25)\n    pub max_iterations: usize,\n\n    /// Convergence tolerance for deviance change (default: 1e-8)\n    pub tolerance: f64,\n\n    /// Minimum weight to prevent numerical issues (default: 1e-10)\n    pub min_weight: f64,\n\n    /// Whether to print iteration info (default: false)\n    pub verbose: bool,\n}\n\n/// Results from IRLS fitting\npub struct IRLSResult {\n    /// Estimated coefficients \u03b2\n    pub coefficients: Array1&lt;f64&gt;,\n\n    /// Fitted means \u03bc = g\u207b\u00b9(X\u03b2)\n    pub fitted_values: Array1&lt;f64&gt;,\n\n    /// Linear predictor \u03b7 = X\u03b2 + offset\n    pub linear_predictor: Array1&lt;f64&gt;,\n\n    /// Final deviance\n    pub deviance: f64,\n\n    /// Number of iterations used\n    pub iterations: usize,\n\n    /// Whether algorithm converged\n    pub converged: bool,\n\n    /// Unscaled covariance matrix (X'WX)\u207b\u00b9\n    pub covariance_unscaled: Array2&lt;f64&gt;,\n\n    /// Final IRLS weights\n    pub irls_weights: Array1&lt;f64&gt;,\n\n    // ... additional fields\n}\n</code></pre>"},{"location":"theory/irls/#83-the-core-loop-simplified","title":"8.3 The Core Loop (Simplified)","text":"<pre><code>pub fn fit_glm(\n    y: &amp;Array1&lt;f64&gt;,\n    x: &amp;Array2&lt;f64&gt;,\n    family: &amp;dyn Family,\n    link: &amp;dyn Link,\n    config: &amp;IRLSConfig,\n) -&gt; Result&lt;IRLSResult&gt; {\n    let (n, p) = (y.len(), x.ncols());\n\n    // Step 1: Initialize\n    let mut mu = family.initialize_mu(y);\n    let mut eta = link.link(&amp;mu);\n    let mut deviance = family.deviance(y, &amp;mu, None);\n\n    // Step 2: Iterate\n    for iteration in 1..=config.max_iterations {\n        // 2a: Compute weights\n        let variance = family.variance(&amp;mu);\n        let link_deriv = link.derivative(&amp;mu);\n        let weights = compute_weights(&amp;variance, &amp;link_deriv, config.min_weight);\n\n        // 2b: Compute working response\n        let z = compute_working_response(&amp;eta, y, &amp;mu, &amp;link_deriv);\n\n        // 2c: Solve weighted least squares\n        let xtwx = compute_xtwx(x, &amp;weights);\n        let xtwz = compute_xtwz(x, &amp;weights, &amp;z);\n        let beta = cholesky_solve(&amp;xtwx, &amp;xtwz)?;\n\n        // 2d: Update\n        eta = x.dot(&amp;beta);\n        mu = link.inverse(&amp;eta);\n        mu = family.clamp_mu(&amp;mu);\n\n        // 2e: Check convergence\n        let new_deviance = family.deviance(y, &amp;mu, None);\n        let relative_change = (deviance - new_deviance).abs() / deviance.max(1e-10);\n\n        if relative_change &lt; config.tolerance {\n            return Ok(IRLSResult { \n                coefficients: beta, \n                converged: true,\n                iterations: iteration,\n                // ...\n            });\n        }\n\n        deviance = new_deviance;\n    }\n\n    // Did not converge\n    Ok(IRLSResult { converged: false, ... })\n}\n</code></pre>"},{"location":"theory/irls/#84-parallel-computation","title":"8.4 Parallel Computation","text":"<p>The most expensive operation is computing \\(\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\\), which is \\(O(np^2)\\). RustyStats parallelizes this using Rayon:</p> <pre><code>fn compute_xtwx_parallel(x: &amp;Array2&lt;f64&gt;, w: &amp;Array1&lt;f64&gt;) -&gt; Array2&lt;f64&gt; {\n    let (n, p) = x.dim();\n\n    // Parallel fold-reduce pattern\n    (0..n).into_par_iter()\n        .fold(\n            // Each thread starts with a zero matrix\n            || Array2::zeros((p, p)),\n            // Accumulate: add w_i * x_i * x_i^T\n            |mut acc, i| {\n                let xi = x.row(i);\n                let wi = w[i];\n                for j in 0..p {\n                    for k in j..p {  // Only upper triangle (symmetric)\n                        acc[[j, k]] += wi * xi[j] * xi[k];\n                    }\n                }\n                acc\n            }\n        )\n        .reduce(\n            // Combine thread results\n            || Array2::zeros((p, p)),\n            |a, b| a + b\n        )\n        // Fill lower triangle from upper\n        .symmetrize()\n}\n</code></pre> <p>This achieves near-linear speedup with the number of cores for large datasets.</p>"},{"location":"theory/irls/#part-9-numerical-stability","title":"Part 9: Numerical Stability","text":""},{"location":"theory/irls/#91-weight-clamping","title":"9.1 Weight Clamping","text":"<p>Very small weights cause numerical problems:</p> <pre><code>fn compute_weights(variance: &amp;Array1&lt;f64&gt;, link_deriv: &amp;Array1&lt;f64&gt;, min_weight: f64) -&gt; Array1&lt;f64&gt; {\n    Zip::from(variance).and(link_deriv).map_collect(|&amp;v, &amp;g| {\n        let w = 1.0 / (v * g * g);\n        w.max(min_weight)  // Prevent division by near-zero\n    })\n}\n</code></pre>"},{"location":"theory/irls/#92-mean-clamping","title":"9.2 Mean Clamping","text":"<p>Means must stay in valid ranges:</p> <pre><code>// Poisson: \u03bc must be positive\nimpl Family for PoissonFamily {\n    fn clamp_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| m.max(1e-10))\n    }\n}\n\n// Binomial: \u03bc must be in (0, 1)\nimpl Family for BinomialFamily {\n    fn clamp_mu(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| m.clamp(1e-10, 1.0 - 1e-10))\n    }\n}\n</code></pre>"},{"location":"theory/irls/#93-cholesky-decomposition","title":"9.3 Cholesky Decomposition","text":"<p>Instead of computing \\((\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}\\) directly, we use Cholesky decomposition:</p> <ol> <li>\\(\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\\) is symmetric positive definite</li> <li>Factor: \\(\\mathbf{X}^T\\mathbf{W}\\mathbf{X} = \\mathbf{L}\\mathbf{L}^T\\) where \\(\\mathbf{L}\\) is lower triangular</li> <li>Solve \\(\\mathbf{L}\\mathbf{L}^T\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{W}\\mathbf{z}\\) by forward/backward substitution</li> </ol> <p>This is faster and more numerically stable than computing the inverse.</p>"},{"location":"theory/irls/#part-10-summary","title":"Part 10: Summary","text":"<p>IRLS converts the nonlinear GLM optimization problem into a sequence of weighted linear regressions:</p> <ol> <li>Linearize the problem using the working response \\(\\mathbf{z}\\)</li> <li>Reweight based on variance and link curvature using weights \\(\\mathbf{W}\\)</li> <li>Solve weighted least squares to update \\(\\boldsymbol{\\beta}\\)</li> <li>Repeat until convergence</li> </ol> <p>Key insights: - Derived from Fisher scoring (Newton-Raphson with expected Hessian) - Converges in 1 step for Gaussian/identity (reduces to OLS) - Typically 4-10 iterations for other families - Parallelizable for large datasets - Provides covariance matrix for inference as a byproduct</p>"},{"location":"theory/irls/#exercises","title":"Exercises","text":"<p>Exercise 1: Weights for Binomial/Logit</p> <p>Verify that for Binomial with logit link:</p> <p>a) \\(g'(\\mu) = \\frac{1}{\\mu(1-\\mu)}\\)</p> <p>b) The weight simplifies to \\(w_i = \\mu_i(1-\\mu_i)\\)</p> <p>c) Why are weights smallest when \\(\\mu \\approx 0\\) or \\(\\mu \\approx 1\\)?</p> <p>Exercise 2: Working Response</p> <p>For Poisson with log link, show that the working response can be written as:</p> \\[z_i = \\log(\\mu_i) + \\frac{y_i - \\mu_i}{\\mu_i} = \\log(\\mu_i) + \\frac{y_i}{\\mu_i} - 1\\] <p>Interpret this: what happens when \\(y_i = \\mu_i\\) (perfect prediction)?</p> <p>Exercise 3: Hand Calculation</p> <p>For the data: \\(y = (2, 5)\\), \\(x = (0, 1)\\), fit a Poisson model \\(\\log(\\mu) = \\beta_0 + \\beta_1 x\\).</p> <p>a) Initialize with \\(\\mu^{(0)} = y\\)</p> <p>b) Compute \\(\\mathbf{W}^{(0)}\\), \\(\\mathbf{z}^{(0)}\\)</p> <p>c) Solve for \\(\\boldsymbol{\\beta}^{(1)}\\)</p> <p>d) How does this compare to the true MLE?</p> <p>Exercise 4: Gaussian Convergence</p> <p>Prove algebraically that for Gaussian/identity:</p> <p>a) The weights are \\(w_i = 1\\)</p> <p>b) The working response is \\(z_i = y_i\\)</p> <p>c) One IRLS iteration gives \\(\\boldsymbol{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)</p> <p>d) This is the OLS solution, so no further iterations change \\(\\boldsymbol{\\beta}\\)</p>"},{"location":"theory/irls/#further-reading","title":"Further Reading","text":"<ul> <li>McCullagh, P. and Nelder, J.A. (1989). Generalized Linear Models, Chapter 2. \u2014 Original derivation</li> <li>Green, P.J. (1984). Iteratively Reweighted Least Squares for Maximum Likelihood Estimation. JRSS B. \u2014 Theoretical foundations</li> <li>Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models. \u2014 Extensions to GAMs</li> </ul>"},{"location":"theory/links/","title":"Link Functions","text":"<p>The link function is the bridge between the linear predictor and the mean response. This chapter explains each link function, its mathematical properties, and when to use it.</p>"},{"location":"theory/links/#what-is-a-link-function","title":"What is a Link Function?","text":"<p>In a GLM, we model:</p> \\[ \\eta = g(\\mu) \\quad \\Leftrightarrow \\quad \\mu = g^{-1}(\\eta) \\] <p>where: - \\(\\eta = X\\beta\\) is the linear predictor (can be any real number) - \\(\\mu = E(Y)\\) is the mean response (must be in valid range) - \\(g(\\cdot)\\) is the link function - \\(g^{-1}(\\cdot)\\) is the inverse link (also called the mean function)</p>"},{"location":"theory/links/#the-link-trait","title":"The Link Trait","text":"<p>In Rust, every link function implements:</p> <pre><code>pub trait Link: Send + Sync {\n    fn name(&amp;self) -&gt; &amp;str;\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;      // \u03b7 = g(\u03bc)\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;;  // \u03bc = g\u207b\u00b9(\u03b7)\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt;; // d\u03b7/d\u03bc\n}\n</code></pre> <p>The derivative is crucial for the IRLS algorithm - it determines how changes in \\(\\mu\\) affect changes in \\(\\eta\\).</p>"},{"location":"theory/links/#identity-link","title":"Identity Link","text":"<p>Formula: \\(\\eta = \\mu\\)</p> <p>The simplest link - no transformation at all.</p> Property Value Link \\(g(\\mu) = \\mu\\) Inverse \\(g^{-1}(\\eta) = \\eta\\) Derivative \\(g'(\\mu) = 1\\) Valid \\(\\mu\\) range \\((-\\infty, +\\infty)\\)"},{"location":"theory/links/#mathematical-properties","title":"Mathematical Properties","text":"<ul> <li>Linear relationship between predictor and mean</li> <li>No constraints on predictions</li> <li>Canonical link for Gaussian family</li> </ul>"},{"location":"theory/links/#when-to-use","title":"When to Use","text":"<ul> <li>Gaussian family (linear regression)</li> <li>When the response can be any real number</li> <li>When you want coefficients to represent additive effects</li> </ul>"},{"location":"theory/links/#interpretation","title":"Interpretation","text":"<p>With identity link: [ \\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots ]</p> <p>A one-unit increase in \\(x_1\\) adds \\(\\beta_1\\) to the expected response.</p>"},{"location":"theory/links/#code-location","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/links/identity.rs\n</code></pre>"},{"location":"theory/links/#implementation","title":"Implementation","text":"<pre><code>pub struct IdentityLink;\n\nimpl Link for IdentityLink {\n    fn name(&amp;self) -&gt; &amp;str { \"Identity\" }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.clone()\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        eta.clone()\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        Array1::ones(mu.len())\n    }\n}\n</code></pre>"},{"location":"theory/links/#log-link","title":"Log Link","text":"<p>Formula: \\(\\eta = \\log(\\mu)\\)</p> <p>Ensures predictions are always positive.</p> Property Value Link \\(g(\\mu) = \\log(\\mu)\\) Inverse \\(g^{-1}(\\eta) = e^\\eta\\) Derivative \\(g'(\\mu) = 1/\\mu\\) Valid \\(\\mu\\) range \\((0, +\\infty)\\)"},{"location":"theory/links/#mathematical-properties_1","title":"Mathematical Properties","text":"<ul> <li>Maps positive means to real numbers</li> <li>Inverse always produces positive values</li> <li>Canonical link for Poisson family</li> </ul>"},{"location":"theory/links/#when-to-use_1","title":"When to Use","text":"<ul> <li>Poisson family (counts)</li> <li>Gamma family (positive continuous)</li> <li>Any response that must be positive</li> <li>When multiplicative effects make sense</li> </ul>"},{"location":"theory/links/#interpretation_1","title":"Interpretation","text":"<p>With log link: [ \\log(\\mu) = \\beta_0 + \\beta_1 x_1 + \\cdots ]</p> <p>Exponentiating: [ \\mu = e^{\\beta_0} \\cdot e^{\\beta_1 x_1} \\cdot e^{\\beta_2 x_2} \\cdots ]</p> <p>A one-unit increase in \\(x_1\\) multiplies the expected response by \\(e^{\\beta_1}\\).</p> <p>This multiplicative factor is called a relativity in actuarial pricing.</p>"},{"location":"theory/links/#relativities","title":"Relativities","text":"<pre><code>result = rs.glm(\"claims ~ age + C(region)\", data, family=\"poisson\").fit()\n\n# Relativities = exp(coefficients)\nrelativities = np.exp(result.params)\n# or\nprint(result.relativities())\n</code></pre>"},{"location":"theory/links/#code-location_1","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/links/log.rs\n</code></pre>"},{"location":"theory/links/#implementation_1","title":"Implementation","text":"<pre><code>pub struct LogLink;\n\nimpl Link for LogLink {\n    fn name(&amp;self) -&gt; &amp;str { \"Log\" }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| m.max(1e-10).ln())  // Clamp to avoid log(0)\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        eta.mapv(|e| e.exp())\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| 1.0 / m.max(1e-10))\n    }\n}\n</code></pre>"},{"location":"theory/links/#logit-link","title":"Logit Link","text":"<p>Formula: \\(\\eta = \\log\\frac{\\mu}{1-\\mu}\\)</p> <p>Maps probabilities to the real line (log-odds transformation).</p> Property Value Link \\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\) Inverse \\(g^{-1}(\\eta) = \\frac{e^\\eta}{1+e^\\eta} = \\frac{1}{1+e^{-\\eta}}\\) Derivative \\(g'(\\mu) = \\frac{1}{\\mu(1-\\mu)}\\) Valid \\(\\mu\\) range \\((0, 1)\\)"},{"location":"theory/links/#mathematical-properties_2","title":"Mathematical Properties","text":"<ul> <li>Maps probabilities (0,1) to real numbers</li> <li>Symmetric: \\(\\text{logit}(p) = -\\text{logit}(1-p)\\)</li> <li>Canonical link for Binomial family</li> </ul>"},{"location":"theory/links/#when-to-use_2","title":"When to Use","text":"<ul> <li>Binomial family (binary outcomes)</li> <li>Logistic regression</li> <li>When the response is a probability</li> </ul>"},{"location":"theory/links/#interpretation-odds-ratios","title":"Interpretation: Odds Ratios","text":"<p>The logit link gives coefficients as log odds ratios.</p> <p>Odds of an event: [ \\text{Odds} = \\frac{P(Y=1)}{P(Y=0)} = \\frac{\\mu}{1-\\mu} ]</p> <p>With logit link: [ \\log\\left(\\frac{\\mu}{1-\\mu}\\right) = \\beta_0 + \\beta_1 x_1 + \\cdots ]</p> <p>A one-unit increase in \\(x_1\\) multiplies the odds by \\(e^{\\beta_1}\\).</p> \\(e^{\\beta}\\) Interpretation 1.0 No effect on odds 2.0 Doubles the odds 0.5 Halves the odds 1.5 Increases odds by 50%"},{"location":"theory/links/#example","title":"Example","text":"<pre><code>result = rs.fit_glm(binary_outcome, X, family=\"binomial\")\nodds_ratios = np.exp(result.params)\nprint(f\"Odds ratios: {odds_ratios}\")\n</code></pre>"},{"location":"theory/links/#code-location_2","title":"Code Location","text":"<pre><code>crates/rustystats-core/src/links/logit.rs\n</code></pre>"},{"location":"theory/links/#implementation_2","title":"Implementation","text":"<pre><code>pub struct LogitLink;\n\nimpl Link for LogitLink {\n    fn name(&amp;self) -&gt; &amp;str { \"Logit\" }\n\n    fn link(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| {\n            let m_clamped = m.clamp(1e-10, 1.0 - 1e-10);\n            (m_clamped / (1.0 - m_clamped)).ln()\n        })\n    }\n\n    fn inverse(&amp;self, eta: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        eta.mapv(|e| 1.0 / (1.0 + (-e).exp()))\n    }\n\n    fn derivative(&amp;self, mu: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {\n        mu.mapv(|m| {\n            let m_clamped = m.clamp(1e-10, 1.0 - 1e-10);\n            1.0 / (m_clamped * (1.0 - m_clamped))\n        })\n    }\n}\n</code></pre>"},{"location":"theory/links/#canonical-links","title":"Canonical Links","text":"<p>The canonical link is the link function that arises naturally from the exponential family form. Using the canonical link simplifies the math (the sufficient statistic equals the linear predictor).</p> Family Canonical Link Gaussian Identity Poisson Log Binomial Logit Gamma Inverse (\\(\\eta = -1/\\mu\\)) <p>Non-Canonical Links</p> <p>You can use non-canonical links. For example, Gamma with log link is common because:</p> <ul> <li>Log link ensures positive predictions</li> <li>Coefficients have multiplicative interpretation</li> <li>Inverse link can produce negative predictions</li> </ul> <p>RustyStats uses log link as default for Gamma.</p>"},{"location":"theory/links/#link-function-derivatives-in-irls","title":"Link Function Derivatives in IRLS","text":"<p>The link derivative \\(g'(\\mu)\\) appears in the IRLS working weights:</p> \\[ W = \\frac{1}{V(\\mu) \\cdot [g'(\\mu)]^2} \\] <p>And in the working response:</p> \\[ z = \\eta + (y - \\mu) \\cdot g'(\\mu) \\] Link \\(g'(\\mu)\\) Effect on Weights Identity 1 Weights depend only on \\(V(\\mu)\\) Log \\(1/\\mu\\) Small \\(\\mu\\) gets higher derivative Logit \\(1/[\\mu(1-\\mu)]\\) Extreme probabilities get higher derivative"},{"location":"theory/links/#choosing-a-link-function","title":"Choosing a Link Function","text":""},{"location":"theory/links/#default-choices","title":"Default Choices","text":"Family Default Link Reason Gaussian Identity Mean can be any real number Poisson Log Counts must be positive Binomial Logit Probabilities in (0,1) Gamma Log Amounts must be positive Tweedie Log Ensures positive predictions"},{"location":"theory/links/#when-to-override-defaults","title":"When to Override Defaults","text":"<ol> <li>Interpretability: Log link gives multiplicative effects</li> <li>Prediction range: Ensure predictions stay valid</li> <li>Domain knowledge: Some links may be more natural for your problem</li> </ol>"},{"location":"theory/links/#adding-a-new-link-function","title":"Adding a New Link Function","text":"<p>See Adding a New Link for instructions on implementing additional link functions like:</p> <ul> <li>Probit: \\(\\Phi^{-1}(\\mu)\\) for binomial (normal CDF inverse)</li> <li>Complementary log-log: \\(\\log(-\\log(1-\\mu))\\) for rare events</li> <li>Inverse: \\(-1/\\mu\\) for Gamma (canonical)</li> <li>Power: \\(\\mu^\\lambda\\) family</li> </ul>"},{"location":"theory/regularization/","title":"Regularization","text":"<p>Regularization adds a penalty to the objective function to prevent overfitting and enable variable selection. This chapter covers Ridge, Lasso, and Elastic Net regularization for GLMs.</p>"},{"location":"theory/regularization/#why-regularize","title":"Why Regularize?","text":"<p>Standard GLM fitting minimizes deviance:</p> \\[ \\hat{\\boldsymbol{\\beta}} = \\arg\\min_{\\boldsymbol{\\beta}} D(\\boldsymbol{\\beta}) \\] <p>Problems can arise with: - Many predictors: Overfitting, poor generalization - Correlated predictors: Unstable coefficients - Variable selection: Which predictors matter?</p> <p>Regularization adds a penalty term:</p> \\[ \\hat{\\boldsymbol{\\beta}} = \\arg\\min_{\\boldsymbol{\\beta}} \\left[ D(\\boldsymbol{\\beta}) + \\lambda P(\\boldsymbol{\\beta}) \\right] \\] <p>where \\(\\lambda\\) controls the penalty strength and \\(P(\\boldsymbol{\\beta})\\) is the penalty function.</p>"},{"location":"theory/regularization/#ridge-regression-l2","title":"Ridge Regression (L2)","text":""},{"location":"theory/regularization/#the-penalty","title":"The Penalty","text":"\\[ P_{\\text{Ridge}}(\\boldsymbol{\\beta}) = \\sum_{j=1}^{p} \\beta_j^2 = \\|\\boldsymbol{\\beta}\\|_2^2 \\]"},{"location":"theory/regularization/#full-objective","title":"Full Objective","text":"\\[ \\min_{\\boldsymbol{\\beta}} \\left[ D(\\boldsymbol{\\beta}) + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right] \\]"},{"location":"theory/regularization/#properties","title":"Properties","text":"<ul> <li>Shrinks all coefficients toward zero</li> <li>Never sets coefficients exactly to zero</li> <li>Handles multicollinearity well</li> <li>Smooth, differentiable penalty</li> </ul>"},{"location":"theory/regularization/#when-to-use","title":"When to Use","text":"<ul> <li>Many correlated predictors</li> <li>Want to keep all variables</li> <li>Stabilize coefficient estimates</li> </ul>"},{"location":"theory/regularization/#example","title":"Example","text":"<pre><code>import rustystats as rs\n\n# Ridge regression (l1_ratio = 0)\nresult = rs.fit_glm(\n    y, X,\n    family=\"gaussian\",\n    alpha=0.1,      # Penalty strength\n    l1_ratio=0.0    # Pure L2 (Ridge)\n)\n</code></pre>"},{"location":"theory/regularization/#lasso-regression-l1","title":"Lasso Regression (L1)","text":""},{"location":"theory/regularization/#the-penalty_1","title":"The Penalty","text":"\\[ P_{\\text{Lasso}}(\\boldsymbol{\\beta}) = \\sum_{j=1}^{p} |\\beta_j| = \\|\\boldsymbol{\\beta}\\|_1 \\]"},{"location":"theory/regularization/#full-objective_1","title":"Full Objective","text":"\\[ \\min_{\\boldsymbol{\\beta}} \\left[ D(\\boldsymbol{\\beta}) + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right] \\]"},{"location":"theory/regularization/#properties_1","title":"Properties","text":"<ul> <li>Sets some coefficients exactly to zero</li> <li>Performs variable selection</li> <li>Selects at most \\(n\\) variables (in high-dimensional settings)</li> <li>Non-smooth penalty (requires special optimization)</li> </ul>"},{"location":"theory/regularization/#the-soft-thresholding-operator","title":"The Soft Thresholding Operator","text":"<p>Lasso uses soft thresholding:</p> \\[ S(z, \\gamma) = \\text{sign}(z) \\max(|z| - \\gamma, 0) \\] <pre><code>pub fn soft_threshold(z: f64, gamma: f64) -&gt; f64 {\n    if z &gt; gamma {\n        z - gamma\n    } else if z &lt; -gamma {\n        z + gamma\n    } else {\n        0.0\n    }\n}\n</code></pre>"},{"location":"theory/regularization/#when-to-use_1","title":"When to Use","text":"<ul> <li>Feature selection is desired</li> <li>Many predictors, few are relevant</li> <li>Interpretable sparse models</li> </ul>"},{"location":"theory/regularization/#example_1","title":"Example","text":"<pre><code># Lasso (l1_ratio = 1)\nresult = rs.fit_glm(\n    y, X,\n    family=\"poisson\",\n    alpha=0.1,      # Penalty strength\n    l1_ratio=1.0    # Pure L1 (Lasso)\n)\n\nprint(f\"Non-zero coefficients: {result.n_nonzero()}\")\nprint(f\"Selected features: {result.selected_features()}\")\n</code></pre>"},{"location":"theory/regularization/#elastic-net","title":"Elastic Net","text":""},{"location":"theory/regularization/#the-penalty_2","title":"The Penalty","text":"<p>A weighted combination of L1 and L2:</p> \\[ P_{\\text{EN}}(\\boldsymbol{\\beta}) = \\rho \\|\\boldsymbol{\\beta}\\|_1 + \\frac{1-\\rho}{2} \\|\\boldsymbol{\\beta}\\|_2^2 \\] <p>where \\(\\rho \\in [0, 1]\\) is the L1 ratio.</p>"},{"location":"theory/regularization/#full-objective_2","title":"Full Objective","text":"\\[ \\min_{\\boldsymbol{\\beta}} \\left[ D(\\boldsymbol{\\beta}) + \\lambda \\left( \\rho \\sum_j |\\beta_j| + \\frac{1-\\rho}{2} \\sum_j \\beta_j^2 \\right) \\right] \\]"},{"location":"theory/regularization/#properties_2","title":"Properties","text":"<ul> <li>Combines variable selection (L1) with stability (L2)</li> <li>Can select more than \\(n\\) variables</li> <li>Handles groups of correlated predictors better than pure Lasso</li> </ul>"},{"location":"theory/regularization/#when-to-use_2","title":"When to Use","text":"<ul> <li>Correlated predictors that should be selected together</li> <li>Want some variable selection but also stability</li> <li>General default when unsure</li> </ul>"},{"location":"theory/regularization/#example_2","title":"Example","text":"<pre><code># Elastic Net (0 &lt; l1_ratio &lt; 1)\nresult = rs.fit_glm(\n    y, X,\n    family=\"gaussian\",\n    alpha=0.1,\n    l1_ratio=0.5    # 50% L1, 50% L2\n)\n</code></pre>"},{"location":"theory/regularization/#coordinate-descent-algorithm","title":"Coordinate Descent Algorithm","text":"<p>RustyStats uses coordinate descent for regularized GLMs, following the glmnet approach.</p>"},{"location":"theory/regularization/#algorithm-overview","title":"Algorithm Overview","text":"<pre><code>Initialize \u03b2 = 0 (or from IRLS)\nRepeat until converged:\n    For j = 1, ..., p:\n        Compute partial residual: r_j = z - X_{-j} \u03b2_{-j}\n        Update \u03b2_j by soft thresholding:\n            \u03b2_j = S(\u27e8X_j, W r_j\u27e9, \u03bb\u03c1) / (\u27e8X_j, W X_j\u27e9 + \u03bb(1-\u03c1))\n</code></pre>"},{"location":"theory/regularization/#why-coordinate-descent","title":"Why Coordinate Descent?","text":"<ul> <li>Efficient: \\(O(np)\\) per cycle</li> <li>Warm starts: Path from large \u03bb to small \u03bb</li> <li>Sparse updates: Skip zero coefficients</li> </ul>"},{"location":"theory/regularization/#implementation","title":"Implementation","text":"<pre><code>crates/rustystats-core/src/solvers/coordinate_descent.rs\n</code></pre> <p>Key optimizations: - Covariance updates (compute X'WX once) - Active set strategy (focus on non-zero coefficients) - Parallelization via Rayon</p>"},{"location":"theory/regularization/#choosing-alpha","title":"Choosing \u03bb (Alpha)","text":"<p>The penalty strength \u03bb (called <code>alpha</code> in RustyStats API) is crucial.</p>"},{"location":"theory/regularization/#lasso-path","title":"Lasso Path","text":"<p>Trace coefficients as \u03bb varies:</p> <pre><code>path = rs.lasso_path(y, X, family=\"gaussian\", n_alphas=50)\n\n# View path\nprint(path.alphas)        # \u03bb values\nprint(path.coefs)         # Coefficients at each \u03bb\n\n# Plot\npath.plot()\n</code></pre>"},{"location":"theory/regularization/#cross-validation","title":"Cross-Validation","text":"<p>Find optimal \u03bb by cross-validation:</p> <pre><code>cv_result = rs.cv_glm(\n    y, X,\n    family=\"poisson\",\n    l1_ratio=1.0,    # Lasso\n    cv=5,            # 5-fold CV\n    n_alphas=100\n)\n\nprint(f\"Best \u03b1 (min CV error): {cv_result.alpha_best}\")\nprint(f\"1-SE \u03b1 (more parsimonious): {cv_result.alpha_1se}\")\n\n# Refit with optimal \u03b1\nresult = rs.fit_glm(y, X, family=\"poisson\", \n                     alpha=cv_result.alpha_best, l1_ratio=1.0)\n</code></pre>"},{"location":"theory/regularization/#the-1-se-rule","title":"The 1-SE Rule","text":"<p>Choose the largest \u03bb within one standard error of the minimum CV error:</p> <pre><code>\u03bb_1se: largest \u03bb such that CV_error(\u03bb) \u2264 CV_error(\u03bb_min) + SE(\u03bb_min)\n</code></pre> <p>This gives a simpler model with comparable performance.</p>"},{"location":"theory/regularization/#standardization","title":"Standardization","text":""},{"location":"theory/regularization/#why-standardize","title":"Why Standardize?","text":"<p>The penalty is applied uniformly to all coefficients. Without standardization, variables on different scales are penalized differently.</p>"},{"location":"theory/regularization/#internal-standardization","title":"Internal Standardization","text":"<p>RustyStats internally standardizes features before fitting:</p> <pre><code># Features are standardized internally\n# Coefficients are returned on original scale\nresult = rs.fit_glm(y, X, family=\"gaussian\", alpha=0.1, l1_ratio=1.0)\n</code></pre>"},{"location":"theory/regularization/#intercept","title":"Intercept","text":"<p>The intercept is never penalized:</p> \\[ \\min_{\\beta_0, \\boldsymbol{\\beta}} \\left[ D(\\beta_0, \\boldsymbol{\\beta}) + \\lambda P(\\boldsymbol{\\beta}) \\right] \\]"},{"location":"theory/regularization/#regularization-with-different-families","title":"Regularization with Different Families","text":"<p>Regularization works with all GLM families:</p> <pre><code># Regularized Poisson\nresult = rs.fit_glm(y, X, family=\"poisson\", alpha=0.1, l1_ratio=1.0)\n\n# Regularized Binomial (Logistic)\nresult = rs.fit_glm(y, X, family=\"binomial\", alpha=0.1, l1_ratio=0.5)\n\n# Regularized Gamma\nresult = rs.fit_glm(y, X, family=\"gamma\", alpha=0.1, l1_ratio=0.0)\n</code></pre> <p>The coordinate descent algorithm adapts to each family's variance function and link.</p>"},{"location":"theory/regularization/#degrees-of-freedom","title":"Degrees of Freedom","text":"<p>For regularized models, the effective degrees of freedom is less than \\(p\\):</p> <ul> <li>Ridge: \\(\\text{df} = \\text{tr}(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T)\\)</li> <li>Lasso: Approximately the number of non-zero coefficients</li> </ul>"},{"location":"theory/regularization/#regularization-and-inference","title":"Regularization and Inference","text":"<p>Standard Errors with Regularization</p> <p>Standard errors from regularized models are biased and should be interpreted with caution. The coefficients are shrunk toward zero, which affects the sampling distribution.</p> <p>For valid inference with selected variables: 1. Use cross-validation to select variables 2. Refit an unregularized model with selected variables 3. Use standard errors from the unregularized model</p>"},{"location":"theory/regularization/#summary","title":"Summary","text":"Method Penalty Variable Selection Best For Ridge L2: \\(\\|\\boldsymbol{\\beta}\\|_2^2\\) No Multicollinearity Lasso L1: \\(\\|\\boldsymbol{\\beta}\\|_1\\) Yes Sparse models Elastic Net Mix Yes (grouped) Correlated predictors <p>Key parameters: - <code>alpha</code>: Penalty strength (larger = more shrinkage) - <code>l1_ratio</code>: Mix of L1/L2 (1 = Lasso, 0 = Ridge)</p> <p>Best practices: 1. Use cross-validation to choose <code>alpha</code> 2. Consider the 1-SE rule for parsimony 3. Start with <code>l1_ratio=0.5</code> (Elastic Net) unless you have a specific reason</p>"}]}